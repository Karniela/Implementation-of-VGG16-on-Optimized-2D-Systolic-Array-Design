{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50435e8",
   "metadata": {},
   "source": [
    "Part1. Train VGG16 with quantization-aware training (15%)\n",
    "\n",
    " - Train for 4-bit input activation and 4-bit weight to achieve >90% accuracy. \n",
    "\n",
    " - But, this time, reduce a certain convolution layer's input channel numbers to be 8 and output channel numbers to be 8. (v)\n",
    "\n",
    " - Also, remove the batch normalization layer after the squeezed convolution. (v)\n",
    "\n",
    "  e.g., replace \"conv -> relu -> batchnorm\" with \"conv -> relu\"\n",
    "\n",
    " - This layer will be mapped on your 8x8 2D systolic array. Thus, reducing to 8 channels helps your layer's mapping in an array nicely without tiling.\n",
    "\n",
    " - This time, compute your \"psum_recovered\" such as HW5 including ReLU and compare with your prehooked input for the next layer (instead of your computed psum_ref).\n",
    "\n",
    " - [hint] It is recommended not to reduce the input channel of Conv layer at too early layer position because the early layer's feature map size (nij) is large incurring long verification cycles.\n",
    "\n",
    "   (recommended location: around 27-th layer, e.g., features[27] for VGGNet)\n",
    "\n",
    " - Measure of success: accuracy >90%  with 8 input/output channels + error < 10^-3 for psum_recorvered for VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [100, 200,300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9127/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant1130_2_newmodel/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# dim means which dimension will be pruned in the weight.\n",
    "# dim = 0 prunes output channel, and dim = 1 prunes input channel\n",
    "# n defines which norm is used. e.g., if n=2, L-2 norm is used\n",
    "\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        #print(\"layer pruned\")\n",
    "        #print(layer)\n",
    "        prune.ln_structured(layer, name='weight', amount=0.8, dim=0, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "251dadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(model.features[39].named_parameters())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0630749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level:  tensor(0.8008, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "### Check sparsity ###\n",
    "mask1 = model.features[39].weight_mask\n",
    "sparsity_mask1 = (mask1 == 0).sum() / mask1.nelement()\n",
    "\n",
    "print(\"Sparsity level: \", sparsity_mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896b20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1000/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check accuracy after pruning\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1929cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.557 (0.557)\tData 0.265 (0.265)\tLoss 4.6970 (4.6970)\tPrec 8.594% (8.594%)\n",
      "Epoch: [0][100/391]\tTime 0.078 (0.057)\tData 0.002 (0.005)\tLoss 2.2413 (2.3219)\tPrec 11.719% (14.890%)\n",
      "Epoch: [0][200/391]\tTime 0.047 (0.057)\tData 0.002 (0.003)\tLoss 2.1010 (2.2295)\tPrec 22.656% (17.055%)\n",
      "Epoch: [0][300/391]\tTime 0.051 (0.054)\tData 0.002 (0.003)\tLoss 2.0263 (2.1710)\tPrec 21.875% (18.488%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 1.9306 (1.9306)\tPrec 28.906% (28.906%)\n",
      " * Prec 23.520% \n",
      "best acc: 23.520000\n",
      "Epoch: [1][0/391]\tTime 0.281 (0.281)\tData 0.236 (0.236)\tLoss 2.0548 (2.0548)\tPrec 20.312% (20.312%)\n",
      "Epoch: [1][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 1.8120 (1.9423)\tPrec 35.938% (23.817%)\n",
      "Epoch: [1][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 1.9091 (1.9300)\tPrec 25.781% (24.433%)\n",
      "Epoch: [1][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 1.7905 (1.9129)\tPrec 31.250% (25.304%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 1.8727 (1.8727)\tPrec 21.094% (21.094%)\n",
      " * Prec 27.590% \n",
      "best acc: 27.590000\n",
      "Epoch: [2][0/391]\tTime 0.294 (0.294)\tData 0.243 (0.243)\tLoss 1.8891 (1.8891)\tPrec 25.000% (25.000%)\n",
      "Epoch: [2][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 1.9854 (1.8551)\tPrec 25.000% (28.079%)\n",
      "Epoch: [2][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 1.8632 (1.8430)\tPrec 26.562% (28.685%)\n",
      "Epoch: [2][300/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 1.8196 (1.8367)\tPrec 32.031% (29.020%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.7235 (1.7235)\tPrec 36.719% (36.719%)\n",
      " * Prec 30.580% \n",
      "best acc: 30.580000\n",
      "Epoch: [3][0/391]\tTime 0.274 (0.274)\tData 0.226 (0.226)\tLoss 1.8415 (1.8415)\tPrec 35.938% (35.938%)\n",
      "Epoch: [3][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 1.7715 (1.7944)\tPrec 28.125% (30.825%)\n",
      "Epoch: [3][200/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 1.8198 (1.7825)\tPrec 29.688% (31.643%)\n",
      "Epoch: [3][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.7200 (1.7689)\tPrec 28.906% (32.107%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.6817 (1.6817)\tPrec 33.594% (33.594%)\n",
      " * Prec 34.870% \n",
      "best acc: 34.870000\n",
      "Epoch: [4][0/391]\tTime 0.284 (0.284)\tData 0.235 (0.235)\tLoss 1.7592 (1.7592)\tPrec 32.812% (32.812%)\n",
      "Epoch: [4][100/391]\tTime 0.052 (0.048)\tData 0.003 (0.004)\tLoss 1.7665 (1.7188)\tPrec 31.250% (34.986%)\n",
      "Epoch: [4][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 1.7049 (1.7127)\tPrec 35.938% (35.086%)\n",
      "Epoch: [4][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 1.7993 (1.7041)\tPrec 29.688% (35.294%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 1.6144 (1.6144)\tPrec 42.188% (42.188%)\n",
      " * Prec 37.470% \n",
      "best acc: 37.470000\n",
      "Epoch: [5][0/391]\tTime 0.326 (0.326)\tData 0.275 (0.275)\tLoss 1.7567 (1.7567)\tPrec 31.250% (31.250%)\n",
      "Epoch: [5][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.004)\tLoss 1.7657 (1.6481)\tPrec 28.906% (37.152%)\n",
      "Epoch: [5][200/391]\tTime 0.046 (0.049)\tData 0.002 (0.003)\tLoss 1.6776 (1.6426)\tPrec 35.156% (37.675%)\n",
      "Epoch: [5][300/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 1.5491 (1.6344)\tPrec 42.188% (37.941%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 1.5186 (1.5186)\tPrec 42.188% (42.188%)\n",
      " * Prec 39.970% \n",
      "best acc: 39.970000\n",
      "Epoch: [6][0/391]\tTime 0.280 (0.280)\tData 0.227 (0.227)\tLoss 1.5265 (1.5265)\tPrec 42.188% (42.188%)\n",
      "Epoch: [6][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 1.6084 (1.5768)\tPrec 35.156% (41.120%)\n",
      "Epoch: [6][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.4705 (1.5674)\tPrec 45.312% (41.360%)\n",
      "Epoch: [6][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 1.5595 (1.5474)\tPrec 40.625% (42.136%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 1.4500 (1.4500)\tPrec 45.312% (45.312%)\n",
      " * Prec 45.860% \n",
      "best acc: 45.860000\n",
      "Epoch: [7][0/391]\tTime 0.282 (0.282)\tData 0.233 (0.233)\tLoss 1.4371 (1.4371)\tPrec 47.656% (47.656%)\n",
      "Epoch: [7][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 1.5577 (1.4438)\tPrec 41.406% (47.300%)\n",
      "Epoch: [7][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 1.3823 (1.4287)\tPrec 48.438% (47.524%)\n",
      "Epoch: [7][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 1.3658 (1.4194)\tPrec 50.781% (47.817%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 1.4424 (1.4424)\tPrec 44.531% (44.531%)\n",
      " * Prec 48.350% \n",
      "best acc: 48.350000\n",
      "Epoch: [8][0/391]\tTime 0.281 (0.281)\tData 0.229 (0.229)\tLoss 1.3788 (1.3788)\tPrec 52.344% (52.344%)\n",
      "Epoch: [8][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 1.3726 (1.3684)\tPrec 48.438% (49.621%)\n",
      "Epoch: [8][200/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 1.2583 (1.3593)\tPrec 49.219% (50.210%)\n",
      "Epoch: [8][300/391]\tTime 0.048 (0.047)\tData 0.003 (0.002)\tLoss 1.2493 (1.3481)\tPrec 48.438% (50.613%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 1.3231 (1.3231)\tPrec 45.312% (45.312%)\n",
      " * Prec 53.200% \n",
      "best acc: 53.200000\n",
      "Epoch: [9][0/391]\tTime 0.282 (0.282)\tData 0.234 (0.234)\tLoss 1.4083 (1.4083)\tPrec 49.219% (49.219%)\n",
      "Epoch: [9][100/391]\tTime 0.046 (0.049)\tData 0.001 (0.004)\tLoss 1.3573 (1.2944)\tPrec 52.344% (52.901%)\n",
      "Epoch: [9][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 1.1945 (1.2930)\tPrec 53.906% (52.977%)\n",
      "Epoch: [9][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 1.2660 (1.2895)\tPrec 53.906% (53.265%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 1.3204 (1.3204)\tPrec 52.344% (52.344%)\n",
      " * Prec 53.820% \n",
      "best acc: 53.820000\n",
      "Epoch: [10][0/391]\tTime 0.318 (0.318)\tData 0.268 (0.268)\tLoss 1.0806 (1.0806)\tPrec 58.594% (58.594%)\n",
      "Epoch: [10][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 1.2283 (1.2599)\tPrec 56.250% (54.030%)\n",
      "Epoch: [10][200/391]\tTime 0.047 (0.048)\tData 0.005 (0.003)\tLoss 1.2303 (1.2480)\tPrec 56.250% (54.579%)\n",
      "Epoch: [10][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 1.2666 (1.2398)\tPrec 51.562% (55.012%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 1.2332 (1.2332)\tPrec 56.250% (56.250%)\n",
      " * Prec 55.520% \n",
      "best acc: 55.520000\n",
      "Epoch: [11][0/391]\tTime 0.314 (0.314)\tData 0.264 (0.264)\tLoss 1.2787 (1.2787)\tPrec 53.906% (53.906%)\n",
      "Epoch: [11][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 1.2113 (1.2063)\tPrec 52.344% (56.892%)\n",
      "Epoch: [11][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.1528 (1.2056)\tPrec 57.031% (56.596%)\n",
      "Epoch: [11][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 1.1407 (1.2019)\tPrec 59.375% (56.730%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.2923 (1.2923)\tPrec 51.562% (51.562%)\n",
      " * Prec 55.580% \n",
      "best acc: 55.580000\n",
      "Epoch: [12][0/391]\tTime 0.290 (0.290)\tData 0.242 (0.242)\tLoss 1.0650 (1.0650)\tPrec 59.375% (59.375%)\n",
      "Epoch: [12][100/391]\tTime 0.046 (0.050)\tData 0.001 (0.004)\tLoss 1.1919 (1.1789)\tPrec 60.156% (57.024%)\n",
      "Epoch: [12][200/391]\tTime 0.052 (0.048)\tData 0.002 (0.003)\tLoss 1.1848 (1.1731)\tPrec 54.688% (57.478%)\n",
      "Epoch: [12][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 1.3149 (1.1706)\tPrec 51.562% (57.566%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 1.2780 (1.2780)\tPrec 57.031% (57.031%)\n",
      " * Prec 55.080% \n",
      "best acc: 55.580000\n",
      "Epoch: [13][0/391]\tTime 0.282 (0.282)\tData 0.234 (0.234)\tLoss 1.2581 (1.2581)\tPrec 50.000% (50.000%)\n",
      "Epoch: [13][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 1.1749 (1.1354)\tPrec 57.812% (58.988%)\n",
      "Epoch: [13][200/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 1.0183 (1.1340)\tPrec 65.625% (59.103%)\n",
      "Epoch: [13][300/391]\tTime 0.042 (0.047)\tData 0.001 (0.002)\tLoss 1.1244 (1.1333)\tPrec 60.938% (59.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 1.3376 (1.3376)\tPrec 53.906% (53.906%)\n",
      " * Prec 57.150% \n",
      "best acc: 57.150000\n",
      "Epoch: [14][0/391]\tTime 0.269 (0.269)\tData 0.228 (0.228)\tLoss 0.9553 (0.9553)\tPrec 64.062% (64.062%)\n",
      "Epoch: [14][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 1.0686 (1.1164)\tPrec 60.938% (59.847%)\n",
      "Epoch: [14][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.0060 (1.1166)\tPrec 64.062% (59.931%)\n",
      "Epoch: [14][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.002)\tLoss 1.1758 (1.1094)\tPrec 55.469% (60.071%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 1.2871 (1.2871)\tPrec 50.781% (50.781%)\n",
      " * Prec 56.950% \n",
      "best acc: 57.150000\n",
      "Epoch: [15][0/391]\tTime 0.310 (0.310)\tData 0.260 (0.260)\tLoss 0.9918 (0.9918)\tPrec 61.719% (61.719%)\n",
      "Epoch: [15][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.9712 (1.0903)\tPrec 64.844% (60.968%)\n",
      "Epoch: [15][200/391]\tTime 0.043 (0.047)\tData 0.001 (0.003)\tLoss 1.0364 (1.0725)\tPrec 65.625% (61.715%)\n",
      "Epoch: [15][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.0646 (1.0700)\tPrec 58.594% (61.721%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 1.2917 (1.2917)\tPrec 60.156% (60.156%)\n",
      " * Prec 55.560% \n",
      "best acc: 57.150000\n",
      "Epoch: [16][0/391]\tTime 0.267 (0.267)\tData 0.218 (0.218)\tLoss 1.1719 (1.1719)\tPrec 60.938% (60.938%)\n",
      "Epoch: [16][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.9480 (1.0651)\tPrec 63.281% (61.796%)\n",
      "Epoch: [16][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.1295 (1.0616)\tPrec 62.500% (61.874%)\n",
      "Epoch: [16][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.9957 (1.0566)\tPrec 60.938% (62.316%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 1.2786 (1.2786)\tPrec 50.000% (50.000%)\n",
      " * Prec 58.170% \n",
      "best acc: 58.170000\n",
      "Epoch: [17][0/391]\tTime 0.305 (0.305)\tData 0.257 (0.257)\tLoss 0.8751 (0.8751)\tPrec 67.188% (67.188%)\n",
      "Epoch: [17][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 1.0728 (1.0265)\tPrec 59.375% (62.608%)\n",
      "Epoch: [17][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.9701 (1.0201)\tPrec 66.406% (63.134%)\n",
      "Epoch: [17][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.1482 (1.0194)\tPrec 62.500% (63.302%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 1.8316 (1.8316)\tPrec 36.719% (36.719%)\n",
      " * Prec 46.370% \n",
      "best acc: 58.170000\n",
      "Epoch: [18][0/391]\tTime 0.278 (0.278)\tData 0.226 (0.226)\tLoss 0.9070 (0.9070)\tPrec 67.969% (67.969%)\n",
      "Epoch: [18][100/391]\tTime 0.047 (0.048)\tData 0.001 (0.004)\tLoss 1.0424 (1.0070)\tPrec 62.500% (63.397%)\n",
      "Epoch: [18][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.9763 (1.0109)\tPrec 71.875% (63.511%)\n",
      "Epoch: [18][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8292 (1.0075)\tPrec 68.750% (63.803%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 1.0774 (1.0774)\tPrec 66.406% (66.406%)\n",
      " * Prec 62.240% \n",
      "best acc: 62.240000\n",
      "Epoch: [19][0/391]\tTime 0.263 (0.263)\tData 0.217 (0.217)\tLoss 0.8421 (0.8421)\tPrec 68.750% (68.750%)\n",
      "Epoch: [19][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.8829 (0.9753)\tPrec 69.531% (65.091%)\n",
      "Epoch: [19][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.9531 (0.9834)\tPrec 67.188% (64.789%)\n",
      "Epoch: [19][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.002)\tLoss 0.9679 (0.9845)\tPrec 65.625% (65.041%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 1.2186 (1.2186)\tPrec 57.812% (57.812%)\n",
      " * Prec 60.130% \n",
      "best acc: 62.240000\n",
      "Epoch: [20][0/391]\tTime 0.285 (0.285)\tData 0.237 (0.237)\tLoss 1.0029 (1.0029)\tPrec 63.281% (63.281%)\n",
      "Epoch: [20][100/391]\tTime 0.048 (0.048)\tData 0.001 (0.004)\tLoss 0.9621 (0.9710)\tPrec 67.188% (65.246%)\n",
      "Epoch: [20][200/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 1.1517 (0.9773)\tPrec 55.469% (65.279%)\n",
      "Epoch: [20][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8929 (0.9751)\tPrec 68.750% (65.339%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 1.3992 (1.3992)\tPrec 58.594% (58.594%)\n",
      " * Prec 55.570% \n",
      "best acc: 62.240000\n",
      "Epoch: [21][0/391]\tTime 0.286 (0.286)\tData 0.241 (0.241)\tLoss 0.9920 (0.9920)\tPrec 67.969% (67.969%)\n",
      "Epoch: [21][100/391]\tTime 0.051 (0.049)\tData 0.001 (0.004)\tLoss 1.0680 (0.9644)\tPrec 60.156% (65.695%)\n",
      "Epoch: [21][200/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.9493 (0.9500)\tPrec 64.062% (66.379%)\n",
      "Epoch: [21][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.8585 (0.9484)\tPrec 71.094% (66.515%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 1.0340 (1.0340)\tPrec 63.281% (63.281%)\n",
      " * Prec 64.310% \n",
      "best acc: 64.310000\n",
      "Epoch: [22][0/391]\tTime 0.295 (0.295)\tData 0.246 (0.246)\tLoss 0.9139 (0.9139)\tPrec 67.188% (67.188%)\n",
      "Epoch: [22][100/391]\tTime 0.047 (0.049)\tData 0.003 (0.004)\tLoss 1.1546 (0.9459)\tPrec 59.375% (66.569%)\n",
      "Epoch: [22][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.7849 (0.9314)\tPrec 67.188% (66.989%)\n",
      "Epoch: [22][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.9286 (0.9279)\tPrec 71.094% (67.156%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 1.2688 (1.2688)\tPrec 63.281% (63.281%)\n",
      " * Prec 61.690% \n",
      "best acc: 64.310000\n",
      "Epoch: [23][0/391]\tTime 0.289 (0.289)\tData 0.242 (0.242)\tLoss 1.0091 (1.0091)\tPrec 65.625% (65.625%)\n",
      "Epoch: [23][100/391]\tTime 0.044 (0.048)\tData 0.001 (0.004)\tLoss 0.9373 (0.9305)\tPrec 67.188% (66.677%)\n",
      "Epoch: [23][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.9765 (0.9234)\tPrec 64.844% (66.962%)\n",
      "Epoch: [23][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.8464 (0.9217)\tPrec 69.531% (67.066%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 1.4532 (1.4532)\tPrec 58.594% (58.594%)\n",
      " * Prec 57.570% \n",
      "best acc: 64.310000\n",
      "Epoch: [24][0/391]\tTime 0.257 (0.257)\tData 0.215 (0.215)\tLoss 0.9519 (0.9519)\tPrec 67.969% (67.969%)\n",
      "Epoch: [24][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.7907 (0.9092)\tPrec 67.188% (67.613%)\n",
      "Epoch: [24][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.8426 (0.9091)\tPrec 71.094% (67.627%)\n",
      "Epoch: [24][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.7571 (0.9153)\tPrec 71.094% (67.592%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 1.1244 (1.1244)\tPrec 53.125% (53.125%)\n",
      " * Prec 61.630% \n",
      "best acc: 64.310000\n",
      "Epoch: [25][0/391]\tTime 0.290 (0.290)\tData 0.238 (0.238)\tLoss 0.8555 (0.8555)\tPrec 71.094% (71.094%)\n",
      "Epoch: [25][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.004)\tLoss 0.7992 (0.8982)\tPrec 71.875% (67.976%)\n",
      "Epoch: [25][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.8320 (0.8993)\tPrec 65.625% (67.957%)\n",
      "Epoch: [25][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.9032 (0.9044)\tPrec 67.188% (67.725%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 2.4163 (2.4163)\tPrec 29.688% (29.688%)\n",
      " * Prec 35.990% \n",
      "best acc: 64.310000\n",
      "Epoch: [26][0/391]\tTime 0.301 (0.301)\tData 0.253 (0.253)\tLoss 0.9810 (0.9810)\tPrec 67.969% (67.969%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.049)\tData 0.001 (0.004)\tLoss 0.8626 (0.8886)\tPrec 70.312% (68.711%)\n",
      "Epoch: [26][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.8484 (0.8874)\tPrec 71.094% (68.886%)\n",
      "Epoch: [26][300/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.9564 (0.8864)\tPrec 67.969% (68.901%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.8133 (0.8133)\tPrec 66.406% (66.406%)\n",
      " * Prec 68.400% \n",
      "best acc: 68.400000\n",
      "Epoch: [27][0/391]\tTime 0.298 (0.298)\tData 0.242 (0.242)\tLoss 0.7491 (0.7491)\tPrec 75.000% (75.000%)\n",
      "Epoch: [27][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.004)\tLoss 0.9389 (0.8689)\tPrec 59.375% (69.098%)\n",
      "Epoch: [27][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.7054 (0.8752)\tPrec 75.000% (68.843%)\n",
      "Epoch: [27][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.9872 (0.8761)\tPrec 67.969% (68.932%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.9490 (0.9490)\tPrec 62.500% (62.500%)\n",
      " * Prec 65.390% \n",
      "best acc: 68.400000\n",
      "Epoch: [28][0/391]\tTime 0.267 (0.267)\tData 0.215 (0.215)\tLoss 0.9360 (0.9360)\tPrec 66.406% (66.406%)\n",
      "Epoch: [28][100/391]\tTime 0.049 (0.048)\tData 0.002 (0.004)\tLoss 0.8399 (0.8676)\tPrec 67.188% (69.222%)\n",
      "Epoch: [28][200/391]\tTime 0.048 (0.048)\tData 0.003 (0.003)\tLoss 0.9692 (0.8648)\tPrec 65.625% (69.263%)\n",
      "Epoch: [28][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.8350 (0.8612)\tPrec 70.312% (69.518%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.9692 (0.9692)\tPrec 60.938% (60.938%)\n",
      " * Prec 64.900% \n",
      "best acc: 68.400000\n",
      "Epoch: [29][0/391]\tTime 0.351 (0.351)\tData 0.299 (0.299)\tLoss 0.9417 (0.9417)\tPrec 66.406% (66.406%)\n",
      "Epoch: [29][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.005)\tLoss 0.9031 (0.8561)\tPrec 67.188% (69.423%)\n",
      "Epoch: [29][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.004)\tLoss 0.9327 (0.8511)\tPrec 67.969% (69.726%)\n",
      "Epoch: [29][300/391]\tTime 0.043 (0.048)\tData 0.001 (0.003)\tLoss 0.8624 (0.8511)\tPrec 63.281% (69.874%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.1370 (1.1370)\tPrec 61.719% (61.719%)\n",
      " * Prec 65.350% \n",
      "best acc: 68.400000\n",
      "Epoch: [30][0/391]\tTime 0.275 (0.275)\tData 0.228 (0.228)\tLoss 0.7921 (0.7921)\tPrec 72.656% (72.656%)\n",
      "Epoch: [30][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.7476 (0.8564)\tPrec 74.219% (69.833%)\n",
      "Epoch: [30][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.9299 (0.8484)\tPrec 64.844% (70.091%)\n",
      "Epoch: [30][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7662 (0.8472)\tPrec 73.438% (70.159%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.8944 (0.8944)\tPrec 66.406% (66.406%)\n",
      " * Prec 68.560% \n",
      "best acc: 68.560000\n",
      "Epoch: [31][0/391]\tTime 0.280 (0.280)\tData 0.234 (0.234)\tLoss 1.0842 (1.0842)\tPrec 64.062% (64.062%)\n",
      "Epoch: [31][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.7895 (0.8245)\tPrec 76.562% (70.908%)\n",
      "Epoch: [31][200/391]\tTime 0.049 (0.048)\tData 0.001 (0.003)\tLoss 0.8032 (0.8317)\tPrec 69.531% (70.635%)\n",
      "Epoch: [31][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.8615 (0.8340)\tPrec 71.094% (70.593%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.9800 (0.9800)\tPrec 61.719% (61.719%)\n",
      " * Prec 65.240% \n",
      "best acc: 68.560000\n",
      "Epoch: [32][0/391]\tTime 0.277 (0.277)\tData 0.230 (0.230)\tLoss 0.8204 (0.8204)\tPrec 72.656% (72.656%)\n",
      "Epoch: [32][100/391]\tTime 0.053 (0.050)\tData 0.003 (0.004)\tLoss 0.8831 (0.8356)\tPrec 69.531% (70.251%)\n",
      "Epoch: [32][200/391]\tTime 0.051 (0.048)\tData 0.001 (0.003)\tLoss 0.7799 (0.8283)\tPrec 73.438% (70.647%)\n",
      "Epoch: [32][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.7715 (0.8280)\tPrec 72.656% (70.808%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.8325 (0.8325)\tPrec 66.406% (66.406%)\n",
      " * Prec 69.610% \n",
      "best acc: 69.610000\n",
      "Epoch: [33][0/391]\tTime 0.352 (0.352)\tData 0.306 (0.306)\tLoss 0.8933 (0.8933)\tPrec 63.281% (63.281%)\n",
      "Epoch: [33][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.8362 (0.8169)\tPrec 68.750% (71.465%)\n",
      "Epoch: [33][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.9670 (0.8195)\tPrec 63.281% (71.179%)\n",
      "Epoch: [33][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 1.0240 (0.8132)\tPrec 65.625% (71.338%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.9588 (0.9588)\tPrec 64.062% (64.062%)\n",
      " * Prec 66.550% \n",
      "best acc: 69.610000\n",
      "Epoch: [34][0/391]\tTime 0.280 (0.280)\tData 0.229 (0.229)\tLoss 0.7498 (0.7498)\tPrec 71.094% (71.094%)\n",
      "Epoch: [34][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 0.7294 (0.8085)\tPrec 77.344% (71.674%)\n",
      "Epoch: [34][200/391]\tTime 0.048 (0.048)\tData 0.001 (0.003)\tLoss 0.9276 (0.8083)\tPrec 68.750% (71.576%)\n",
      "Epoch: [34][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.6063 (0.8139)\tPrec 76.562% (71.382%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.9272 (0.9272)\tPrec 67.969% (67.969%)\n",
      " * Prec 70.900% \n",
      "best acc: 70.900000\n",
      "Epoch: [35][0/391]\tTime 0.296 (0.296)\tData 0.248 (0.248)\tLoss 0.6751 (0.6751)\tPrec 78.906% (78.906%)\n",
      "Epoch: [35][100/391]\tTime 0.048 (0.050)\tData 0.002 (0.004)\tLoss 0.7687 (0.7966)\tPrec 74.219% (71.844%)\n",
      "Epoch: [35][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.8519 (0.8044)\tPrec 68.750% (71.451%)\n",
      "Epoch: [35][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.8568 (0.8058)\tPrec 67.969% (71.499%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.8883 (0.8883)\tPrec 65.625% (65.625%)\n",
      " * Prec 70.470% \n",
      "best acc: 70.900000\n",
      "Epoch: [36][0/391]\tTime 0.239 (0.239)\tData 0.190 (0.190)\tLoss 0.7902 (0.7902)\tPrec 71.094% (71.094%)\n",
      "Epoch: [36][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.004)\tLoss 0.7924 (0.8039)\tPrec 71.875% (71.682%)\n",
      "Epoch: [36][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.8430 (0.8055)\tPrec 67.969% (71.490%)\n",
      "Epoch: [36][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.002)\tLoss 0.9379 (0.8085)\tPrec 63.281% (71.296%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.7948 (0.7948)\tPrec 67.188% (67.188%)\n",
      " * Prec 71.050% \n",
      "best acc: 71.050000\n",
      "Epoch: [37][0/391]\tTime 0.275 (0.275)\tData 0.226 (0.226)\tLoss 0.7257 (0.7257)\tPrec 74.219% (74.219%)\n",
      "Epoch: [37][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.9433 (0.7895)\tPrec 61.719% (72.285%)\n",
      "Epoch: [37][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.7417 (0.7909)\tPrec 75.781% (72.023%)\n",
      "Epoch: [37][300/391]\tTime 0.050 (0.047)\tData 0.001 (0.002)\tLoss 0.6468 (0.7946)\tPrec 78.125% (72.124%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.9051 (0.9051)\tPrec 67.188% (67.188%)\n",
      " * Prec 71.260% \n",
      "best acc: 71.260000\n",
      "Epoch: [38][0/391]\tTime 0.282 (0.282)\tData 0.231 (0.231)\tLoss 0.6980 (0.6980)\tPrec 76.562% (76.562%)\n",
      "Epoch: [38][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.004)\tLoss 0.9582 (0.7956)\tPrec 68.750% (71.774%)\n",
      "Epoch: [38][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.7982 (0.7938)\tPrec 71.094% (71.933%)\n",
      "Epoch: [38][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.8211 (0.7907)\tPrec 69.531% (72.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 1.1299 (1.1299)\tPrec 64.844% (64.844%)\n",
      " * Prec 65.760% \n",
      "best acc: 71.260000\n",
      "Epoch: [39][0/391]\tTime 0.287 (0.287)\tData 0.238 (0.238)\tLoss 0.7930 (0.7930)\tPrec 71.875% (71.875%)\n",
      "Epoch: [39][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 0.8216 (0.7934)\tPrec 65.625% (72.030%)\n",
      "Epoch: [39][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.8433 (0.7847)\tPrec 68.750% (72.334%)\n",
      "Epoch: [39][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.7459 (0.7849)\tPrec 76.562% (72.342%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.7867 (0.7867)\tPrec 71.094% (71.094%)\n",
      " * Prec 70.610% \n",
      "best acc: 71.260000\n",
      "Epoch: [40][0/391]\tTime 0.259 (0.259)\tData 0.218 (0.218)\tLoss 0.7880 (0.7880)\tPrec 74.219% (74.219%)\n",
      "Epoch: [40][100/391]\tTime 0.047 (0.048)\tData 0.001 (0.004)\tLoss 0.7588 (0.7657)\tPrec 75.781% (73.167%)\n",
      "Epoch: [40][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.9004 (0.7792)\tPrec 66.406% (72.516%)\n",
      "Epoch: [40][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.002)\tLoss 0.7171 (0.7814)\tPrec 71.875% (72.433%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.9076 (0.9076)\tPrec 65.625% (65.625%)\n",
      " * Prec 69.880% \n",
      "best acc: 71.260000\n",
      "Epoch: [41][0/391]\tTime 0.250 (0.250)\tData 0.207 (0.207)\tLoss 0.7770 (0.7770)\tPrec 68.750% (68.750%)\n",
      "Epoch: [41][100/391]\tTime 0.044 (0.049)\tData 0.001 (0.004)\tLoss 0.7785 (0.7906)\tPrec 75.000% (72.130%)\n",
      "Epoch: [41][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.6886 (0.7832)\tPrec 75.781% (72.361%)\n",
      "Epoch: [41][300/391]\tTime 0.051 (0.047)\tData 0.003 (0.002)\tLoss 0.8299 (0.7834)\tPrec 70.312% (72.482%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.8816 (0.8816)\tPrec 68.750% (68.750%)\n",
      " * Prec 69.890% \n",
      "best acc: 71.260000\n",
      "Epoch: [42][0/391]\tTime 0.307 (0.307)\tData 0.257 (0.257)\tLoss 0.6791 (0.6791)\tPrec 75.781% (75.781%)\n",
      "Epoch: [42][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.004)\tLoss 0.6423 (0.7652)\tPrec 76.562% (73.445%)\n",
      "Epoch: [42][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.8051 (0.7562)\tPrec 76.562% (73.772%)\n",
      "Epoch: [42][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.7901 (0.7679)\tPrec 71.094% (73.142%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.9049 (0.9049)\tPrec 64.062% (64.062%)\n",
      " * Prec 70.610% \n",
      "best acc: 71.260000\n",
      "Epoch: [43][0/391]\tTime 0.293 (0.293)\tData 0.237 (0.237)\tLoss 0.6664 (0.6664)\tPrec 78.125% (78.125%)\n",
      "Epoch: [43][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.6217 (0.7536)\tPrec 76.562% (73.484%)\n",
      "Epoch: [43][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.7408 (0.7569)\tPrec 73.438% (73.298%)\n",
      "Epoch: [43][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.6362 (0.7602)\tPrec 78.906% (73.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.7363 (0.7363)\tPrec 71.094% (71.094%)\n",
      " * Prec 72.490% \n",
      "best acc: 72.490000\n",
      "Epoch: [44][0/391]\tTime 0.307 (0.307)\tData 0.255 (0.255)\tLoss 0.6984 (0.6984)\tPrec 79.688% (79.688%)\n",
      "Epoch: [44][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.7059 (0.7612)\tPrec 72.656% (73.337%)\n",
      "Epoch: [44][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.7495 (0.7547)\tPrec 74.219% (73.461%)\n",
      "Epoch: [44][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.9503 (0.7602)\tPrec 66.406% (73.456%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.7911 (0.7911)\tPrec 70.312% (70.312%)\n",
      " * Prec 71.700% \n",
      "best acc: 72.490000\n",
      "Epoch: [45][0/391]\tTime 0.277 (0.277)\tData 0.227 (0.227)\tLoss 0.6972 (0.6972)\tPrec 75.000% (75.000%)\n",
      "Epoch: [45][100/391]\tTime 0.051 (0.049)\tData 0.001 (0.004)\tLoss 0.6283 (0.7588)\tPrec 78.906% (73.205%)\n",
      "Epoch: [45][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.7935 (0.7550)\tPrec 76.562% (73.570%)\n",
      "Epoch: [45][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.002)\tLoss 0.9276 (0.7610)\tPrec 69.531% (73.258%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.8777 (0.8777)\tPrec 66.406% (66.406%)\n",
      " * Prec 70.540% \n",
      "best acc: 72.490000\n",
      "Epoch: [46][0/391]\tTime 0.319 (0.319)\tData 0.267 (0.267)\tLoss 0.8309 (0.8309)\tPrec 70.312% (70.312%)\n",
      "Epoch: [46][100/391]\tTime 0.048 (0.050)\tData 0.002 (0.005)\tLoss 0.9743 (0.7681)\tPrec 67.188% (73.028%)\n",
      "Epoch: [46][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.6116 (0.7587)\tPrec 79.688% (73.430%)\n",
      "Epoch: [46][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.7063 (0.7613)\tPrec 75.781% (73.360%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.8823 (0.8823)\tPrec 67.188% (67.188%)\n",
      " * Prec 67.820% \n",
      "best acc: 72.490000\n",
      "Epoch: [47][0/391]\tTime 0.352 (0.352)\tData 0.302 (0.302)\tLoss 1.0418 (1.0418)\tPrec 64.062% (64.062%)\n",
      "Epoch: [47][100/391]\tTime 0.047 (0.049)\tData 0.001 (0.005)\tLoss 0.7490 (0.7708)\tPrec 72.656% (73.159%)\n",
      "Epoch: [47][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.6553 (0.7554)\tPrec 78.125% (73.624%)\n",
      "Epoch: [47][300/391]\tTime 0.054 (0.047)\tData 0.002 (0.003)\tLoss 0.8252 (0.7514)\tPrec 71.094% (73.767%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 1.0646 (1.0646)\tPrec 64.844% (64.844%)\n",
      " * Prec 66.440% \n",
      "best acc: 72.490000\n",
      "Epoch: [48][0/391]\tTime 0.305 (0.305)\tData 0.259 (0.259)\tLoss 0.7356 (0.7356)\tPrec 75.781% (75.781%)\n",
      "Epoch: [48][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.004)\tLoss 0.6539 (0.7359)\tPrec 76.562% (74.451%)\n",
      "Epoch: [48][200/391]\tTime 0.055 (0.047)\tData 0.002 (0.003)\tLoss 0.6518 (0.7496)\tPrec 74.219% (73.721%)\n",
      "Epoch: [48][300/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.8522 (0.7469)\tPrec 71.875% (73.822%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.9328 (0.9328)\tPrec 63.281% (63.281%)\n",
      " * Prec 65.150% \n",
      "best acc: 72.490000\n",
      "Epoch: [49][0/391]\tTime 0.290 (0.290)\tData 0.240 (0.240)\tLoss 0.7114 (0.7114)\tPrec 71.875% (71.875%)\n",
      "Epoch: [49][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 1.0485 (0.7407)\tPrec 64.062% (73.244%)\n",
      "Epoch: [49][200/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 0.8413 (0.7424)\tPrec 73.438% (73.546%)\n",
      "Epoch: [49][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.8061 (0.7506)\tPrec 67.969% (73.448%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.9295 (0.9295)\tPrec 67.969% (67.969%)\n",
      " * Prec 70.620% \n",
      "best acc: 72.490000\n"
     ]
    }
   ],
   "source": [
    "## Start finetuning (training here), and see how much you can recover your accuracy ##\n",
    "## You can change hyper parameters such as epochs or lr ##\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 50\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)+'pruned'\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6871bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7249/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check your accuracy again after finetuning\n",
    "PATH = \"result/VGG16_quantpruned/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f97244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "## Send an image and use prehook to grab the inputs of all the QuantConv2d layers\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5c9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find \"weight_int\" for features[3] ####\n",
    "w_bit = 4\n",
    "weight_q = model.features[3].weight_q\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "#print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a34b756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level:  tensor(0.8834, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### check your sparsity for weight_int is near 90% #####\n",
    "#### Your sparsity could be >90% after quantization #####\n",
    "sparsity_weight_int = (weight_int == 0).sum() / weight_int.nelement()\n",
    "print(\"Sparsity level: \", sparsity_weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707569b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
