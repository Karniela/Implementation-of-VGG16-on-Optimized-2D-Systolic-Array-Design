{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50435e8",
   "metadata": {},
   "source": [
    "Part1. Train VGG16 with quantization-aware training (15%)\n",
    "\n",
    " - Train for 4-bit input activation and 4-bit weight to achieve >90% accuracy. \n",
    "\n",
    " - But, this time, reduce a certain convolution layer's input channel numbers to be 8 and output channel numbers to be 8. (v)\n",
    "\n",
    " - Also, remove the batch normalization layer after the squeezed convolution. (v)\n",
    "\n",
    "  e.g., replace \"conv -> relu -> batchnorm\" with \"conv -> relu\"\n",
    "\n",
    " - This layer will be mapped on your 8x8 2D systolic array. Thus, reducing to 8 channels helps your layer's mapping in an array nicely without tiling.\n",
    "\n",
    " - This time, compute your \"psum_recovered\" such as HW5 including ReLU and compare with your prehooked input for the next layer (instead of your computed psum_ref).\n",
    "\n",
    " - [hint] It is recommended not to reduce the input channel of Conv layer at too early layer position because the early layer's feature map size (nij) is large incurring long verification cycles.\n",
    "\n",
    "   (recommended location: around 27-th layer, e.g., features[27] for VGGNet)\n",
    "\n",
    " - Measure of success: accuracy >90%  with 8 input/output channels + error < 10^-3 for psum_recorvered for VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [100, 200,300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9127/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant1130_2_newmodel/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f9c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# dim means which dimension will be pruned in the weight.\n",
    "# dim = 0 prunes output channel, and dim = 1 prunes input channel\n",
    "# n defines which norm is used. e.g., if n=2, L-2 norm is used\n",
    "\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        #print(\"layer pruned\")\n",
    "        #print(layer)\n",
    "        prune.l1_unstructured(layer, name='weight', amount=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251dadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(model.features[39].named_parameters())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0630749b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level:  tensor(0.8000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "### Check sparsity ###\n",
    "mask1 = model.features[39].weight_mask\n",
    "sparsity_mask1 = (mask1 == 0).sum() / mask1.nelement()\n",
    "\n",
    "print(\"Sparsity level: \", sparsity_mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896b20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1000/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check accuracy after pruning\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1929cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.573 (0.573)\tData 0.263 (0.263)\tLoss 4.8865 (4.8865)\tPrec 20.312% (20.312%)\n",
      "Epoch: [0][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.004)\tLoss 0.4311 (0.9496)\tPrec 86.719% (73.066%)\n",
      "Epoch: [0][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.4459 (0.6848)\tPrec 88.281% (79.901%)\n",
      "Epoch: [0][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.3719 (0.5764)\tPrec 89.844% (82.696%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.3301 (0.3301)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.630% \n",
      "best acc: 86.630000\n",
      "Epoch: [1][0/391]\tTime 0.290 (0.290)\tData 0.245 (0.245)\tLoss 0.2545 (0.2545)\tPrec 89.844% (89.844%)\n",
      "Epoch: [1][100/391]\tTime 0.043 (0.046)\tData 0.001 (0.004)\tLoss 0.3104 (0.2946)\tPrec 91.406% (90.548%)\n",
      "Epoch: [1][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.2556 (0.2926)\tPrec 92.188% (90.532%)\n",
      "Epoch: [1][300/391]\tTime 0.044 (0.044)\tData 0.001 (0.003)\tLoss 0.3856 (0.2873)\tPrec 89.062% (90.594%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.2759 (0.2759)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.170% \n",
      "best acc: 88.170000\n",
      "Epoch: [2][0/391]\tTime 0.286 (0.286)\tData 0.239 (0.239)\tLoss 0.3912 (0.3912)\tPrec 85.156% (85.156%)\n",
      "Epoch: [2][100/391]\tTime 0.048 (0.046)\tData 0.002 (0.004)\tLoss 0.2435 (0.2480)\tPrec 90.625% (91.847%)\n",
      "Epoch: [2][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.003)\tLoss 0.2373 (0.2490)\tPrec 89.062% (91.783%)\n",
      "Epoch: [2][300/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.2221 (0.2499)\tPrec 93.750% (91.661%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.3510 (0.3510)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.490% \n",
      "best acc: 88.170000\n",
      "Epoch: [3][0/391]\tTime 0.298 (0.298)\tData 0.247 (0.247)\tLoss 0.2784 (0.2784)\tPrec 90.625% (90.625%)\n",
      "Epoch: [3][100/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.2439 (0.2290)\tPrec 89.062% (92.126%)\n",
      "Epoch: [3][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.1812 (0.2279)\tPrec 96.094% (92.188%)\n",
      "Epoch: [3][300/391]\tTime 0.047 (0.045)\tData 0.001 (0.002)\tLoss 0.2494 (0.2360)\tPrec 93.750% (92.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3766 (0.3766)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.060% \n",
      "best acc: 88.170000\n",
      "Epoch: [4][0/391]\tTime 0.296 (0.296)\tData 0.247 (0.247)\tLoss 0.1565 (0.1565)\tPrec 95.312% (95.312%)\n",
      "Epoch: [4][100/391]\tTime 0.045 (0.046)\tData 0.001 (0.004)\tLoss 0.2621 (0.2221)\tPrec 91.406% (92.296%)\n",
      "Epoch: [4][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.2486 (0.2207)\tPrec 90.625% (92.390%)\n",
      "Epoch: [4][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1457 (0.2187)\tPrec 94.531% (92.499%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.2490 (0.2490)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.640% \n",
      "best acc: 88.640000\n",
      "Epoch: [5][0/391]\tTime 0.280 (0.280)\tData 0.229 (0.229)\tLoss 0.2302 (0.2302)\tPrec 91.406% (91.406%)\n",
      "Epoch: [5][100/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.2001 (0.2051)\tPrec 94.531% (93.069%)\n",
      "Epoch: [5][200/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.2410 (0.2042)\tPrec 93.750% (93.163%)\n",
      "Epoch: [5][300/391]\tTime 0.042 (0.045)\tData 0.002 (0.002)\tLoss 0.1354 (0.2068)\tPrec 95.312% (92.964%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.2529 (0.2529)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.220% \n",
      "best acc: 88.640000\n",
      "Epoch: [6][0/391]\tTime 0.308 (0.308)\tData 0.264 (0.264)\tLoss 0.2362 (0.2362)\tPrec 92.969% (92.969%)\n",
      "Epoch: [6][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.2242 (0.1946)\tPrec 92.188% (93.301%)\n",
      "Epoch: [6][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1517 (0.2005)\tPrec 95.312% (93.128%)\n",
      "Epoch: [6][300/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.1982 (0.2012)\tPrec 94.531% (93.096%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2259 (0.2259)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.430% \n",
      "best acc: 88.640000\n",
      "Epoch: [7][0/391]\tTime 0.311 (0.311)\tData 0.262 (0.262)\tLoss 0.1470 (0.1470)\tPrec 94.531% (94.531%)\n",
      "Epoch: [7][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.0986 (0.1837)\tPrec 97.656% (93.680%)\n",
      "Epoch: [7][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1494 (0.1887)\tPrec 94.531% (93.490%)\n",
      "Epoch: [7][300/391]\tTime 0.046 (0.045)\tData 0.002 (0.002)\tLoss 0.3511 (0.1914)\tPrec 88.281% (93.381%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 0.2045 (0.2045)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.090% \n",
      "best acc: 88.640000\n",
      "Epoch: [8][0/391]\tTime 0.267 (0.267)\tData 0.223 (0.223)\tLoss 0.1021 (0.1021)\tPrec 96.094% (96.094%)\n",
      "Epoch: [8][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.004)\tLoss 0.1982 (0.1830)\tPrec 92.188% (93.680%)\n",
      "Epoch: [8][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1359 (0.1891)\tPrec 96.094% (93.552%)\n",
      "Epoch: [8][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2270 (0.1865)\tPrec 91.406% (93.623%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.1687 (0.1687)\tPrec 94.531% (94.531%)\n",
      " * Prec 88.550% \n",
      "best acc: 88.640000\n",
      "Epoch: [9][0/391]\tTime 0.320 (0.320)\tData 0.273 (0.273)\tLoss 0.1377 (0.1377)\tPrec 96.094% (96.094%)\n",
      "Epoch: [9][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.1577 (0.1865)\tPrec 95.312% (93.541%)\n",
      "Epoch: [9][200/391]\tTime 0.045 (0.045)\tData 0.004 (0.003)\tLoss 0.1051 (0.1776)\tPrec 96.875% (93.968%)\n",
      "Epoch: [9][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1887 (0.1819)\tPrec 93.750% (93.825%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.2091 (0.2091)\tPrec 93.750% (93.750%)\n",
      " * Prec 88.470% \n",
      "best acc: 88.640000\n",
      "Epoch: [10][0/391]\tTime 0.274 (0.274)\tData 0.230 (0.230)\tLoss 0.1243 (0.1243)\tPrec 97.656% (97.656%)\n",
      "Epoch: [10][100/391]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.2045 (0.1669)\tPrec 92.969% (94.655%)\n",
      "Epoch: [10][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1074 (0.1663)\tPrec 93.750% (94.516%)\n",
      "Epoch: [10][300/391]\tTime 0.043 (0.045)\tData 0.001 (0.002)\tLoss 0.1651 (0.1700)\tPrec 92.969% (94.342%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2103 (0.2103)\tPrec 92.969% (92.969%)\n",
      " * Prec 87.930% \n",
      "best acc: 88.640000\n",
      "Epoch: [11][0/391]\tTime 0.308 (0.308)\tData 0.256 (0.256)\tLoss 0.1711 (0.1711)\tPrec 94.531% (94.531%)\n",
      "Epoch: [11][100/391]\tTime 0.041 (0.047)\tData 0.002 (0.004)\tLoss 0.1382 (0.1636)\tPrec 94.531% (94.330%)\n",
      "Epoch: [11][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1807 (0.1649)\tPrec 93.750% (94.481%)\n",
      "Epoch: [11][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2631 (0.1681)\tPrec 89.062% (94.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3791 (0.3791)\tPrec 85.938% (85.938%)\n",
      " * Prec 87.020% \n",
      "best acc: 88.640000\n",
      "Epoch: [12][0/391]\tTime 0.295 (0.295)\tData 0.248 (0.248)\tLoss 0.2442 (0.2442)\tPrec 90.625% (90.625%)\n",
      "Epoch: [12][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.0798 (0.1585)\tPrec 98.438% (94.485%)\n",
      "Epoch: [12][200/391]\tTime 0.041 (0.045)\tData 0.001 (0.003)\tLoss 0.3539 (0.1640)\tPrec 89.844% (94.407%)\n",
      "Epoch: [12][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1922 (0.1674)\tPrec 95.312% (94.357%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.2277 (0.2277)\tPrec 93.750% (93.750%)\n",
      " * Prec 88.860% \n",
      "best acc: 88.860000\n",
      "Epoch: [13][0/391]\tTime 0.279 (0.279)\tData 0.242 (0.242)\tLoss 0.1310 (0.1310)\tPrec 97.656% (97.656%)\n",
      "Epoch: [13][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.1399 (0.1747)\tPrec 96.094% (94.075%)\n",
      "Epoch: [13][200/391]\tTime 0.043 (0.045)\tData 0.001 (0.003)\tLoss 0.2479 (0.1711)\tPrec 92.188% (94.185%)\n",
      "Epoch: [13][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1739 (0.1705)\tPrec 92.969% (94.080%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2311 (0.2311)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.460% \n",
      "best acc: 88.860000\n",
      "Epoch: [14][0/391]\tTime 0.291 (0.291)\tData 0.248 (0.248)\tLoss 0.1395 (0.1395)\tPrec 95.312% (95.312%)\n",
      "Epoch: [14][100/391]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.1809 (0.1561)\tPrec 93.750% (94.431%)\n",
      "Epoch: [14][200/391]\tTime 0.046 (0.045)\tData 0.001 (0.003)\tLoss 0.1403 (0.1554)\tPrec 95.312% (94.488%)\n",
      "Epoch: [14][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.0767 (0.1594)\tPrec 96.875% (94.433%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2535 (0.2535)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.250% \n",
      "best acc: 88.860000\n",
      "Epoch: [15][0/391]\tTime 0.254 (0.254)\tData 0.207 (0.207)\tLoss 0.1103 (0.1103)\tPrec 95.312% (95.312%)\n",
      "Epoch: [15][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.1934 (0.1609)\tPrec 92.969% (94.686%)\n",
      "Epoch: [15][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.003)\tLoss 0.1243 (0.1611)\tPrec 96.094% (94.558%)\n",
      "Epoch: [15][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1824 (0.1623)\tPrec 95.312% (94.479%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.2337 (0.2337)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.200% \n",
      "best acc: 88.860000\n",
      "Epoch: [16][0/391]\tTime 0.293 (0.293)\tData 0.247 (0.247)\tLoss 0.2269 (0.2269)\tPrec 92.969% (92.969%)\n",
      "Epoch: [16][100/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.2108 (0.1560)\tPrec 93.750% (94.616%)\n",
      "Epoch: [16][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1107 (0.1549)\tPrec 96.875% (94.636%)\n",
      "Epoch: [16][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1561 (0.1558)\tPrec 96.094% (94.622%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.3309 (0.3309)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.470% \n",
      "best acc: 88.860000\n",
      "Epoch: [17][0/391]\tTime 0.281 (0.281)\tData 0.233 (0.233)\tLoss 0.1434 (0.1434)\tPrec 92.188% (92.188%)\n",
      "Epoch: [17][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.1143 (0.1538)\tPrec 96.094% (94.725%)\n",
      "Epoch: [17][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1018 (0.1511)\tPrec 96.094% (94.823%)\n",
      "Epoch: [17][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1156 (0.1511)\tPrec 96.094% (94.835%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2950 (0.2950)\tPrec 88.281% (88.281%)\n",
      " * Prec 88.700% \n",
      "best acc: 88.860000\n",
      "Epoch: [18][0/391]\tTime 0.343 (0.343)\tData 0.296 (0.296)\tLoss 0.1111 (0.1111)\tPrec 96.094% (96.094%)\n",
      "Epoch: [18][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.1757 (0.1395)\tPrec 93.750% (95.181%)\n",
      "Epoch: [18][200/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.1633 (0.1498)\tPrec 92.969% (94.850%)\n",
      "Epoch: [18][300/391]\tTime 0.045 (0.045)\tData 0.001 (0.002)\tLoss 0.1742 (0.1513)\tPrec 95.312% (94.804%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2362 (0.2362)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.170% \n",
      "best acc: 89.170000\n",
      "Epoch: [19][0/391]\tTime 0.328 (0.328)\tData 0.283 (0.283)\tLoss 0.1426 (0.1426)\tPrec 94.531% (94.531%)\n",
      "Epoch: [19][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.2538 (0.1510)\tPrec 91.406% (94.694%)\n",
      "Epoch: [19][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.2348 (0.1480)\tPrec 92.969% (94.729%)\n",
      "Epoch: [19][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2487 (0.1491)\tPrec 90.625% (94.705%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.2323 (0.2323)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.400% \n",
      "best acc: 89.170000\n",
      "Epoch: [20][0/391]\tTime 0.290 (0.290)\tData 0.245 (0.245)\tLoss 0.2057 (0.2057)\tPrec 92.188% (92.188%)\n",
      "Epoch: [20][100/391]\tTime 0.043 (0.046)\tData 0.001 (0.004)\tLoss 0.1929 (0.1403)\tPrec 93.750% (95.382%)\n",
      "Epoch: [20][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1509 (0.1437)\tPrec 93.750% (95.095%)\n",
      "Epoch: [20][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2064 (0.1461)\tPrec 95.312% (94.991%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.3532 (0.3532)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.540% \n",
      "best acc: 89.170000\n",
      "Epoch: [21][0/391]\tTime 0.289 (0.289)\tData 0.243 (0.243)\tLoss 0.1064 (0.1064)\tPrec 98.438% (98.438%)\n",
      "Epoch: [21][100/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.0659 (0.1357)\tPrec 97.656% (95.274%)\n",
      "Epoch: [21][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.1071 (0.1420)\tPrec 96.875% (95.180%)\n",
      "Epoch: [21][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1460 (0.1453)\tPrec 96.094% (95.030%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.2424 (0.2424)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.710% \n",
      "best acc: 89.170000\n",
      "Epoch: [22][0/391]\tTime 0.270 (0.270)\tData 0.222 (0.222)\tLoss 0.2064 (0.2064)\tPrec 94.531% (94.531%)\n",
      "Epoch: [22][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.2046 (0.1406)\tPrec 93.750% (95.042%)\n",
      "Epoch: [22][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.2278 (0.1467)\tPrec 89.062% (94.912%)\n",
      "Epoch: [22][300/391]\tTime 0.046 (0.045)\tData 0.002 (0.002)\tLoss 0.1514 (0.1462)\tPrec 96.094% (94.972%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.2584 (0.2584)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.160% \n",
      "best acc: 89.170000\n",
      "Epoch: [23][0/391]\tTime 0.353 (0.353)\tData 0.309 (0.309)\tLoss 0.1227 (0.1227)\tPrec 97.656% (97.656%)\n",
      "Epoch: [23][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.005)\tLoss 0.1210 (0.1468)\tPrec 95.312% (94.926%)\n",
      "Epoch: [23][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.1654 (0.1501)\tPrec 92.969% (94.772%)\n",
      "Epoch: [23][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1560 (0.1489)\tPrec 94.531% (94.928%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.2106 (0.2106)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.650% \n",
      "best acc: 89.170000\n",
      "Epoch: [24][0/391]\tTime 0.312 (0.312)\tData 0.268 (0.268)\tLoss 0.1200 (0.1200)\tPrec 96.875% (96.875%)\n",
      "Epoch: [24][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.004)\tLoss 0.1168 (0.1393)\tPrec 94.531% (95.251%)\n",
      "Epoch: [24][200/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.1840 (0.1419)\tPrec 96.875% (95.192%)\n",
      "Epoch: [24][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1177 (0.1454)\tPrec 96.094% (95.022%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.331 (0.331)\tLoss 0.3173 (0.3173)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.230% \n",
      "best acc: 89.170000\n",
      "Epoch: [25][0/391]\tTime 0.317 (0.317)\tData 0.267 (0.267)\tLoss 0.1100 (0.1100)\tPrec 95.312% (95.312%)\n",
      "Epoch: [25][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.0873 (0.1393)\tPrec 96.875% (95.104%)\n",
      "Epoch: [25][200/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.1364 (0.1412)\tPrec 96.094% (95.079%)\n",
      "Epoch: [25][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.002)\tLoss 0.2475 (0.1431)\tPrec 92.969% (94.985%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2431 (0.2431)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.990% \n",
      "best acc: 89.170000\n",
      "Epoch: [26][0/391]\tTime 0.333 (0.333)\tData 0.285 (0.285)\tLoss 0.1667 (0.1667)\tPrec 94.531% (94.531%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.005)\tLoss 0.2169 (0.1317)\tPrec 91.406% (95.320%)\n",
      "Epoch: [26][200/391]\tTime 0.044 (0.046)\tData 0.003 (0.003)\tLoss 0.1875 (0.1408)\tPrec 94.531% (95.021%)\n",
      "Epoch: [26][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.2203 (0.1431)\tPrec 91.406% (94.944%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.3417 (0.3417)\tPrec 87.500% (87.500%)\n",
      " * Prec 88.550% \n",
      "best acc: 89.170000\n",
      "Epoch: [27][0/391]\tTime 0.302 (0.302)\tData 0.256 (0.256)\tLoss 0.1140 (0.1140)\tPrec 96.094% (96.094%)\n",
      "Epoch: [27][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.1853 (0.1378)\tPrec 94.531% (95.065%)\n",
      "Epoch: [27][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1304 (0.1452)\tPrec 92.969% (94.811%)\n",
      "Epoch: [27][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1250 (0.1449)\tPrec 95.312% (94.856%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.2651 (0.2651)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.740% \n",
      "best acc: 89.170000\n",
      "Epoch: [28][0/391]\tTime 0.316 (0.316)\tData 0.269 (0.269)\tLoss 0.1425 (0.1425)\tPrec 96.094% (96.094%)\n",
      "Epoch: [28][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.2401 (0.1468)\tPrec 90.625% (94.895%)\n",
      "Epoch: [28][200/391]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.1316 (0.1464)\tPrec 96.875% (94.877%)\n",
      "Epoch: [28][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.0907 (0.1466)\tPrec 96.875% (94.887%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2714 (0.2714)\tPrec 93.750% (93.750%)\n",
      " * Prec 87.990% \n",
      "best acc: 89.170000\n",
      "Epoch: [29][0/391]\tTime 0.329 (0.329)\tData 0.282 (0.282)\tLoss 0.2403 (0.2403)\tPrec 89.844% (89.844%)\n",
      "Epoch: [29][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.1245 (0.1396)\tPrec 94.531% (95.096%)\n",
      "Epoch: [29][200/391]\tTime 0.046 (0.045)\tData 0.002 (0.003)\tLoss 0.1799 (0.1429)\tPrec 94.531% (94.924%)\n",
      "Epoch: [29][300/391]\tTime 0.041 (0.045)\tData 0.002 (0.003)\tLoss 0.0872 (0.1444)\tPrec 97.656% (94.949%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.1481 (0.1481)\tPrec 94.531% (94.531%)\n",
      " * Prec 88.720% \n",
      "best acc: 89.170000\n",
      "Epoch: [30][0/391]\tTime 0.294 (0.294)\tData 0.249 (0.249)\tLoss 0.0988 (0.0988)\tPrec 95.312% (95.312%)\n",
      "Epoch: [30][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.0772 (0.1381)\tPrec 97.656% (95.181%)\n",
      "Epoch: [30][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.003)\tLoss 0.0798 (0.1391)\tPrec 97.656% (95.114%)\n",
      "Epoch: [30][300/391]\tTime 0.042 (0.045)\tData 0.001 (0.002)\tLoss 0.1679 (0.1406)\tPrec 96.094% (95.035%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.2443 (0.2443)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.580% \n",
      "best acc: 89.170000\n",
      "Epoch: [31][0/391]\tTime 0.301 (0.301)\tData 0.249 (0.249)\tLoss 0.0918 (0.0918)\tPrec 96.875% (96.875%)\n",
      "Epoch: [31][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.1575 (0.1419)\tPrec 92.969% (94.941%)\n",
      "Epoch: [31][200/391]\tTime 0.042 (0.045)\tData 0.003 (0.003)\tLoss 0.2342 (0.1423)\tPrec 92.969% (95.009%)\n",
      "Epoch: [31][300/391]\tTime 0.047 (0.045)\tData 0.001 (0.002)\tLoss 0.1717 (0.1387)\tPrec 94.531% (95.089%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.3132 (0.3132)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.480% \n",
      "best acc: 89.170000\n",
      "Epoch: [32][0/391]\tTime 0.302 (0.302)\tData 0.259 (0.259)\tLoss 0.1981 (0.1981)\tPrec 91.406% (91.406%)\n",
      "Epoch: [32][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1520 (0.1351)\tPrec 96.875% (95.297%)\n",
      "Epoch: [32][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.2864 (0.1370)\tPrec 90.625% (95.165%)\n",
      "Epoch: [32][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2591 (0.1397)\tPrec 92.188% (95.139%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.1923 (0.1923)\tPrec 93.750% (93.750%)\n",
      " * Prec 88.930% \n",
      "best acc: 89.170000\n",
      "Epoch: [33][0/391]\tTime 0.285 (0.285)\tData 0.239 (0.239)\tLoss 0.1718 (0.1718)\tPrec 92.969% (92.969%)\n",
      "Epoch: [33][100/391]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.0818 (0.1338)\tPrec 96.875% (95.413%)\n",
      "Epoch: [33][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1240 (0.1378)\tPrec 96.094% (95.266%)\n",
      "Epoch: [33][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1013 (0.1390)\tPrec 97.656% (95.203%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.2491 (0.2491)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.960% \n",
      "best acc: 89.170000\n",
      "Epoch: [34][0/391]\tTime 0.312 (0.312)\tData 0.270 (0.270)\tLoss 0.1176 (0.1176)\tPrec 96.094% (96.094%)\n",
      "Epoch: [34][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.1230 (0.1243)\tPrec 96.094% (95.738%)\n",
      "Epoch: [34][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1359 (0.1310)\tPrec 97.656% (95.425%)\n",
      "Epoch: [34][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1872 (0.1335)\tPrec 92.188% (95.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2019 (0.2019)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.360% \n",
      "best acc: 89.170000\n",
      "Epoch: [35][0/391]\tTime 0.310 (0.310)\tData 0.256 (0.256)\tLoss 0.2825 (0.2825)\tPrec 90.625% (90.625%)\n",
      "Epoch: [35][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2822 (0.1363)\tPrec 91.406% (95.227%)\n",
      "Epoch: [35][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1545 (0.1372)\tPrec 96.094% (95.301%)\n",
      "Epoch: [35][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1068 (0.1360)\tPrec 95.312% (95.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.2259 (0.2259)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.480% \n",
      "best acc: 89.170000\n",
      "Epoch: [36][0/391]\tTime 0.311 (0.311)\tData 0.266 (0.266)\tLoss 0.1592 (0.1592)\tPrec 95.312% (95.312%)\n",
      "Epoch: [36][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1066 (0.1226)\tPrec 95.312% (95.676%)\n",
      "Epoch: [36][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.003)\tLoss 0.1829 (0.1249)\tPrec 93.750% (95.561%)\n",
      "Epoch: [36][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1520 (0.1337)\tPrec 93.750% (95.279%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.2363 (0.2363)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.700% \n",
      "best acc: 89.170000\n",
      "Epoch: [37][0/391]\tTime 0.297 (0.297)\tData 0.247 (0.247)\tLoss 0.1320 (0.1320)\tPrec 95.312% (95.312%)\n",
      "Epoch: [37][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1036 (0.1269)\tPrec 96.094% (95.738%)\n",
      "Epoch: [37][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.2194 (0.1335)\tPrec 92.969% (95.464%)\n",
      "Epoch: [37][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1290 (0.1344)\tPrec 96.094% (95.398%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.2012 (0.2012)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.990% \n",
      "best acc: 89.170000\n",
      "Epoch: [38][0/391]\tTime 0.315 (0.315)\tData 0.271 (0.271)\tLoss 0.1914 (0.1914)\tPrec 91.406% (91.406%)\n",
      "Epoch: [38][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.1231 (0.1281)\tPrec 96.094% (95.537%)\n",
      "Epoch: [38][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.1450 (0.1279)\tPrec 93.750% (95.519%)\n",
      "Epoch: [38][300/391]\tTime 0.041 (0.045)\tData 0.002 (0.002)\tLoss 0.0996 (0.1289)\tPrec 96.875% (95.497%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2049 (0.2049)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.500% \n",
      "best acc: 89.170000\n",
      "Epoch: [39][0/391]\tTime 0.289 (0.289)\tData 0.238 (0.238)\tLoss 0.1141 (0.1141)\tPrec 96.875% (96.875%)\n",
      "Epoch: [39][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.0872 (0.1257)\tPrec 96.875% (95.490%)\n",
      "Epoch: [39][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.2052 (0.1282)\tPrec 93.750% (95.437%)\n",
      "Epoch: [39][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1953 (0.1308)\tPrec 94.531% (95.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2993 (0.2993)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.340% \n",
      "best acc: 89.170000\n",
      "Epoch: [40][0/391]\tTime 0.314 (0.314)\tData 0.267 (0.267)\tLoss 0.1508 (0.1508)\tPrec 93.750% (93.750%)\n",
      "Epoch: [40][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1002 (0.1158)\tPrec 96.094% (95.893%)\n",
      "Epoch: [40][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.0725 (0.1201)\tPrec 98.438% (95.744%)\n",
      "Epoch: [40][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1223 (0.1278)\tPrec 93.750% (95.484%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2003 (0.2003)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.510% \n",
      "best acc: 89.170000\n",
      "Epoch: [41][0/391]\tTime 0.293 (0.293)\tData 0.245 (0.245)\tLoss 0.0924 (0.0924)\tPrec 96.094% (96.094%)\n",
      "Epoch: [41][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.1344 (0.1314)\tPrec 95.312% (95.312%)\n",
      "Epoch: [41][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1096 (0.1330)\tPrec 96.875% (95.312%)\n",
      "Epoch: [41][300/391]\tTime 0.041 (0.045)\tData 0.001 (0.002)\tLoss 0.1457 (0.1386)\tPrec 95.312% (95.193%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.1723 (0.1723)\tPrec 94.531% (94.531%)\n",
      " * Prec 87.610% \n",
      "best acc: 89.170000\n",
      "Epoch: [42][0/391]\tTime 0.276 (0.276)\tData 0.232 (0.232)\tLoss 0.2487 (0.2487)\tPrec 89.844% (89.844%)\n",
      "Epoch: [42][100/391]\tTime 0.044 (0.046)\tData 0.001 (0.004)\tLoss 0.0617 (0.1226)\tPrec 99.219% (95.862%)\n",
      "Epoch: [42][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1414 (0.1299)\tPrec 96.094% (95.499%)\n",
      "Epoch: [42][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1319 (0.1310)\tPrec 96.094% (95.388%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.2295 (0.2295)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.780% \n",
      "best acc: 89.170000\n",
      "Epoch: [43][0/391]\tTime 0.311 (0.311)\tData 0.266 (0.266)\tLoss 0.1334 (0.1334)\tPrec 93.750% (93.750%)\n",
      "Epoch: [43][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1506 (0.1270)\tPrec 96.094% (95.568%)\n",
      "Epoch: [43][200/391]\tTime 0.040 (0.045)\tData 0.002 (0.003)\tLoss 0.2196 (0.1220)\tPrec 91.406% (95.709%)\n",
      "Epoch: [43][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.2573 (0.1260)\tPrec 94.531% (95.588%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.2044 (0.2044)\tPrec 94.531% (94.531%)\n",
      " * Prec 88.350% \n",
      "best acc: 89.170000\n",
      "Epoch: [44][0/391]\tTime 0.302 (0.302)\tData 0.256 (0.256)\tLoss 0.1251 (0.1251)\tPrec 95.312% (95.312%)\n",
      "Epoch: [44][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.0823 (0.1301)\tPrec 96.875% (95.343%)\n",
      "Epoch: [44][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.0968 (0.1295)\tPrec 95.312% (95.437%)\n",
      "Epoch: [44][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1016 (0.1294)\tPrec 96.094% (95.468%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.2658 (0.2658)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.140% \n",
      "best acc: 89.170000\n",
      "Epoch: [45][0/391]\tTime 0.310 (0.310)\tData 0.263 (0.263)\tLoss 0.1143 (0.1143)\tPrec 96.094% (96.094%)\n",
      "Epoch: [45][100/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.0881 (0.1234)\tPrec 98.438% (95.753%)\n",
      "Epoch: [45][200/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.0927 (0.1304)\tPrec 96.875% (95.530%)\n",
      "Epoch: [45][300/391]\tTime 0.044 (0.045)\tData 0.002 (0.002)\tLoss 0.1157 (0.1279)\tPrec 96.094% (95.601%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.2241 (0.2241)\tPrec 90.625% (90.625%)\n",
      " * Prec 89.090% \n",
      "best acc: 89.170000\n",
      "Epoch: [46][0/391]\tTime 0.277 (0.277)\tData 0.231 (0.231)\tLoss 0.0649 (0.0649)\tPrec 97.656% (97.656%)\n",
      "Epoch: [46][100/391]\tTime 0.044 (0.046)\tData 0.002 (0.004)\tLoss 0.1235 (0.1184)\tPrec 94.531% (95.893%)\n",
      "Epoch: [46][200/391]\tTime 0.045 (0.045)\tData 0.002 (0.003)\tLoss 0.0792 (0.1250)\tPrec 96.094% (95.639%)\n",
      "Epoch: [46][300/391]\tTime 0.044 (0.045)\tData 0.001 (0.002)\tLoss 0.1448 (0.1264)\tPrec 93.750% (95.627%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3175 (0.3175)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.870% \n",
      "best acc: 89.170000\n",
      "Epoch: [47][0/391]\tTime 0.250 (0.250)\tData 0.201 (0.201)\tLoss 0.1288 (0.1288)\tPrec 95.312% (95.312%)\n",
      "Epoch: [47][100/391]\tTime 0.043 (0.046)\tData 0.002 (0.004)\tLoss 0.1056 (0.1143)\tPrec 96.094% (96.148%)\n",
      "Epoch: [47][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.003)\tLoss 0.1197 (0.1200)\tPrec 96.875% (95.880%)\n",
      "Epoch: [47][300/391]\tTime 0.042 (0.045)\tData 0.001 (0.002)\tLoss 0.1228 (0.1264)\tPrec 96.094% (95.629%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3610 (0.3610)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.200% \n",
      "best acc: 89.170000\n",
      "Epoch: [48][0/391]\tTime 0.301 (0.301)\tData 0.254 (0.254)\tLoss 0.1544 (0.1544)\tPrec 93.750% (93.750%)\n",
      "Epoch: [48][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.0974 (0.1160)\tPrec 96.094% (95.970%)\n",
      "Epoch: [48][200/391]\tTime 0.044 (0.045)\tData 0.001 (0.003)\tLoss 0.1706 (0.1251)\tPrec 97.656% (95.748%)\n",
      "Epoch: [48][300/391]\tTime 0.042 (0.045)\tData 0.002 (0.002)\tLoss 0.0950 (0.1275)\tPrec 97.656% (95.614%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2738 (0.2738)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.970% \n",
      "best acc: 89.170000\n",
      "Epoch: [49][0/391]\tTime 0.350 (0.350)\tData 0.301 (0.301)\tLoss 0.2821 (0.2821)\tPrec 92.969% (92.969%)\n",
      "Epoch: [49][100/391]\tTime 0.047 (0.047)\tData 0.002 (0.005)\tLoss 0.1654 (0.1151)\tPrec 92.188% (95.900%)\n",
      "Epoch: [49][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1175 (0.1221)\tPrec 96.094% (95.592%)\n",
      "Epoch: [49][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.0997 (0.1250)\tPrec 96.094% (95.507%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.3160 (0.3160)\tPrec 85.938% (85.938%)\n",
      " * Prec 88.250% \n",
      "best acc: 89.170000\n"
     ]
    }
   ],
   "source": [
    "## Start finetuning (training here), and see how much you can recover your accuracy ##\n",
    "## You can change hyper parameters such as epochs or lr ##\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 50\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)+'_UnstructuredPruned'\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6871bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8917/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check your accuracy again after finetuning\n",
    "PATH = \"result/VGG16_quant_UnstructuredPruned/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f97244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "## Send an image and use prehook to grab the inputs of all the QuantConv2d layers\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5c9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find \"weight_int\" for features[3] ####\n",
    "w_bit = 4\n",
    "weight_q = model.features[3].weight_q\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "#print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a34b756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity level:  tensor(0.8098, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#### check your sparsity for weight_int is near 90% #####\n",
    "#### Your sparsity could be >90% after quantization #####\n",
    "sparsity_weight_int = (weight_int == 0).sum() / weight_int.nelement()\n",
    "print(\"Sparsity level: \", sparsity_weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707569b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
