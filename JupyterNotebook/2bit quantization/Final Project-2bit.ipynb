{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50435e8",
   "metadata": {},
   "source": [
    "Part1. Train VGG16 with quantization-aware training (15%)\n",
    "\n",
    " - Train for 4-bit input activation and 4-bit weight to achieve >90% accuracy. \n",
    "\n",
    " - But, this time, reduce a certain convolution layer's input channel numbers to be 8 and output channel numbers to be 8. (v)\n",
    "\n",
    " - Also, remove the batch normalization layer after the squeezed convolution. (v)\n",
    "\n",
    "  e.g., replace \"conv -> relu -> batchnorm\" with \"conv -> relu\"\n",
    "\n",
    " - This layer will be mapped on your 8x8 2D systolic array. Thus, reducing to 8 channels helps your layer's mapping in an array nicely without tiling.\n",
    "\n",
    " - This time, compute your \"psum_recovered\" such as HW5 including ReLU and compare with your prehooked input for the next layer (instead of your computed psum_ref).\n",
    "\n",
    " - [hint] It is recommended not to reduce the input channel of Conv layer at too early layer position because the early layer's feature map size (nij) is large incurring long verification cycles.\n",
    "\n",
    "   (recommended location: around 27-th layer, e.g., features[27] for VGGNet)\n",
    "\n",
    " - Measure of success: accuracy >90%  with 8 input/output channels + error < 10^-3 for psum_recorvered for VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [100, 200,300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34dca17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.862 (1.862)\tData 0.444 (0.444)\tLoss 2.4682 (2.4682)\tPrec 7.812% (7.812%)\n",
      "Epoch: [0][100/391]\tTime 0.045 (0.063)\tData 0.002 (0.006)\tLoss 2.3877 (2.4142)\tPrec 11.719% (14.534%)\n",
      "Epoch: [0][200/391]\tTime 0.045 (0.054)\tData 0.002 (0.004)\tLoss 2.0488 (2.2710)\tPrec 21.094% (16.834%)\n",
      "Epoch: [0][300/391]\tTime 0.045 (0.051)\tData 0.002 (0.003)\tLoss 2.0191 (2.2028)\tPrec 16.406% (18.246%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.316 (0.316)\tLoss 2.0058 (2.0058)\tPrec 17.188% (17.188%)\n",
      " * Prec 23.160% \n",
      "best acc: 23.160000\n",
      "Epoch: [1][0/391]\tTime 0.384 (0.384)\tData 0.347 (0.347)\tLoss 1.9399 (1.9399)\tPrec 23.438% (23.438%)\n",
      "Epoch: [1][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 2.0678 (1.9813)\tPrec 22.656% (24.134%)\n",
      "Epoch: [1][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 2.0899 (1.9734)\tPrec 17.969% (24.056%)\n",
      "Epoch: [1][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 2.0215 (1.9588)\tPrec 22.656% (24.647%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.393 (0.393)\tLoss 2.0877 (2.0877)\tPrec 15.625% (15.625%)\n",
      " * Prec 24.110% \n",
      "best acc: 24.110000\n",
      "Epoch: [2][0/391]\tTime 0.413 (0.413)\tData 0.370 (0.370)\tLoss 1.9344 (1.9344)\tPrec 30.469% (30.469%)\n",
      "Epoch: [2][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.9223 (1.9137)\tPrec 28.906% (27.669%)\n",
      "Epoch: [2][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.8390 (1.9054)\tPrec 25.781% (27.853%)\n",
      "Epoch: [2][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.8007 (1.9070)\tPrec 34.375% (27.611%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.302 (0.302)\tLoss 1.8546 (1.8546)\tPrec 34.375% (34.375%)\n",
      " * Prec 29.070% \n",
      "best acc: 29.070000\n",
      "Epoch: [3][0/391]\tTime 0.377 (0.377)\tData 0.333 (0.333)\tLoss 1.7295 (1.7295)\tPrec 30.469% (30.469%)\n",
      "Epoch: [3][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.6599 (1.8290)\tPrec 42.188% (31.443%)\n",
      "Epoch: [3][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.8551 (1.8153)\tPrec 30.469% (32.128%)\n",
      "Epoch: [3][300/391]\tTime 0.042 (0.047)\tData 0.002 (0.003)\tLoss 1.7470 (1.8051)\tPrec 30.469% (32.556%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.315 (0.315)\tLoss 1.8384 (1.8384)\tPrec 32.812% (32.812%)\n",
      " * Prec 33.360% \n",
      "best acc: 33.360000\n",
      "Epoch: [4][0/391]\tTime 0.503 (0.503)\tData 0.466 (0.466)\tLoss 1.8532 (1.8532)\tPrec 29.688% (29.688%)\n",
      "Epoch: [4][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.007)\tLoss 1.8736 (1.7452)\tPrec 36.719% (35.071%)\n",
      "Epoch: [4][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 1.8643 (1.7415)\tPrec 37.500% (35.417%)\n",
      "Epoch: [4][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.6968 (1.7319)\tPrec 37.500% (35.979%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.328 (0.328)\tLoss 2.0682 (2.0682)\tPrec 32.812% (32.812%)\n",
      " * Prec 31.950% \n",
      "best acc: 33.360000\n",
      "Epoch: [5][0/391]\tTime 0.485 (0.485)\tData 0.447 (0.447)\tLoss 1.7007 (1.7007)\tPrec 38.281% (38.281%)\n",
      "Epoch: [5][100/391]\tTime 0.079 (0.055)\tData 0.002 (0.007)\tLoss 1.6467 (1.6845)\tPrec 40.625% (38.753%)\n",
      "Epoch: [5][200/391]\tTime 0.045 (0.055)\tData 0.002 (0.005)\tLoss 1.7647 (1.6770)\tPrec 34.375% (38.437%)\n",
      "Epoch: [5][300/391]\tTime 0.045 (0.051)\tData 0.002 (0.004)\tLoss 1.5239 (1.6706)\tPrec 45.312% (38.458%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.305 (0.305)\tLoss 1.8206 (1.8206)\tPrec 38.281% (38.281%)\n",
      " * Prec 38.680% \n",
      "best acc: 38.680000\n",
      "Epoch: [6][0/391]\tTime 0.354 (0.354)\tData 0.318 (0.318)\tLoss 1.4593 (1.4593)\tPrec 45.312% (45.312%)\n",
      "Epoch: [6][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.5464 (1.6176)\tPrec 43.750% (40.640%)\n",
      "Epoch: [6][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.4978 (1.6146)\tPrec 42.969% (41.290%)\n",
      "Epoch: [6][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 1.5611 (1.6129)\tPrec 43.750% (41.393%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.332 (0.332)\tLoss 1.6238 (1.6238)\tPrec 45.312% (45.312%)\n",
      " * Prec 43.710% \n",
      "best acc: 43.710000\n",
      "Epoch: [7][0/391]\tTime 0.454 (0.454)\tData 0.417 (0.417)\tLoss 1.4972 (1.4972)\tPrec 46.094% (46.094%)\n",
      "Epoch: [7][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 1.6736 (1.5727)\tPrec 41.406% (43.209%)\n",
      "Epoch: [7][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 1.6267 (1.5661)\tPrec 46.094% (43.633%)\n",
      "Epoch: [7][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 1.5077 (1.5561)\tPrec 44.531% (43.888%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.385 (0.385)\tLoss 1.5557 (1.5557)\tPrec 46.875% (46.875%)\n",
      " * Prec 46.430% \n",
      "best acc: 46.430000\n",
      "Epoch: [8][0/391]\tTime 0.469 (0.469)\tData 0.433 (0.433)\tLoss 1.4992 (1.4992)\tPrec 50.000% (50.000%)\n",
      "Epoch: [8][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 1.3935 (1.4927)\tPrec 53.125% (46.341%)\n",
      "Epoch: [8][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 1.3256 (1.4903)\tPrec 57.031% (46.475%)\n",
      "Epoch: [8][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 1.4480 (1.4836)\tPrec 52.344% (46.758%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.363 (0.363)\tLoss 1.5392 (1.5392)\tPrec 40.625% (40.625%)\n",
      " * Prec 49.710% \n",
      "best acc: 49.710000\n",
      "Epoch: [9][0/391]\tTime 0.512 (0.512)\tData 0.466 (0.466)\tLoss 1.2035 (1.2035)\tPrec 56.250% (56.250%)\n",
      "Epoch: [9][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 1.5312 (1.4373)\tPrec 45.312% (48.368%)\n",
      "Epoch: [9][200/391]\tTime 0.045 (0.048)\tData 0.004 (0.004)\tLoss 1.3528 (1.4229)\tPrec 47.656% (48.935%)\n",
      "Epoch: [9][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.2444 (1.4134)\tPrec 50.781% (49.525%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.312 (0.312)\tLoss 1.5989 (1.5989)\tPrec 46.094% (46.094%)\n",
      " * Prec 48.300% \n",
      "best acc: 49.710000\n",
      "Epoch: [10][0/391]\tTime 0.384 (0.384)\tData 0.348 (0.348)\tLoss 1.3159 (1.3159)\tPrec 50.781% (50.781%)\n",
      "Epoch: [10][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.5754 (1.3794)\tPrec 44.531% (50.750%)\n",
      "Epoch: [10][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.4029 (1.3615)\tPrec 52.344% (51.442%)\n",
      "Epoch: [10][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 1.3585 (1.3566)\tPrec 58.594% (51.695%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.337 (0.337)\tLoss 1.3037 (1.3037)\tPrec 52.344% (52.344%)\n",
      " * Prec 52.430% \n",
      "best acc: 52.430000\n",
      "Epoch: [11][0/391]\tTime 0.353 (0.353)\tData 0.316 (0.316)\tLoss 1.2046 (1.2046)\tPrec 54.688% (54.688%)\n",
      "Epoch: [11][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.3381 (1.3090)\tPrec 46.875% (52.715%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.2674 (1.2990)\tPrec 57.812% (53.436%)\n",
      "Epoch: [11][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 1.3203 (1.2906)\tPrec 57.812% (53.971%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 1.3159 (1.3159)\tPrec 53.906% (53.906%)\n",
      " * Prec 56.140% \n",
      "best acc: 56.140000\n",
      "Epoch: [12][0/391]\tTime 0.404 (0.404)\tData 0.362 (0.362)\tLoss 1.2453 (1.2453)\tPrec 53.125% (53.125%)\n",
      "Epoch: [12][100/391]\tTime 0.045 (0.049)\tData 0.005 (0.007)\tLoss 1.1956 (1.2531)\tPrec 57.031% (55.376%)\n",
      "Epoch: [12][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 1.1266 (1.2548)\tPrec 59.375% (55.442%)\n",
      "Epoch: [12][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.4304 (1.2518)\tPrec 47.656% (55.593%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.322 (0.322)\tLoss 1.2950 (1.2950)\tPrec 54.688% (54.688%)\n",
      " * Prec 53.050% \n",
      "best acc: 56.140000\n",
      "Epoch: [13][0/391]\tTime 0.416 (0.416)\tData 0.379 (0.379)\tLoss 1.3254 (1.3254)\tPrec 57.812% (57.812%)\n",
      "Epoch: [13][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 1.1123 (1.1904)\tPrec 61.719% (57.611%)\n",
      "Epoch: [13][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 1.1565 (1.1961)\tPrec 53.125% (57.362%)\n",
      "Epoch: [13][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.3382 (1.1976)\tPrec 55.469% (57.374%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 1.3520 (1.3520)\tPrec 53.906% (53.906%)\n",
      " * Prec 54.040% \n",
      "best acc: 56.140000\n",
      "Epoch: [14][0/391]\tTime 0.373 (0.373)\tData 0.321 (0.321)\tLoss 1.3151 (1.3151)\tPrec 52.344% (52.344%)\n",
      "Epoch: [14][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 1.1159 (1.1678)\tPrec 57.812% (58.168%)\n",
      "Epoch: [14][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.0994 (1.1728)\tPrec 65.625% (58.388%)\n",
      "Epoch: [14][300/391]\tTime 0.043 (0.047)\tData 0.005 (0.003)\tLoss 1.2375 (1.1667)\tPrec 57.031% (58.539%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 1.2580 (1.2580)\tPrec 57.031% (57.031%)\n",
      " * Prec 57.320% \n",
      "best acc: 57.320000\n",
      "Epoch: [15][0/391]\tTime 0.366 (0.366)\tData 0.330 (0.330)\tLoss 1.0889 (1.0889)\tPrec 61.719% (61.719%)\n",
      "Epoch: [15][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 1.1964 (1.1285)\tPrec 57.031% (60.195%)\n",
      "Epoch: [15][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.1359 (1.1381)\tPrec 58.594% (60.028%)\n",
      "Epoch: [15][300/391]\tTime 0.046 (0.046)\tData 0.003 (0.003)\tLoss 1.1829 (1.1286)\tPrec 62.500% (60.294%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.344 (0.344)\tLoss 1.2837 (1.2837)\tPrec 57.812% (57.812%)\n",
      " * Prec 57.140% \n",
      "best acc: 57.320000\n",
      "Epoch: [16][0/391]\tTime 0.391 (0.391)\tData 0.342 (0.342)\tLoss 0.9277 (0.9277)\tPrec 64.062% (64.062%)\n",
      "Epoch: [16][100/391]\tTime 0.045 (0.049)\tData 0.007 (0.006)\tLoss 1.1451 (1.1052)\tPrec 60.156% (60.999%)\n",
      "Epoch: [16][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.9723 (1.0997)\tPrec 69.531% (61.427%)\n",
      "Epoch: [16][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 1.1418 (1.1079)\tPrec 64.062% (61.278%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 1.1861 (1.1861)\tPrec 54.688% (54.688%)\n",
      " * Prec 56.820% \n",
      "best acc: 57.320000\n",
      "Epoch: [17][0/391]\tTime 0.361 (0.361)\tData 0.325 (0.325)\tLoss 1.0759 (1.0759)\tPrec 60.938% (60.938%)\n",
      "Epoch: [17][100/391]\tTime 0.045 (0.049)\tData 0.001 (0.005)\tLoss 1.1835 (1.0778)\tPrec 56.250% (62.144%)\n",
      "Epoch: [17][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 1.0851 (1.0695)\tPrec 63.281% (62.539%)\n",
      "Epoch: [17][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.0884 (1.0674)\tPrec 66.406% (62.614%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.274 (0.274)\tLoss 1.0410 (1.0410)\tPrec 64.062% (64.062%)\n",
      " * Prec 61.660% \n",
      "best acc: 61.660000\n",
      "Epoch: [18][0/391]\tTime 0.447 (0.447)\tData 0.405 (0.405)\tLoss 0.9278 (0.9278)\tPrec 68.750% (68.750%)\n",
      "Epoch: [18][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 1.0415 (1.0491)\tPrec 63.281% (63.335%)\n",
      "Epoch: [18][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.9244 (1.0600)\tPrec 67.969% (63.009%)\n",
      "Epoch: [18][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 1.0334 (1.0616)\tPrec 68.750% (63.113%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.327 (0.327)\tLoss 1.1138 (1.1138)\tPrec 60.938% (60.938%)\n",
      " * Prec 63.900% \n",
      "best acc: 63.900000\n",
      "Epoch: [19][0/391]\tTime 0.456 (0.456)\tData 0.420 (0.420)\tLoss 0.9644 (0.9644)\tPrec 67.969% (67.969%)\n",
      "Epoch: [19][100/391]\tTime 0.050 (0.050)\tData 0.005 (0.006)\tLoss 0.9171 (1.0187)\tPrec 71.875% (64.573%)\n",
      "Epoch: [19][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.2701 (1.0067)\tPrec 60.938% (65.139%)\n",
      "Epoch: [19][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 1.0004 (1.0134)\tPrec 64.062% (64.932%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.9676 (0.9676)\tPrec 66.406% (66.406%)\n",
      " * Prec 63.970% \n",
      "best acc: 63.970000\n",
      "Epoch: [20][0/391]\tTime 0.384 (0.384)\tData 0.345 (0.345)\tLoss 1.0793 (1.0793)\tPrec 61.719% (61.719%)\n",
      "Epoch: [20][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.005)\tLoss 0.9682 (0.9843)\tPrec 65.625% (66.089%)\n",
      "Epoch: [20][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.9591 (0.9820)\tPrec 64.062% (66.033%)\n",
      "Epoch: [20][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.8341 (0.9775)\tPrec 70.312% (66.131%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.411 (0.411)\tLoss 1.0198 (1.0198)\tPrec 60.938% (60.938%)\n",
      " * Prec 63.520% \n",
      "best acc: 63.970000\n",
      "Epoch: [21][0/391]\tTime 0.527 (0.527)\tData 0.487 (0.487)\tLoss 0.9812 (0.9812)\tPrec 67.188% (67.188%)\n",
      "Epoch: [21][100/391]\tTime 0.046 (0.051)\tData 0.002 (0.008)\tLoss 0.9518 (0.9670)\tPrec 68.750% (66.429%)\n",
      "Epoch: [21][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.005)\tLoss 0.9542 (0.9595)\tPrec 68.750% (66.659%)\n",
      "Epoch: [21][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.8705 (0.9541)\tPrec 69.531% (66.858%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 1.1037 (1.1037)\tPrec 60.156% (60.156%)\n",
      " * Prec 63.600% \n",
      "best acc: 63.970000\n",
      "Epoch: [22][0/391]\tTime 0.404 (0.404)\tData 0.362 (0.362)\tLoss 1.1859 (1.1859)\tPrec 57.812% (57.812%)\n",
      "Epoch: [22][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.8513 (0.9546)\tPrec 71.094% (67.590%)\n",
      "Epoch: [22][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.8514 (0.9448)\tPrec 66.406% (67.813%)\n",
      "Epoch: [22][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.6865 (0.9400)\tPrec 74.219% (67.899%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.358 (0.358)\tLoss 0.8439 (0.8439)\tPrec 68.750% (68.750%)\n",
      " * Prec 66.130% \n",
      "best acc: 66.130000\n",
      "Epoch: [23][0/391]\tTime 0.457 (0.457)\tData 0.421 (0.421)\tLoss 0.8178 (0.8178)\tPrec 73.438% (73.438%)\n",
      "Epoch: [23][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.006)\tLoss 0.8691 (0.8896)\tPrec 65.625% (69.500%)\n",
      "Epoch: [23][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 1.0312 (0.8924)\tPrec 64.844% (69.415%)\n",
      "Epoch: [23][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 1.0630 (0.9002)\tPrec 60.156% (69.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.350 (0.350)\tLoss 0.8842 (0.8842)\tPrec 64.844% (64.844%)\n",
      " * Prec 69.360% \n",
      "best acc: 69.360000\n",
      "Epoch: [24][0/391]\tTime 0.356 (0.356)\tData 0.320 (0.320)\tLoss 0.8725 (0.8725)\tPrec 67.969% (67.969%)\n",
      "Epoch: [24][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 1.0429 (0.8711)\tPrec 63.281% (69.910%)\n",
      "Epoch: [24][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.9113 (0.8587)\tPrec 69.531% (70.394%)\n",
      "Epoch: [24][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 1.0152 (0.8668)\tPrec 64.844% (70.092%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.360 (0.360)\tLoss 0.8547 (0.8547)\tPrec 71.094% (71.094%)\n",
      " * Prec 67.240% \n",
      "best acc: 69.360000\n",
      "Epoch: [25][0/391]\tTime 0.446 (0.446)\tData 0.407 (0.407)\tLoss 0.9227 (0.9227)\tPrec 65.625% (65.625%)\n",
      "Epoch: [25][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.9890 (0.8700)\tPrec 60.938% (70.374%)\n",
      "Epoch: [25][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7767 (0.8616)\tPrec 75.781% (70.682%)\n",
      "Epoch: [25][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.7725 (0.8565)\tPrec 77.344% (70.826%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.332 (0.332)\tLoss 0.6893 (0.6893)\tPrec 75.781% (75.781%)\n",
      " * Prec 70.010% \n",
      "best acc: 70.010000\n",
      "Epoch: [26][0/391]\tTime 0.407 (0.407)\tData 0.353 (0.353)\tLoss 0.9464 (0.9464)\tPrec 65.625% (65.625%)\n",
      "Epoch: [26][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.006)\tLoss 0.8394 (0.8366)\tPrec 71.094% (71.264%)\n",
      "Epoch: [26][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.9858 (0.8351)\tPrec 67.969% (71.611%)\n",
      "Epoch: [26][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.003)\tLoss 0.8962 (0.8336)\tPrec 66.406% (71.753%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.417 (0.417)\tLoss 0.7693 (0.7693)\tPrec 76.562% (76.562%)\n",
      " * Prec 70.340% \n",
      "best acc: 70.340000\n",
      "Epoch: [27][0/391]\tTime 0.567 (0.567)\tData 0.531 (0.531)\tLoss 0.8346 (0.8346)\tPrec 74.219% (74.219%)\n",
      "Epoch: [27][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.007)\tLoss 0.8811 (0.8246)\tPrec 67.969% (71.999%)\n",
      "Epoch: [27][200/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.8056 (0.8211)\tPrec 71.094% (72.112%)\n",
      "Epoch: [27][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.8531 (0.8140)\tPrec 73.438% (72.353%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.402 (0.402)\tLoss 0.7449 (0.7449)\tPrec 72.656% (72.656%)\n",
      " * Prec 71.280% \n",
      "best acc: 71.280000\n",
      "Epoch: [28][0/391]\tTime 0.433 (0.433)\tData 0.397 (0.397)\tLoss 0.9019 (0.9019)\tPrec 68.750% (68.750%)\n",
      "Epoch: [28][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 0.6841 (0.7844)\tPrec 76.562% (73.453%)\n",
      "Epoch: [28][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.9517 (0.7946)\tPrec 68.750% (72.924%)\n",
      "Epoch: [28][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.7809 (0.7939)\tPrec 72.656% (72.970%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 0.8091 (0.8091)\tPrec 71.094% (71.094%)\n",
      " * Prec 68.930% \n",
      "best acc: 71.280000\n",
      "Epoch: [29][0/391]\tTime 0.499 (0.499)\tData 0.447 (0.447)\tLoss 0.9029 (0.9029)\tPrec 71.094% (71.094%)\n",
      "Epoch: [29][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 0.8715 (0.8044)\tPrec 65.625% (72.556%)\n",
      "Epoch: [29][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.9066 (0.7835)\tPrec 71.094% (73.403%)\n",
      "Epoch: [29][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.004)\tLoss 0.7580 (0.7869)\tPrec 82.031% (73.367%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.416 (0.416)\tLoss 0.8968 (0.8968)\tPrec 74.219% (74.219%)\n",
      " * Prec 70.650% \n",
      "best acc: 71.280000\n",
      "Epoch: [30][0/391]\tTime 0.466 (0.466)\tData 0.430 (0.430)\tLoss 0.6227 (0.6227)\tPrec 76.562% (76.562%)\n",
      "Epoch: [30][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.7411 (0.7670)\tPrec 76.562% (73.677%)\n",
      "Epoch: [30][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6587 (0.7626)\tPrec 79.688% (74.013%)\n",
      "Epoch: [30][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.8318 (0.7657)\tPrec 72.656% (73.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.301 (0.301)\tLoss 0.7962 (0.7962)\tPrec 71.094% (71.094%)\n",
      " * Prec 71.700% \n",
      "best acc: 71.700000\n",
      "Epoch: [31][0/391]\tTime 0.390 (0.390)\tData 0.341 (0.341)\tLoss 0.7505 (0.7505)\tPrec 76.562% (76.562%)\n",
      "Epoch: [31][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6864 (0.7465)\tPrec 75.781% (74.513%)\n",
      "Epoch: [31][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7993 (0.7521)\tPrec 71.875% (74.495%)\n",
      "Epoch: [31][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.8751 (0.7584)\tPrec 71.094% (74.400%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.7503 (0.7503)\tPrec 72.656% (72.656%)\n",
      " * Prec 72.810% \n",
      "best acc: 72.810000\n",
      "Epoch: [32][0/391]\tTime 0.417 (0.417)\tData 0.375 (0.375)\tLoss 0.6857 (0.6857)\tPrec 74.219% (74.219%)\n",
      "Epoch: [32][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6924 (0.7423)\tPrec 76.562% (75.039%)\n",
      "Epoch: [32][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6611 (0.7330)\tPrec 80.469% (75.295%)\n",
      "Epoch: [32][300/391]\tTime 0.045 (0.046)\tData 0.003 (0.003)\tLoss 0.9664 (0.7354)\tPrec 63.281% (75.145%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.298 (0.298)\tLoss 0.7783 (0.7783)\tPrec 77.344% (77.344%)\n",
      " * Prec 72.430% \n",
      "best acc: 72.810000\n",
      "Epoch: [33][0/391]\tTime 0.396 (0.396)\tData 0.361 (0.361)\tLoss 0.6599 (0.6599)\tPrec 78.125% (78.125%)\n",
      "Epoch: [33][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.8096 (0.7294)\tPrec 74.219% (75.750%)\n",
      "Epoch: [33][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7852 (0.7302)\tPrec 75.000% (75.474%)\n",
      "Epoch: [33][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.6720 (0.7233)\tPrec 79.688% (75.675%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.347 (0.347)\tLoss 0.8580 (0.8580)\tPrec 71.094% (71.094%)\n",
      " * Prec 70.760% \n",
      "best acc: 72.810000\n",
      "Epoch: [34][0/391]\tTime 0.367 (0.367)\tData 0.330 (0.330)\tLoss 0.7810 (0.7810)\tPrec 75.000% (75.000%)\n",
      "Epoch: [34][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.7790 (0.7177)\tPrec 72.656% (76.122%)\n",
      "Epoch: [34][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6692 (0.7132)\tPrec 77.344% (76.248%)\n",
      "Epoch: [34][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.8501 (0.7081)\tPrec 73.438% (76.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.7244 (0.7244)\tPrec 74.219% (74.219%)\n",
      " * Prec 72.980% \n",
      "best acc: 72.980000\n",
      "Epoch: [35][0/391]\tTime 0.374 (0.374)\tData 0.336 (0.336)\tLoss 0.7257 (0.7257)\tPrec 77.344% (77.344%)\n",
      "Epoch: [35][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6280 (0.7067)\tPrec 78.125% (76.230%)\n",
      "Epoch: [35][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7042 (0.6995)\tPrec 76.562% (76.582%)\n",
      "Epoch: [35][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5873 (0.6992)\tPrec 79.688% (76.482%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.456 (0.456)\tLoss 0.7665 (0.7665)\tPrec 75.781% (75.781%)\n",
      " * Prec 71.750% \n",
      "best acc: 72.980000\n",
      "Epoch: [36][0/391]\tTime 0.387 (0.387)\tData 0.351 (0.351)\tLoss 0.6532 (0.6532)\tPrec 76.562% (76.562%)\n",
      "Epoch: [36][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.7210 (0.6806)\tPrec 75.781% (77.112%)\n",
      "Epoch: [36][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7173 (0.6898)\tPrec 76.562% (76.873%)\n",
      "Epoch: [36][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.003)\tLoss 0.5099 (0.6919)\tPrec 81.250% (76.773%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.445 (0.445)\tLoss 0.7288 (0.7288)\tPrec 75.000% (75.000%)\n",
      " * Prec 72.800% \n",
      "best acc: 72.980000\n",
      "Epoch: [37][0/391]\tTime 0.361 (0.361)\tData 0.325 (0.325)\tLoss 0.6851 (0.6851)\tPrec 81.250% (81.250%)\n",
      "Epoch: [37][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.5959 (0.6660)\tPrec 82.031% (78.125%)\n",
      "Epoch: [37][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5787 (0.6748)\tPrec 80.469% (77.752%)\n",
      "Epoch: [37][300/391]\tTime 0.045 (0.047)\tData 0.005 (0.004)\tLoss 0.6614 (0.6803)\tPrec 74.219% (77.429%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.356 (0.356)\tLoss 0.6999 (0.6999)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.050% \n",
      "best acc: 76.050000\n",
      "Epoch: [38][0/391]\tTime 0.417 (0.417)\tData 0.381 (0.381)\tLoss 0.4818 (0.4818)\tPrec 85.938% (85.938%)\n",
      "Epoch: [38][100/391]\tTime 0.045 (0.049)\tData 0.001 (0.006)\tLoss 0.4554 (0.6708)\tPrec 83.594% (77.259%)\n",
      "Epoch: [38][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.004)\tLoss 0.6993 (0.6721)\tPrec 75.000% (77.418%)\n",
      "Epoch: [38][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.7342 (0.6736)\tPrec 76.562% (77.349%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.355 (0.355)\tLoss 0.6022 (0.6022)\tPrec 81.250% (81.250%)\n",
      " * Prec 77.280% \n",
      "best acc: 77.280000\n",
      "Epoch: [39][0/391]\tTime 0.645 (0.645)\tData 0.601 (0.601)\tLoss 0.7200 (0.7200)\tPrec 71.875% (71.875%)\n",
      "Epoch: [39][100/391]\tTime 0.045 (0.055)\tData 0.003 (0.013)\tLoss 0.7712 (0.6569)\tPrec 72.656% (77.800%)\n",
      "Epoch: [39][200/391]\tTime 0.045 (0.051)\tData 0.001 (0.009)\tLoss 0.6650 (0.6541)\tPrec 78.125% (77.989%)\n",
      "Epoch: [39][300/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 0.7085 (0.6589)\tPrec 80.469% (77.891%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.347 (0.347)\tLoss 0.7534 (0.7534)\tPrec 74.219% (74.219%)\n",
      " * Prec 74.910% \n",
      "best acc: 77.280000\n",
      "Epoch: [40][0/391]\tTime 0.412 (0.412)\tData 0.375 (0.375)\tLoss 0.6494 (0.6494)\tPrec 80.469% (80.469%)\n",
      "Epoch: [40][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.006)\tLoss 0.6153 (0.6419)\tPrec 78.906% (78.450%)\n",
      "Epoch: [40][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.5490 (0.6571)\tPrec 79.688% (78.024%)\n",
      "Epoch: [40][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.6158 (0.6505)\tPrec 84.375% (78.390%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.282 (0.282)\tLoss 0.6669 (0.6669)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.710% \n",
      "best acc: 77.280000\n",
      "Epoch: [41][0/391]\tTime 0.389 (0.389)\tData 0.351 (0.351)\tLoss 0.6928 (0.6928)\tPrec 80.469% (80.469%)\n",
      "Epoch: [41][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6470 (0.6399)\tPrec 75.781% (79.015%)\n",
      "Epoch: [41][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6840 (0.6362)\tPrec 78.906% (78.945%)\n",
      "Epoch: [41][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.7332 (0.6464)\tPrec 75.781% (78.540%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.409 (0.409)\tLoss 0.7047 (0.7047)\tPrec 74.219% (74.219%)\n",
      " * Prec 76.240% \n",
      "best acc: 77.280000\n",
      "Epoch: [42][0/391]\tTime 0.353 (0.353)\tData 0.315 (0.315)\tLoss 0.6375 (0.6375)\tPrec 77.344% (77.344%)\n",
      "Epoch: [42][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6798 (0.6559)\tPrec 77.344% (78.326%)\n",
      "Epoch: [42][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.8581 (0.6679)\tPrec 71.875% (77.725%)\n",
      "Epoch: [42][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5102 (0.6676)\tPrec 82.812% (77.694%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.342 (0.342)\tLoss 0.7480 (0.7480)\tPrec 74.219% (74.219%)\n",
      " * Prec 74.160% \n",
      "best acc: 77.280000\n",
      "Epoch: [43][0/391]\tTime 0.414 (0.414)\tData 0.372 (0.372)\tLoss 0.7347 (0.7347)\tPrec 73.438% (73.438%)\n",
      "Epoch: [43][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.8439 (0.6467)\tPrec 71.875% (78.388%)\n",
      "Epoch: [43][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5724 (0.6498)\tPrec 81.250% (78.269%)\n",
      "Epoch: [43][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5816 (0.6480)\tPrec 78.906% (78.353%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 0.6845 (0.6845)\tPrec 75.781% (75.781%)\n",
      " * Prec 74.950% \n",
      "best acc: 77.280000\n",
      "Epoch: [44][0/391]\tTime 0.338 (0.338)\tData 0.302 (0.302)\tLoss 0.6041 (0.6041)\tPrec 82.812% (82.812%)\n",
      "Epoch: [44][100/391]\tTime 0.042 (0.048)\tData 0.002 (0.005)\tLoss 0.6464 (0.6244)\tPrec 80.469% (78.899%)\n",
      "Epoch: [44][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.6591 (0.6236)\tPrec 80.469% (79.031%)\n",
      "Epoch: [44][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.4978 (0.6192)\tPrec 83.594% (79.373%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.332 (0.332)\tLoss 0.6682 (0.6682)\tPrec 76.562% (76.562%)\n",
      " * Prec 76.070% \n",
      "best acc: 77.280000\n",
      "Epoch: [45][0/391]\tTime 0.388 (0.388)\tData 0.352 (0.352)\tLoss 0.6798 (0.6798)\tPrec 78.125% (78.125%)\n",
      "Epoch: [45][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6425 (0.5917)\tPrec 78.125% (79.989%)\n",
      "Epoch: [45][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5408 (0.5946)\tPrec 82.812% (80.138%)\n",
      "Epoch: [45][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.7506 (0.5977)\tPrec 78.906% (80.085%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 0.6224 (0.6224)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.800% \n",
      "best acc: 78.800000\n",
      "Epoch: [46][0/391]\tTime 0.408 (0.408)\tData 0.371 (0.371)\tLoss 0.5120 (0.5120)\tPrec 82.031% (82.031%)\n",
      "Epoch: [46][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 0.6338 (0.5818)\tPrec 78.906% (80.701%)\n",
      "Epoch: [46][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.5066 (0.5841)\tPrec 84.375% (80.496%)\n",
      "Epoch: [46][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5796 (0.5881)\tPrec 80.469% (80.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.378 (0.378)\tLoss 0.5443 (0.5443)\tPrec 83.594% (83.594%)\n",
      " * Prec 77.740% \n",
      "best acc: 78.800000\n",
      "Epoch: [47][0/391]\tTime 0.509 (0.509)\tData 0.466 (0.466)\tLoss 0.5940 (0.5940)\tPrec 80.469% (80.469%)\n",
      "Epoch: [47][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.4500 (0.5541)\tPrec 88.281% (81.219%)\n",
      "Epoch: [47][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.6283 (0.5650)\tPrec 81.250% (81.149%)\n",
      "Epoch: [47][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.4562 (0.5740)\tPrec 82.812% (80.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.305 (0.305)\tLoss 0.8084 (0.8084)\tPrec 75.781% (75.781%)\n",
      " * Prec 76.730% \n",
      "best acc: 78.800000\n",
      "Epoch: [48][0/391]\tTime 0.348 (0.348)\tData 0.304 (0.304)\tLoss 0.4012 (0.4012)\tPrec 87.500% (87.500%)\n",
      "Epoch: [48][100/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.4141 (0.5451)\tPrec 86.719% (82.024%)\n",
      "Epoch: [48][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6272 (0.5474)\tPrec 80.469% (81.864%)\n",
      "Epoch: [48][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5100 (0.5503)\tPrec 84.375% (81.696%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.413 (0.413)\tLoss 0.5928 (0.5928)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.670% \n",
      "best acc: 78.800000\n",
      "Epoch: [49][0/391]\tTime 0.351 (0.351)\tData 0.314 (0.314)\tLoss 0.5363 (0.5363)\tPrec 85.156% (85.156%)\n",
      "Epoch: [49][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.6668 (0.5343)\tPrec 77.344% (81.985%)\n",
      "Epoch: [49][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.5610 (0.5434)\tPrec 78.125% (81.662%)\n",
      "Epoch: [49][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5716 (0.5459)\tPrec 78.906% (81.707%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.374 (0.374)\tLoss 0.7016 (0.7016)\tPrec 75.000% (75.000%)\n",
      " * Prec 76.390% \n",
      "best acc: 78.800000\n",
      "Epoch: [50][0/391]\tTime 0.338 (0.338)\tData 0.301 (0.301)\tLoss 0.6312 (0.6312)\tPrec 79.688% (79.688%)\n",
      "Epoch: [50][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.5131 (0.5304)\tPrec 85.156% (82.464%)\n",
      "Epoch: [50][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4976 (0.5330)\tPrec 85.156% (82.393%)\n",
      "Epoch: [50][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5865 (0.5370)\tPrec 79.688% (82.260%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.6544 (0.6544)\tPrec 78.906% (78.906%)\n",
      " * Prec 76.130% \n",
      "best acc: 78.800000\n",
      "Epoch: [51][0/391]\tTime 0.485 (0.485)\tData 0.448 (0.448)\tLoss 0.7949 (0.7949)\tPrec 78.906% (78.906%)\n",
      "Epoch: [51][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.4802 (0.5326)\tPrec 82.812% (82.279%)\n",
      "Epoch: [51][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5040 (0.5359)\tPrec 82.812% (82.296%)\n",
      "Epoch: [51][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5754 (0.5354)\tPrec 75.000% (82.423%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 0.6728 (0.6728)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.810% \n",
      "best acc: 78.810000\n",
      "Epoch: [52][0/391]\tTime 0.361 (0.361)\tData 0.325 (0.325)\tLoss 0.4412 (0.4412)\tPrec 86.719% (86.719%)\n",
      "Epoch: [52][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.5216 (0.5321)\tPrec 80.469% (82.186%)\n",
      "Epoch: [52][200/391]\tTime 0.048 (0.047)\tData 0.016 (0.005)\tLoss 0.5020 (0.5259)\tPrec 83.594% (82.490%)\n",
      "Epoch: [52][300/391]\tTime 0.045 (0.047)\tData 0.004 (0.005)\tLoss 0.4647 (0.5251)\tPrec 85.156% (82.485%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.450 (0.450)\tLoss 0.5015 (0.5015)\tPrec 85.156% (85.156%)\n",
      " * Prec 79.270% \n",
      "best acc: 79.270000\n",
      "Epoch: [53][0/391]\tTime 0.345 (0.345)\tData 0.310 (0.310)\tLoss 0.3892 (0.3892)\tPrec 85.938% (85.938%)\n",
      "Epoch: [53][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.6295 (0.5231)\tPrec 81.250% (82.511%)\n",
      "Epoch: [53][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4525 (0.5218)\tPrec 85.938% (82.591%)\n",
      "Epoch: [53][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.5781 (0.5212)\tPrec 82.812% (82.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.522 (0.522)\tLoss 0.6053 (0.6053)\tPrec 79.688% (79.688%)\n",
      " * Prec 79.560% \n",
      "best acc: 79.560000\n",
      "Epoch: [54][0/391]\tTime 0.357 (0.357)\tData 0.319 (0.319)\tLoss 0.4929 (0.4929)\tPrec 81.250% (81.250%)\n",
      "Epoch: [54][100/391]\tTime 0.045 (0.049)\tData 0.001 (0.006)\tLoss 0.6160 (0.5101)\tPrec 79.688% (82.913%)\n",
      "Epoch: [54][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.4936 (0.5098)\tPrec 85.156% (83.178%)\n",
      "Epoch: [54][300/391]\tTime 0.045 (0.051)\tData 0.002 (0.003)\tLoss 0.5437 (0.5127)\tPrec 76.562% (82.906%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.392 (0.392)\tLoss 0.7466 (0.7466)\tPrec 72.656% (72.656%)\n",
      " * Prec 77.830% \n",
      "best acc: 79.560000\n",
      "Epoch: [55][0/391]\tTime 0.418 (0.418)\tData 0.383 (0.383)\tLoss 0.4629 (0.4629)\tPrec 84.375% (84.375%)\n",
      "Epoch: [55][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.3932 (0.5019)\tPrec 86.719% (83.416%)\n",
      "Epoch: [55][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.5491 (0.5125)\tPrec 83.594% (82.847%)\n",
      "Epoch: [55][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6123 (0.5122)\tPrec 76.562% (82.909%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.292 (0.292)\tLoss 0.5607 (0.5607)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.610% \n",
      "best acc: 80.610000\n",
      "Epoch: [56][0/391]\tTime 0.415 (0.415)\tData 0.379 (0.379)\tLoss 0.4463 (0.4463)\tPrec 85.156% (85.156%)\n",
      "Epoch: [56][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.4143 (0.4970)\tPrec 86.719% (83.478%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4387 (0.5072)\tPrec 83.594% (83.151%)\n",
      "Epoch: [56][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.4556 (0.5059)\tPrec 85.156% (83.225%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.5257 (0.5257)\tPrec 82.812% (82.812%)\n",
      " * Prec 79.050% \n",
      "best acc: 80.610000\n",
      "Epoch: [57][0/391]\tTime 0.379 (0.379)\tData 0.343 (0.343)\tLoss 0.4939 (0.4939)\tPrec 82.812% (82.812%)\n",
      "Epoch: [57][100/391]\tTime 0.045 (0.049)\tData 0.006 (0.006)\tLoss 0.6214 (0.4959)\tPrec 80.469% (83.571%)\n",
      "Epoch: [57][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3677 (0.5029)\tPrec 88.281% (83.349%)\n",
      "Epoch: [57][300/391]\tTime 0.042 (0.046)\tData 0.002 (0.003)\tLoss 0.5043 (0.5078)\tPrec 81.250% (83.272%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.352 (0.352)\tLoss 0.5144 (0.5144)\tPrec 82.031% (82.031%)\n",
      " * Prec 79.970% \n",
      "best acc: 80.610000\n",
      "Epoch: [58][0/391]\tTime 0.371 (0.371)\tData 0.335 (0.335)\tLoss 0.4096 (0.4096)\tPrec 87.500% (87.500%)\n",
      "Epoch: [58][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.3915 (0.4910)\tPrec 87.500% (83.895%)\n",
      "Epoch: [58][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4959 (0.4971)\tPrec 80.469% (83.462%)\n",
      "Epoch: [58][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4486 (0.5007)\tPrec 86.719% (83.378%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.6895 (0.6895)\tPrec 75.000% (75.000%)\n",
      " * Prec 76.990% \n",
      "best acc: 80.610000\n",
      "Epoch: [59][0/391]\tTime 0.542 (0.542)\tData 0.506 (0.506)\tLoss 0.4196 (0.4196)\tPrec 85.938% (85.938%)\n",
      "Epoch: [59][100/391]\tTime 0.045 (0.051)\tData 0.002 (0.008)\tLoss 0.5909 (0.4724)\tPrec 80.469% (84.197%)\n",
      "Epoch: [59][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3084 (0.4838)\tPrec 89.844% (84.002%)\n",
      "Epoch: [59][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3403 (0.4851)\tPrec 87.500% (83.923%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.7816 (0.7816)\tPrec 74.219% (74.219%)\n",
      " * Prec 74.170% \n",
      "best acc: 80.610000\n",
      "Epoch: [60][0/391]\tTime 0.470 (0.470)\tData 0.434 (0.434)\tLoss 0.4594 (0.4594)\tPrec 82.031% (82.031%)\n",
      "Epoch: [60][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.4560 (0.4727)\tPrec 85.156% (84.259%)\n",
      "Epoch: [60][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5437 (0.4813)\tPrec 82.812% (84.095%)\n",
      "Epoch: [60][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5102 (0.4836)\tPrec 85.156% (83.993%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.412 (0.412)\tLoss 0.5509 (0.5509)\tPrec 83.594% (83.594%)\n",
      " * Prec 80.830% \n",
      "best acc: 80.830000\n",
      "Epoch: [61][0/391]\tTime 0.420 (0.420)\tData 0.384 (0.384)\tLoss 0.5581 (0.5581)\tPrec 81.250% (81.250%)\n",
      "Epoch: [61][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.4554 (0.4722)\tPrec 82.812% (84.390%)\n",
      "Epoch: [61][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3376 (0.4835)\tPrec 86.719% (84.056%)\n",
      "Epoch: [61][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5371 (0.4884)\tPrec 78.906% (83.775%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.316 (0.316)\tLoss 0.4390 (0.4390)\tPrec 85.938% (85.938%)\n",
      " * Prec 80.260% \n",
      "best acc: 80.830000\n",
      "Epoch: [62][0/391]\tTime 0.383 (0.383)\tData 0.346 (0.346)\tLoss 0.6030 (0.6030)\tPrec 79.688% (79.688%)\n",
      "Epoch: [62][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.5757 (0.4737)\tPrec 85.156% (84.375%)\n",
      "Epoch: [62][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.5683 (0.4743)\tPrec 82.031% (84.371%)\n",
      "Epoch: [62][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4489 (0.4761)\tPrec 84.375% (84.339%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.5932 (0.5932)\tPrec 78.125% (78.125%)\n",
      " * Prec 79.260% \n",
      "best acc: 80.830000\n",
      "Epoch: [63][0/391]\tTime 0.433 (0.433)\tData 0.397 (0.397)\tLoss 0.5566 (0.5566)\tPrec 82.031% (82.031%)\n",
      "Epoch: [63][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.5058 (0.4807)\tPrec 81.250% (83.911%)\n",
      "Epoch: [63][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.004)\tLoss 0.4878 (0.4715)\tPrec 80.469% (84.309%)\n",
      "Epoch: [63][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4550 (0.4782)\tPrec 87.500% (84.048%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.490 (0.490)\tLoss 0.7775 (0.7775)\tPrec 71.875% (71.875%)\n",
      " * Prec 76.440% \n",
      "best acc: 80.830000\n",
      "Epoch: [64][0/391]\tTime 0.382 (0.382)\tData 0.345 (0.345)\tLoss 0.5164 (0.5164)\tPrec 85.938% (85.938%)\n",
      "Epoch: [64][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 0.3730 (0.4760)\tPrec 85.156% (84.499%)\n",
      "Epoch: [64][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.4944 (0.4759)\tPrec 83.594% (84.410%)\n",
      "Epoch: [64][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.4341 (0.4784)\tPrec 84.375% (84.256%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.8712 (0.8712)\tPrec 74.219% (74.219%)\n",
      " * Prec 76.140% \n",
      "best acc: 80.830000\n",
      "Epoch: [65][0/391]\tTime 0.373 (0.373)\tData 0.337 (0.337)\tLoss 0.4631 (0.4631)\tPrec 87.500% (87.500%)\n",
      "Epoch: [65][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.4673 (0.4594)\tPrec 85.156% (84.491%)\n",
      "Epoch: [65][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3082 (0.4623)\tPrec 90.625% (84.480%)\n",
      "Epoch: [65][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.6170 (0.4655)\tPrec 80.469% (84.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.4605 (0.4605)\tPrec 84.375% (84.375%)\n",
      " * Prec 79.610% \n",
      "best acc: 80.830000\n",
      "Epoch: [66][0/391]\tTime 0.344 (0.344)\tData 0.307 (0.307)\tLoss 0.5814 (0.5814)\tPrec 78.125% (78.125%)\n",
      "Epoch: [66][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.6330 (0.4601)\tPrec 76.562% (84.940%)\n",
      "Epoch: [66][200/391]\tTime 0.068 (0.047)\tData 0.002 (0.004)\tLoss 0.3761 (0.4630)\tPrec 86.719% (84.771%)\n",
      "Epoch: [66][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3963 (0.4639)\tPrec 82.031% (84.744%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.5837 (0.5837)\tPrec 81.250% (81.250%)\n",
      " * Prec 78.710% \n",
      "best acc: 80.830000\n",
      "Epoch: [67][0/391]\tTime 0.388 (0.388)\tData 0.351 (0.351)\tLoss 0.4419 (0.4419)\tPrec 85.938% (85.938%)\n",
      "Epoch: [67][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.4636 (0.4469)\tPrec 84.375% (85.241%)\n",
      "Epoch: [67][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.3055 (0.4619)\tPrec 91.406% (84.690%)\n",
      "Epoch: [67][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5106 (0.4590)\tPrec 79.688% (84.772%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.494 (0.494)\tLoss 0.6005 (0.6005)\tPrec 83.594% (83.594%)\n",
      " * Prec 80.640% \n",
      "best acc: 80.830000\n",
      "Epoch: [68][0/391]\tTime 0.349 (0.349)\tData 0.312 (0.312)\tLoss 0.4706 (0.4706)\tPrec 82.031% (82.031%)\n",
      "Epoch: [68][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.6521 (0.4409)\tPrec 78.906% (85.597%)\n",
      "Epoch: [68][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4440 (0.4524)\tPrec 88.281% (85.183%)\n",
      "Epoch: [68][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3695 (0.4546)\tPrec 85.938% (84.998%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.369 (0.369)\tLoss 0.5243 (0.5243)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.710% \n",
      "best acc: 80.830000\n",
      "Epoch: [69][0/391]\tTime 0.415 (0.415)\tData 0.377 (0.377)\tLoss 0.4481 (0.4481)\tPrec 85.156% (85.156%)\n",
      "Epoch: [69][100/391]\tTime 0.045 (0.049)\tData 0.001 (0.006)\tLoss 0.4453 (0.4392)\tPrec 85.938% (85.241%)\n",
      "Epoch: [69][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 0.4311 (0.4481)\tPrec 84.375% (85.071%)\n",
      "Epoch: [69][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4099 (0.4506)\tPrec 88.281% (84.998%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.6324 (0.6324)\tPrec 80.469% (80.469%)\n",
      " * Prec 80.710% \n",
      "best acc: 80.830000\n",
      "Epoch: [70][0/391]\tTime 0.291 (0.291)\tData 0.255 (0.255)\tLoss 0.5054 (0.5054)\tPrec 82.812% (82.812%)\n",
      "Epoch: [70][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.5251 (0.4438)\tPrec 85.156% (85.094%)\n",
      "Epoch: [70][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.4990 (0.4518)\tPrec 82.812% (84.954%)\n",
      "Epoch: [70][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3258 (0.4446)\tPrec 91.406% (85.177%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.400 (0.400)\tLoss 0.5783 (0.5783)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.410% \n",
      "best acc: 80.830000\n",
      "Epoch: [71][0/391]\tTime 0.528 (0.528)\tData 0.475 (0.475)\tLoss 0.4716 (0.4716)\tPrec 89.062% (89.062%)\n",
      "Epoch: [71][100/391]\tTime 0.046 (0.051)\tData 0.002 (0.008)\tLoss 0.3260 (0.4362)\tPrec 88.281% (85.705%)\n",
      "Epoch: [71][200/391]\tTime 0.046 (0.048)\tData 0.003 (0.005)\tLoss 0.5101 (0.4385)\tPrec 82.031% (85.545%)\n",
      "Epoch: [71][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.3664 (0.4483)\tPrec 84.375% (85.156%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.373 (0.373)\tLoss 0.6320 (0.6320)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.760% \n",
      "best acc: 80.830000\n",
      "Epoch: [72][0/391]\tTime 0.370 (0.370)\tData 0.334 (0.334)\tLoss 0.5427 (0.5427)\tPrec 80.469% (80.469%)\n",
      "Epoch: [72][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.5745 (0.4482)\tPrec 82.812% (85.203%)\n",
      "Epoch: [72][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4483 (0.4535)\tPrec 83.594% (85.187%)\n",
      "Epoch: [72][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.4068 (0.4538)\tPrec 85.156% (85.071%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.5889 (0.5889)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.590% \n",
      "best acc: 80.830000\n",
      "Epoch: [73][0/391]\tTime 0.366 (0.366)\tData 0.330 (0.330)\tLoss 0.3911 (0.3911)\tPrec 86.719% (86.719%)\n",
      "Epoch: [73][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3140 (0.4346)\tPrec 89.844% (85.651%)\n",
      "Epoch: [73][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.5289 (0.4416)\tPrec 83.594% (85.269%)\n",
      "Epoch: [73][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.4522 (0.4443)\tPrec 85.156% (85.252%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.5920 (0.5920)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.880% \n",
      "best acc: 80.880000\n",
      "Epoch: [74][0/391]\tTime 0.385 (0.385)\tData 0.349 (0.349)\tLoss 0.3907 (0.3907)\tPrec 85.938% (85.938%)\n",
      "Epoch: [74][100/391]\tTime 0.052 (0.049)\tData 0.001 (0.006)\tLoss 0.4363 (0.4291)\tPrec 82.812% (85.744%)\n",
      "Epoch: [74][200/391]\tTime 0.048 (0.049)\tData 0.001 (0.006)\tLoss 0.4291 (0.4315)\tPrec 88.281% (85.572%)\n",
      "Epoch: [74][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.005)\tLoss 0.4234 (0.4385)\tPrec 89.062% (85.330%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.345 (0.345)\tLoss 0.6605 (0.6605)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.190% \n",
      "best acc: 81.190000\n",
      "Epoch: [75][0/391]\tTime 0.361 (0.361)\tData 0.323 (0.323)\tLoss 0.3028 (0.3028)\tPrec 91.406% (91.406%)\n",
      "Epoch: [75][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.4698 (0.4119)\tPrec 84.375% (86.711%)\n",
      "Epoch: [75][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.4480 (0.4240)\tPrec 85.156% (86.140%)\n",
      "Epoch: [75][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2836 (0.4250)\tPrec 91.406% (86.109%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.402 (0.402)\tLoss 0.5843 (0.5843)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.830% \n",
      "best acc: 81.190000\n",
      "Epoch: [76][0/391]\tTime 0.416 (0.416)\tData 0.379 (0.379)\tLoss 0.3842 (0.3842)\tPrec 89.844% (89.844%)\n",
      "Epoch: [76][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.4761 (0.4262)\tPrec 85.938% (86.046%)\n",
      "Epoch: [76][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3716 (0.4211)\tPrec 86.719% (86.031%)\n",
      "Epoch: [76][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.004)\tLoss 0.3011 (0.4298)\tPrec 89.844% (85.738%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.390 (0.390)\tLoss 0.5220 (0.5220)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.980% \n",
      "best acc: 82.980000\n",
      "Epoch: [77][0/391]\tTime 0.509 (0.509)\tData 0.467 (0.467)\tLoss 0.5208 (0.5208)\tPrec 84.375% (84.375%)\n",
      "Epoch: [77][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.3794 (0.4090)\tPrec 85.156% (86.355%)\n",
      "Epoch: [77][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.3768 (0.4240)\tPrec 86.719% (85.953%)\n",
      "Epoch: [77][300/391]\tTime 0.047 (0.047)\tData 0.004 (0.004)\tLoss 0.3703 (0.4297)\tPrec 85.938% (85.761%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.326 (0.326)\tLoss 0.5320 (0.5320)\tPrec 82.812% (82.812%)\n",
      " * Prec 79.990% \n",
      "best acc: 82.980000\n",
      "Epoch: [78][0/391]\tTime 0.365 (0.365)\tData 0.317 (0.317)\tLoss 0.3389 (0.3389)\tPrec 89.062% (89.062%)\n",
      "Epoch: [78][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.3842 (0.4094)\tPrec 85.938% (86.177%)\n",
      "Epoch: [78][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4200 (0.4199)\tPrec 83.594% (85.926%)\n",
      "Epoch: [78][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3967 (0.4246)\tPrec 89.844% (85.862%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.422 (0.422)\tLoss 0.4540 (0.4540)\tPrec 85.156% (85.156%)\n",
      " * Prec 80.720% \n",
      "best acc: 82.980000\n",
      "Epoch: [79][0/391]\tTime 0.396 (0.396)\tData 0.350 (0.350)\tLoss 0.3233 (0.3233)\tPrec 88.281% (88.281%)\n",
      "Epoch: [79][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.5252 (0.4205)\tPrec 81.250% (86.293%)\n",
      "Epoch: [79][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3814 (0.4248)\tPrec 89.062% (86.085%)\n",
      "Epoch: [79][300/391]\tTime 0.041 (0.046)\tData 0.002 (0.003)\tLoss 0.4818 (0.4253)\tPrec 85.156% (86.010%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.412 (0.412)\tLoss 0.5212 (0.5212)\tPrec 82.031% (82.031%)\n",
      " * Prec 81.820% \n",
      "best acc: 82.980000\n",
      "Epoch: [80][0/391]\tTime 0.504 (0.504)\tData 0.468 (0.468)\tLoss 0.3539 (0.3539)\tPrec 89.062% (89.062%)\n",
      "Epoch: [80][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.4704 (0.4126)\tPrec 84.375% (86.332%)\n",
      "Epoch: [80][200/391]\tTime 0.046 (0.048)\tData 0.005 (0.005)\tLoss 0.4505 (0.4202)\tPrec 83.594% (85.984%)\n",
      "Epoch: [80][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3231 (0.4244)\tPrec 87.500% (85.875%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 0.6452 (0.6452)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.330% \n",
      "best acc: 82.980000\n",
      "Epoch: [81][0/391]\tTime 0.494 (0.494)\tData 0.452 (0.452)\tLoss 0.3938 (0.3938)\tPrec 90.625% (90.625%)\n",
      "Epoch: [81][100/391]\tTime 0.046 (0.050)\tData 0.003 (0.007)\tLoss 0.2754 (0.4002)\tPrec 89.844% (86.696%)\n",
      "Epoch: [81][200/391]\tTime 0.055 (0.049)\tData 0.003 (0.005)\tLoss 0.4704 (0.4066)\tPrec 85.938% (86.789%)\n",
      "Epoch: [81][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.3147 (0.4157)\tPrec 88.281% (86.353%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.429 (0.429)\tLoss 0.5893 (0.5893)\tPrec 80.469% (80.469%)\n",
      " * Prec 81.110% \n",
      "best acc: 82.980000\n",
      "Epoch: [82][0/391]\tTime 0.431 (0.431)\tData 0.395 (0.395)\tLoss 0.3586 (0.3586)\tPrec 88.281% (88.281%)\n",
      "Epoch: [82][100/391]\tTime 0.050 (0.049)\tData 0.002 (0.007)\tLoss 0.3763 (0.4175)\tPrec 82.812% (85.868%)\n",
      "Epoch: [82][200/391]\tTime 0.035 (0.048)\tData 0.002 (0.005)\tLoss 0.4288 (0.4133)\tPrec 85.938% (86.283%)\n",
      "Epoch: [82][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3836 (0.4194)\tPrec 83.594% (86.070%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.361 (0.361)\tLoss 0.6257 (0.6257)\tPrec 82.031% (82.031%)\n",
      " * Prec 81.350% \n",
      "best acc: 82.980000\n",
      "Epoch: [83][0/391]\tTime 0.425 (0.425)\tData 0.389 (0.389)\tLoss 0.3954 (0.3954)\tPrec 85.156% (85.156%)\n",
      "Epoch: [83][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.4340 (0.3903)\tPrec 82.812% (87.044%)\n",
      "Epoch: [83][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.7081 (0.4003)\tPrec 77.344% (86.773%)\n",
      "Epoch: [83][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.3205 (0.4066)\tPrec 87.500% (86.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.5317 (0.5317)\tPrec 85.156% (85.156%)\n",
      " * Prec 80.380% \n",
      "best acc: 82.980000\n",
      "Epoch: [84][0/391]\tTime 0.409 (0.409)\tData 0.348 (0.348)\tLoss 0.4645 (0.4645)\tPrec 86.719% (86.719%)\n",
      "Epoch: [84][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.5328 (0.4056)\tPrec 84.375% (86.618%)\n",
      "Epoch: [84][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3970 (0.4047)\tPrec 85.156% (86.688%)\n",
      "Epoch: [84][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.4778 (0.4092)\tPrec 86.719% (86.493%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.385 (0.385)\tLoss 0.5794 (0.5794)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.040% \n",
      "best acc: 82.980000\n",
      "Epoch: [85][0/391]\tTime 0.440 (0.440)\tData 0.405 (0.405)\tLoss 0.3509 (0.3509)\tPrec 87.500% (87.500%)\n",
      "Epoch: [85][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.3794 (0.3860)\tPrec 86.719% (87.059%)\n",
      "Epoch: [85][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4514 (0.3961)\tPrec 85.156% (86.800%)\n",
      "Epoch: [85][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.3257 (0.3991)\tPrec 90.625% (86.734%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.388 (0.388)\tLoss 0.4639 (0.4639)\tPrec 80.469% (80.469%)\n",
      " * Prec 81.100% \n",
      "best acc: 82.980000\n",
      "Epoch: [86][0/391]\tTime 0.454 (0.454)\tData 0.419 (0.419)\tLoss 0.3983 (0.3983)\tPrec 85.938% (85.938%)\n",
      "Epoch: [86][100/391]\tTime 0.042 (0.049)\tData 0.002 (0.006)\tLoss 0.5087 (0.4012)\tPrec 80.469% (86.494%)\n",
      "Epoch: [86][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3245 (0.4000)\tPrec 88.281% (86.758%)\n",
      "Epoch: [86][300/391]\tTime 0.037 (0.047)\tData 0.002 (0.003)\tLoss 0.4264 (0.4038)\tPrec 89.062% (86.646%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.5091 (0.5091)\tPrec 80.469% (80.469%)\n",
      " * Prec 80.630% \n",
      "best acc: 82.980000\n",
      "Epoch: [87][0/391]\tTime 0.402 (0.402)\tData 0.365 (0.365)\tLoss 0.4351 (0.4351)\tPrec 85.938% (85.938%)\n",
      "Epoch: [87][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.3431 (0.3991)\tPrec 89.844% (86.587%)\n",
      "Epoch: [87][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2869 (0.3980)\tPrec 89.844% (86.738%)\n",
      "Epoch: [87][300/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 0.4178 (0.4044)\tPrec 83.594% (86.547%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.323 (0.323)\tLoss 0.5447 (0.5447)\tPrec 79.688% (79.688%)\n",
      " * Prec 79.980% \n",
      "best acc: 82.980000\n",
      "Epoch: [88][0/391]\tTime 0.526 (0.526)\tData 0.491 (0.491)\tLoss 0.3576 (0.3576)\tPrec 89.062% (89.062%)\n",
      "Epoch: [88][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.4004 (0.3832)\tPrec 88.281% (87.206%)\n",
      "Epoch: [88][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3580 (0.3973)\tPrec 90.625% (86.633%)\n",
      "Epoch: [88][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2931 (0.4069)\tPrec 91.406% (86.327%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.314 (0.314)\tLoss 0.5132 (0.5132)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.900% \n",
      "best acc: 82.980000\n",
      "Epoch: [89][0/391]\tTime 0.445 (0.445)\tData 0.408 (0.408)\tLoss 0.2989 (0.2989)\tPrec 88.281% (88.281%)\n",
      "Epoch: [89][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.3249 (0.3888)\tPrec 86.719% (87.044%)\n",
      "Epoch: [89][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.2251 (0.3903)\tPrec 94.531% (87.045%)\n",
      "Epoch: [89][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2949 (0.3960)\tPrec 92.188% (86.841%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.396 (0.396)\tLoss 0.5022 (0.5022)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.980% \n",
      "best acc: 82.980000\n",
      "Epoch: [90][0/391]\tTime 0.391 (0.391)\tData 0.355 (0.355)\tLoss 0.5211 (0.5211)\tPrec 86.719% (86.719%)\n",
      "Epoch: [90][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.4065 (0.3912)\tPrec 86.719% (86.935%)\n",
      "Epoch: [90][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4377 (0.3896)\tPrec 87.500% (87.139%)\n",
      "Epoch: [90][300/391]\tTime 0.045 (0.046)\tData 0.003 (0.003)\tLoss 0.3927 (0.3992)\tPrec 85.938% (86.838%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.406 (0.406)\tLoss 0.6829 (0.6829)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.070% \n",
      "best acc: 82.980000\n",
      "Epoch: [91][0/391]\tTime 0.531 (0.531)\tData 0.483 (0.483)\tLoss 0.3075 (0.3075)\tPrec 89.844% (89.844%)\n",
      "Epoch: [91][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.007)\tLoss 0.5543 (0.3813)\tPrec 83.594% (87.283%)\n",
      "Epoch: [91][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3816 (0.3885)\tPrec 84.375% (87.193%)\n",
      "Epoch: [91][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4532 (0.3926)\tPrec 87.500% (87.002%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.280 (0.280)\tLoss 0.4357 (0.4357)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.360% \n",
      "best acc: 83.360000\n",
      "Epoch: [92][0/391]\tTime 0.482 (0.482)\tData 0.440 (0.440)\tLoss 0.3767 (0.3767)\tPrec 88.281% (88.281%)\n",
      "Epoch: [92][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.4234 (0.3896)\tPrec 85.156% (87.198%)\n",
      "Epoch: [92][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.4510 (0.3899)\tPrec 85.938% (87.263%)\n",
      "Epoch: [92][300/391]\tTime 0.039 (0.047)\tData 0.001 (0.004)\tLoss 0.4302 (0.4007)\tPrec 85.938% (86.776%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.409 (0.409)\tLoss 0.5219 (0.5219)\tPrec 82.031% (82.031%)\n",
      " * Prec 79.030% \n",
      "best acc: 83.360000\n",
      "Epoch: [93][0/391]\tTime 0.456 (0.456)\tData 0.420 (0.420)\tLoss 0.2768 (0.2768)\tPrec 92.188% (92.188%)\n",
      "Epoch: [93][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.009)\tLoss 0.4321 (0.3939)\tPrec 81.250% (86.587%)\n",
      "Epoch: [93][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 0.4384 (0.3899)\tPrec 86.719% (86.933%)\n",
      "Epoch: [93][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3693 (0.3936)\tPrec 84.375% (86.921%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.5750 (0.5750)\tPrec 81.250% (81.250%)\n",
      " * Prec 79.990% \n",
      "best acc: 83.360000\n",
      "Epoch: [94][0/391]\tTime 0.406 (0.406)\tData 0.359 (0.359)\tLoss 0.4980 (0.4980)\tPrec 83.594% (83.594%)\n",
      "Epoch: [94][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.006)\tLoss 0.4235 (0.4020)\tPrec 85.938% (86.463%)\n",
      "Epoch: [94][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4863 (0.3928)\tPrec 84.375% (86.940%)\n",
      "Epoch: [94][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.003)\tLoss 0.5757 (0.3969)\tPrec 80.469% (86.880%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 0.6298 (0.6298)\tPrec 79.688% (79.688%)\n",
      " * Prec 81.420% \n",
      "best acc: 83.360000\n",
      "Epoch: [95][0/391]\tTime 0.469 (0.469)\tData 0.429 (0.429)\tLoss 0.2315 (0.2315)\tPrec 89.844% (89.844%)\n",
      "Epoch: [95][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.006)\tLoss 0.5550 (0.3790)\tPrec 84.375% (87.539%)\n",
      "Epoch: [95][200/391]\tTime 0.055 (0.048)\tData 0.003 (0.004)\tLoss 0.3234 (0.3852)\tPrec 92.188% (87.232%)\n",
      "Epoch: [95][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.4593 (0.3845)\tPrec 85.156% (87.207%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.395 (0.395)\tLoss 0.3922 (0.3922)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.010% \n",
      "best acc: 83.360000\n",
      "Epoch: [96][0/391]\tTime 0.457 (0.457)\tData 0.421 (0.421)\tLoss 0.4029 (0.4029)\tPrec 86.719% (86.719%)\n",
      "Epoch: [96][100/391]\tTime 0.045 (0.050)\tData 0.001 (0.007)\tLoss 0.2600 (0.3716)\tPrec 89.844% (87.570%)\n",
      "Epoch: [96][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.3537 (0.3757)\tPrec 87.500% (87.496%)\n",
      "Epoch: [96][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.3356 (0.3779)\tPrec 90.625% (87.500%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.410 (0.410)\tLoss 0.5295 (0.5295)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.060% \n",
      "best acc: 83.360000\n",
      "Epoch: [97][0/391]\tTime 0.371 (0.371)\tData 0.334 (0.334)\tLoss 0.4134 (0.4134)\tPrec 85.938% (85.938%)\n",
      "Epoch: [97][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.5063 (0.3575)\tPrec 84.375% (88.281%)\n",
      "Epoch: [97][200/391]\tTime 0.045 (0.050)\tData 0.003 (0.005)\tLoss 0.3868 (0.3711)\tPrec 86.719% (87.760%)\n",
      "Epoch: [97][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.3963 (0.3816)\tPrec 87.500% (87.308%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.501 (0.501)\tLoss 0.5736 (0.5736)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.390% \n",
      "best acc: 83.360000\n",
      "Epoch: [98][0/391]\tTime 0.526 (0.526)\tData 0.490 (0.490)\tLoss 0.3206 (0.3206)\tPrec 90.625% (90.625%)\n",
      "Epoch: [98][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 0.3381 (0.3826)\tPrec 89.844% (87.562%)\n",
      "Epoch: [98][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.4540 (0.3795)\tPrec 82.812% (87.620%)\n",
      "Epoch: [98][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.5614 (0.3828)\tPrec 82.812% (87.453%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.454 (0.454)\tLoss 0.4392 (0.4392)\tPrec 86.719% (86.719%)\n",
      " * Prec 82.320% \n",
      "best acc: 83.360000\n",
      "Epoch: [99][0/391]\tTime 0.380 (0.380)\tData 0.344 (0.344)\tLoss 0.2633 (0.2633)\tPrec 90.625% (90.625%)\n",
      "Epoch: [99][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.006)\tLoss 0.5122 (0.3838)\tPrec 82.812% (87.276%)\n",
      "Epoch: [99][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2075 (0.3828)\tPrec 94.531% (87.391%)\n",
      "Epoch: [99][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.003)\tLoss 0.3736 (0.3861)\tPrec 85.156% (87.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.339 (0.339)\tLoss 0.5065 (0.5065)\tPrec 81.250% (81.250%)\n",
      " * Prec 79.490% \n",
      "best acc: 83.360000\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "weight_decay = 1e-3\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# weight decay: for regularization to prevent overfitting\n",
    "     \n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "    \n",
    "fdir = 'result/'+str(model_name)+str('_1206_2bit')\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "#PATH = \"result/VGG16_quant1129_5/model_best.pth.tar\"\n",
    "#checkpoint = torch.load(PATH)\n",
    "#model.load_state_dict(checkpoint['state_dict'])\n",
    "#device = torch.device(\"cuda\") \n",
    "        \n",
    "        \n",
    "        \n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8336/10000 (83%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant_1206_2bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driving-tanzania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "QuantConv2d(\n",
      "  3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 1\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 2\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 3\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 4\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 5\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 6\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 7\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 8\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 9\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 10\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 11\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 12\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 13\n"
     ]
    }
   ],
   "source": [
    "## Send an image and use prehook to grab the inputs of all the QuantConv2d layers\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "counter =0\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        counter += 1\n",
    "        print(layer, counter)\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e6a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[[[ 1.8314,  0.0000, -0.0000],\n",
      "          [ 1.8314,  0.0000, -0.0000],\n",
      "          [ 1.8314,  1.8314,  0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -1.8314],\n",
      "          [ 1.8314,  0.0000, -1.8314],\n",
      "          [ 1.8314,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -0.0000, -1.8314],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.8314,  1.8314, -0.0000],\n",
      "          [ 1.8314,  1.8314, -0.0000],\n",
      "          [ 1.8314,  1.8314, -0.0000]],\n",
      "\n",
      "         [[ 1.8314,  1.8314,  0.0000],\n",
      "          [-1.8314, -0.0000, -1.8314],\n",
      "          [-1.8314, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000, -0.0000, -1.8314]],\n",
      "\n",
      "         [[ 1.8314, -0.0000, -0.0000],\n",
      "          [-1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  1.8314,  1.8314],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314,  0.0000,  1.8314],\n",
      "          [-0.0000,  1.8314,  1.8314],\n",
      "          [ 0.0000,  1.8314,  1.8314]],\n",
      "\n",
      "         [[-1.8314, -0.0000,  1.8314],\n",
      "          [ 0.0000,  0.0000,  1.8314],\n",
      "          [-1.8314, -0.0000, -1.8314]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -0.0000],\n",
      "          [-1.8314, -1.8314,  0.0000],\n",
      "          [-1.8314, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -1.8314, -1.8314],\n",
      "          [ 1.8314, -0.0000, -1.8314],\n",
      "          [ 1.8314, -1.8314, -1.8314]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -1.8314, -1.8314],\n",
      "          [-1.8314, -0.0000, -0.0000],\n",
      "          [-1.8314, -1.8314, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.8314,  1.8314],\n",
      "          [ 1.8314,  1.8314,  0.0000],\n",
      "          [-1.8314, -1.8314, -1.8314]],\n",
      "\n",
      "         [[-0.0000, -1.8314, -0.0000],\n",
      "          [-1.8314, -1.8314,  0.0000],\n",
      "          [-0.0000, -1.8314, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000,  1.8314,  1.8314]],\n",
      "\n",
      "         [[ 0.0000, -1.8314,  1.8314],\n",
      "          [ 1.8314, -0.0000,  1.8314],\n",
      "          [ 0.0000, -1.8314,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.8314,  1.8314,  1.8314],\n",
      "          [-0.0000, -0.0000,  1.8314],\n",
      "          [-1.8314, -1.8314, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -1.8314],\n",
      "          [ 1.8314,  1.8314,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.8314]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8314,  1.8314,  0.0000],\n",
      "          [-0.0000, -1.8314, -1.8314],\n",
      "          [ 0.0000, -1.8314, -1.8314]],\n",
      "\n",
      "         [[ 1.8314,  1.8314, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-1.8314,  1.8314,  1.8314]],\n",
      "\n",
      "         [[ 0.0000, -1.8314, -1.8314],\n",
      "          [-0.0000, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -1.8314],\n",
      "          [ 0.0000,  1.8314,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314,  0.0000],\n",
      "          [-0.0000, -1.8314,  0.0000],\n",
      "          [-0.0000, -1.8314,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.8314,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.8314, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -1.8314],\n",
      "          [ 1.8314,  1.8314, -1.8314]],\n",
      "\n",
      "         [[ 0.0000,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 0.0000,  1.8314, -0.0000]],\n",
      "\n",
      "         [[ 1.8314,  0.0000,  1.8314],\n",
      "          [ 1.8314,  1.8314,  0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  1.8314],\n",
      "          [-1.8314,  0.0000, -0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314]],\n",
      "\n",
      "         [[ 1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -1.8314],\n",
      "          [-1.8314, -1.8314, -0.0000],\n",
      "          [ 1.8314,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.8314,  0.0000,  0.0000],\n",
      "          [ 1.8314,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8314,  1.8314,  1.8314],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [ 1.8314,  1.8314,  1.8314]],\n",
      "\n",
      "         [[ 0.0000,  1.8314, -0.0000],\n",
      "          [ 1.8314,  0.0000, -0.0000],\n",
      "          [ 1.8314,  1.8314,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.8314, -0.0000],\n",
      "          [ 1.8314, -1.8314, -0.0000],\n",
      "          [-1.8314, -1.8314, -1.8314]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -1.8314],\n",
      "          [-0.0000, -0.0000, -1.8314],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314,  0.0000],\n",
      "          [-0.0000, -1.8314,  1.8314],\n",
      "          [ 0.0000, -0.0000,  1.8314]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.8314, -1.8314, -0.0000],\n",
      "          [-1.8314, -0.0000,  0.0000],\n",
      "          [-1.8314, -1.8314, -1.8314]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 1.8314,  1.8314,  1.8314],\n",
      "          [-1.8314, -1.8314, -1.8314]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(1.8314, device='cuda:0', requires_grad=True)\n",
      "tensor([[[[ 1.,  0., -0.],\n",
      "          [ 1.,  0., -0.],\n",
      "          [ 1.,  1.,  0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  0., -1.],\n",
      "          [ 1.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -0., -1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  1., -0.],\n",
      "          [ 1.,  1., -0.],\n",
      "          [ 1.,  1., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  0.],\n",
      "          [-1., -0., -1.],\n",
      "          [-1., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0., -0., -0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0., -0., -1.]],\n",
      "\n",
      "         [[ 1., -0., -0.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-0.,  0., -0.]],\n",
      "\n",
      "         [[-1.,  0.,  1.],\n",
      "          [-0.,  1.,  1.],\n",
      "          [ 0.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -0.,  1.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [-1., -0., -1.]],\n",
      "\n",
      "         [[-1., -1., -0.],\n",
      "          [-1., -1.,  0.],\n",
      "          [-1., -0.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0.,  0., -0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0., -1., -1.],\n",
      "          [ 1., -0., -1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -1., -1.],\n",
      "          [-1., -0., -0.],\n",
      "          [-1., -1., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -1., -0.],\n",
      "          [-1., -1.,  0.],\n",
      "          [-0., -1., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  1.,  1.]],\n",
      "\n",
      "         [[ 0., -1.,  1.],\n",
      "          [ 1., -0.,  1.],\n",
      "          [ 0., -1.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-0., -0.,  1.],\n",
      "          [-1., -1., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 0.,  0.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  0.],\n",
      "          [-0., -1., -1.],\n",
      "          [ 0., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -0.],\n",
      "          [-0.,  0.,  0.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 0., -1., -1.],\n",
      "          [-0., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -1.],\n",
      "          [ 0.,  1.,  0.],\n",
      "          [-0.,  0.,  0.]],\n",
      "\n",
      "         [[-1., -1.,  0.],\n",
      "          [-0., -1.,  0.],\n",
      "          [-0., -1.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -0.,  0.],\n",
      "          [-0., -0., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  1., -0.]],\n",
      "\n",
      "         [[ 1.,  0.,  1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-0.,  0.,  1.],\n",
      "          [-1.,  0., -0.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -0.],\n",
      "          [ 1.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 0.,  1., -0.],\n",
      "          [ 1.,  0., -0.],\n",
      "          [ 1.,  1.,  0.]],\n",
      "\n",
      "         [[-0., -1., -0.],\n",
      "          [ 1., -1., -0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -1.],\n",
      "          [-0., -0., -1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1.,  0.],\n",
      "          [-0., -1.,  1.],\n",
      "          [ 0., -0.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1., -0.],\n",
      "          [-1., -0.,  0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 2\n",
    "quantConv2d = model.features[27]\n",
    "print(quantConv2d)\n",
    "#weight_q = model.layer1[0].conv2.weight_q # quantized value is stored during the training\n",
    "weight_q = quantConv2d.weight_q\n",
    "print(weight_q)\n",
    "\n",
    "weight_alpha = quantConv2d.weight_quant.wgt_alpha\n",
    "print(weight_alpha)\n",
    "w_delta = weight_alpha / (2 ** (w_bit - 1) - 1)   # delta can be calculated by using alpha and w_bit\n",
    "weight_int =  weight_q / w_delta# w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "furnished-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  0., -0.],\n",
      "          [ 1.,  0., -0.],\n",
      "          [ 1.,  1.,  0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  0., -1.],\n",
      "          [ 1.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -0., -1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  1., -0.],\n",
      "          [ 1.,  1., -0.],\n",
      "          [ 1.,  1., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  0.],\n",
      "          [-1., -0., -1.],\n",
      "          [-1., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0., -0., -0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0., -0., -1.]],\n",
      "\n",
      "         [[ 1., -0., -0.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[ 0.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-0.,  0., -0.]],\n",
      "\n",
      "         [[-1.,  0.,  1.],\n",
      "          [-0.,  1.,  1.],\n",
      "          [ 0.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -0.,  1.],\n",
      "          [ 0.,  0.,  1.],\n",
      "          [-1., -0., -1.]],\n",
      "\n",
      "         [[-1., -1., -0.],\n",
      "          [-1., -1.,  0.],\n",
      "          [-1., -0.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0.,  0., -0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0.,  0.]],\n",
      "\n",
      "         [[ 0., -1., -1.],\n",
      "          [ 1., -0., -1.],\n",
      "          [ 1., -1., -1.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -1., -1.],\n",
      "          [-1., -0., -0.],\n",
      "          [-1., -1., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -1., -0.],\n",
      "          [-1., -1.,  0.],\n",
      "          [-0., -1., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  1.,  1.]],\n",
      "\n",
      "         [[ 0., -1.,  1.],\n",
      "          [ 1., -0.,  1.],\n",
      "          [ 0., -1.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  1.],\n",
      "          [-0., -0.,  1.],\n",
      "          [-1., -1., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 0.,  0.,  1.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  0.],\n",
      "          [-0., -1., -1.],\n",
      "          [ 0., -1., -1.]],\n",
      "\n",
      "         [[ 1.,  1., -0.],\n",
      "          [-0.,  0.,  0.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 0., -1., -1.],\n",
      "          [-0., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -1.],\n",
      "          [ 0.,  1.,  0.],\n",
      "          [-0.,  0.,  0.]],\n",
      "\n",
      "         [[-1., -1.,  0.],\n",
      "          [-0., -1.,  0.],\n",
      "          [-0., -1.,  0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -0.,  0.],\n",
      "          [-0., -0., -1.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 0.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 0.,  1., -0.]],\n",
      "\n",
      "         [[ 1.,  0.,  1.],\n",
      "          [ 1.,  1.,  0.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[-0.,  0.,  1.],\n",
      "          [-1.,  0., -0.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -1., -1.],\n",
      "          [-1., -1., -1.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-1., -1., -0.],\n",
      "          [ 1.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  0.,  0.],\n",
      "          [ 1.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.]],\n",
      "\n",
      "         [[ 0.,  1., -0.],\n",
      "          [ 1.,  0., -0.],\n",
      "          [ 1.,  1.,  0.]],\n",
      "\n",
      "         [[-0., -1., -0.],\n",
      "          [ 1., -1., -0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[-0., -0., -1.],\n",
      "          [-0., -0., -1.],\n",
      "          [ 0.,  0., -0.]],\n",
      "\n",
      "         [[-1., -1.,  0.],\n",
      "          [-0., -1.,  1.],\n",
      "          [ 0., -0.,  1.]],\n",
      "\n",
      "         [[-0., -0., -0.],\n",
      "          [-0., -0., -0.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[-1., -1., -0.],\n",
      "          [-1., -0.,  0.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 0.,  0.,  0.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-1., -1., -1.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "##### Find \"weight_int\" for features[3] ####\n",
    "w_bit = 2\n",
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "textile-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2., 1., 0.],\n",
      "          [0., 1., 1., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0.],\n",
      "          [0., 1., 2., 1.],\n",
      "          [0., 1., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0., 1.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0.],\n",
      "          [1., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 1.],\n",
      "          [1., 1., 0., 1.],\n",
      "          [1., 1., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3., 3.],\n",
      "          [0., 0., 0., 2.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1.],\n",
      "          [0., 1., 3., 1.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 1., 1., 2.],\n",
      "          [2., 1., 0., 1.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [3., 3., 3., 2.],\n",
      "          [2., 3., 3., 2.]],\n",
      "\n",
      "         [[1., 2., 1., 0.],\n",
      "          [0., 1., 0., 0.],\n",
      "          [0., 2., 0., 0.],\n",
      "          [0., 1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [2., 3., 1., 1.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 1., 0., 0.],\n",
      "          [2., 1., 0., 0.],\n",
      "          [2., 2., 0., 0.],\n",
      "          [3., 3., 2., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 3., 3., 3.],\n",
      "          [1., 3., 3., 3.],\n",
      "          [0., 0., 1., 2.],\n",
      "          [0., 0., 2., 2.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1.]],\n",
      "\n",
      "         [[2., 2., 1., 0.],\n",
      "          [2., 2., 2., 0.],\n",
      "          [1., 2., 3., 0.],\n",
      "          [0., 0., 1., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [2., 0., 0., 1.],\n",
      "          [2., 0., 0., 1.],\n",
      "          [2., 3., 2., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 1.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[3., 2., 1., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[3., 3., 3., 1.],\n",
      "          [3., 3., 2., 1.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [3., 3., 3., 2.],\n",
      "          [3., 3., 3., 3.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 2., 2.],\n",
      "          [0., 3., 3., 3.],\n",
      "          [0., 2., 3., 2.],\n",
      "          [0., 1., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [2., 3., 3., 2.],\n",
      "          [1., 3., 3., 1.]],\n",
      "\n",
      "         [[0., 0., 0., 1.],\n",
      "          [0., 0., 2., 3.],\n",
      "          [0., 0., 2., 3.],\n",
      "          [2., 3., 3., 2.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 1., 0.]],\n",
      "\n",
      "         [[2., 2., 0., 0.],\n",
      "          [3., 3., 0., 0.],\n",
      "          [3., 3., 0., 0.],\n",
      "          [3., 3., 2., 1.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "act = save_output.outputs[8][0] # input of 27th quantconv\n",
    "act_alpha  = model.features[27].act_alpha\n",
    "act_bit = 2\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "act_delta = act_alpha / (2**act_bit - 1)\n",
    "act_int = act_q / act_delta\n",
    "print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "selected-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref = save_output.outputs[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(8,8, kernel_size = 3, padding = 1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "out_int = conv_int(act_int)\n",
    "relu = model.features[28]\n",
    "out_recovered = out_int * act_delta * w_delta\n",
    "out_recovered = relu(out_recovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7509e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs(out_ref - out_recovered)\n",
    "print(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9bdae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
