{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50435e8",
   "metadata": {},
   "source": [
    "Part1. Train VGG16 with quantization-aware training (15%)\n",
    "\n",
    " - Train for 4-bit input activation and 4-bit weight to achieve >90% accuracy. \n",
    "\n",
    " - But, this time, reduce a certain convolution layer's input channel numbers to be 8 and output channel numbers to be 8. (v)\n",
    "\n",
    " - Also, remove the batch normalization layer after the squeezed convolution. (v)\n",
    "\n",
    "  e.g., replace \"conv -> relu -> batchnorm\" with \"conv -> relu\"\n",
    "\n",
    " - This layer will be mapped on your 8x8 2D systolic array. Thus, reducing to 8 channels helps your layer's mapping in an array nicely without tiling.\n",
    "\n",
    " - This time, compute your \"psum_recovered\" such as HW5 including ReLU and compare with your prehooked input for the next layer (instead of your computed psum_ref).\n",
    "\n",
    " - [hint] It is recommended not to reduce the input channel of Conv layer at too early layer position because the early layer's feature map size (nij) is large incurring long verification cycles.\n",
    "\n",
    "   (recommended location: around 27-th layer, e.g., features[27] for VGGNet)\n",
    "\n",
    " - Measure of success: accuracy >90%  with 8 input/output channels + error < 10^-3 for psum_recorvered for VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [100, 200,300]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34dca17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.751 (1.751)\tData 0.188 (0.188)\tLoss 2.5184 (2.5184)\tPrec 10.938% (10.938%)\n",
      "Epoch: [0][100/391]\tTime 0.043 (0.063)\tData 0.002 (0.004)\tLoss 1.8848 (2.0341)\tPrec 34.375% (25.820%)\n",
      "Epoch: [0][200/391]\tTime 0.045 (0.054)\tData 0.001 (0.003)\tLoss 1.7193 (1.8549)\tPrec 36.719% (31.775%)\n",
      "Epoch: [0][300/391]\tTime 0.045 (0.051)\tData 0.001 (0.002)\tLoss 1.4214 (1.7307)\tPrec 46.094% (36.483%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 1.4728 (1.4728)\tPrec 51.562% (51.562%)\n",
      " * Prec 47.220% \n",
      "best acc: 47.220000\n",
      "Epoch: [1][0/391]\tTime 0.288 (0.288)\tData 0.243 (0.243)\tLoss 1.4557 (1.4557)\tPrec 44.531% (44.531%)\n",
      "Epoch: [1][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 1.2104 (1.2725)\tPrec 55.469% (54.332%)\n",
      "Epoch: [1][200/391]\tTime 0.044 (0.046)\tData 0.005 (0.003)\tLoss 1.1576 (1.2265)\tPrec 61.719% (56.095%)\n",
      "Epoch: [1][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 1.0519 (1.1833)\tPrec 60.938% (57.714%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.265 (0.265)\tLoss 1.2407 (1.2407)\tPrec 54.688% (54.688%)\n",
      " * Prec 60.130% \n",
      "best acc: 60.130000\n",
      "Epoch: [2][0/391]\tTime 0.289 (0.289)\tData 0.241 (0.241)\tLoss 0.9507 (0.9507)\tPrec 67.188% (67.188%)\n",
      "Epoch: [2][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 1.0777 (0.9894)\tPrec 56.250% (64.836%)\n",
      "Epoch: [2][200/391]\tTime 0.049 (0.048)\tData 0.001 (0.003)\tLoss 0.7353 (0.9706)\tPrec 74.219% (65.380%)\n",
      "Epoch: [2][300/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 1.1118 (0.9563)\tPrec 57.812% (65.994%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 1.1266 (1.1266)\tPrec 64.062% (64.062%)\n",
      " * Prec 65.720% \n",
      "best acc: 65.720000\n",
      "Epoch: [3][0/391]\tTime 0.323 (0.323)\tData 0.276 (0.276)\tLoss 0.8678 (0.8678)\tPrec 69.531% (69.531%)\n",
      "Epoch: [3][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.8256 (0.8565)\tPrec 71.094% (70.019%)\n",
      "Epoch: [3][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.8024 (0.8508)\tPrec 75.000% (70.355%)\n",
      "Epoch: [3][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.8376 (0.8405)\tPrec 67.188% (70.582%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.8033 (0.8033)\tPrec 73.438% (73.438%)\n",
      " * Prec 70.060% \n",
      "best acc: 70.060000\n",
      "Epoch: [4][0/391]\tTime 0.293 (0.293)\tData 0.245 (0.245)\tLoss 0.8519 (0.8519)\tPrec 69.531% (69.531%)\n",
      "Epoch: [4][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.8641 (0.7664)\tPrec 70.312% (73.499%)\n",
      "Epoch: [4][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.5488 (0.7548)\tPrec 82.031% (73.869%)\n",
      "Epoch: [4][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.6181 (0.7543)\tPrec 78.125% (73.819%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.8146 (0.8146)\tPrec 71.875% (71.875%)\n",
      " * Prec 73.750% \n",
      "best acc: 73.750000\n",
      "Epoch: [5][0/391]\tTime 0.332 (0.332)\tData 0.283 (0.283)\tLoss 0.7920 (0.7920)\tPrec 71.875% (71.875%)\n",
      "Epoch: [5][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.7331 (0.6811)\tPrec 72.656% (76.083%)\n",
      "Epoch: [5][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.7191 (0.6930)\tPrec 74.219% (75.750%)\n",
      "Epoch: [5][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.7364 (0.6837)\tPrec 73.438% (76.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.7684 (0.7684)\tPrec 71.875% (71.875%)\n",
      " * Prec 74.360% \n",
      "best acc: 74.360000\n",
      "Epoch: [6][0/391]\tTime 0.307 (0.307)\tData 0.260 (0.260)\tLoss 0.6597 (0.6597)\tPrec 77.344% (77.344%)\n",
      "Epoch: [6][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.5795 (0.6423)\tPrec 75.000% (77.955%)\n",
      "Epoch: [6][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.6576 (0.6494)\tPrec 76.562% (77.569%)\n",
      "Epoch: [6][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.5980 (0.6431)\tPrec 84.375% (77.884%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.7323 (0.7323)\tPrec 76.562% (76.562%)\n",
      " * Prec 74.630% \n",
      "best acc: 74.630000\n",
      "Epoch: [7][0/391]\tTime 0.325 (0.325)\tData 0.278 (0.278)\tLoss 0.5320 (0.5320)\tPrec 84.375% (84.375%)\n",
      "Epoch: [7][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.7611 (0.6052)\tPrec 68.750% (79.680%)\n",
      "Epoch: [7][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.5199 (0.6003)\tPrec 82.031% (79.447%)\n",
      "Epoch: [7][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.4873 (0.6012)\tPrec 79.688% (79.386%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.5527 (0.5527)\tPrec 78.125% (78.125%)\n",
      " * Prec 78.060% \n",
      "best acc: 78.060000\n",
      "Epoch: [8][0/391]\tTime 0.284 (0.284)\tData 0.237 (0.237)\tLoss 0.7140 (0.7140)\tPrec 75.000% (75.000%)\n",
      "Epoch: [8][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.4904 (0.5616)\tPrec 79.688% (80.446%)\n",
      "Epoch: [8][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.5198 (0.5600)\tPrec 83.594% (80.659%)\n",
      "Epoch: [8][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.5167 (0.5616)\tPrec 81.250% (80.736%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.7868 (0.7868)\tPrec 75.781% (75.781%)\n",
      " * Prec 75.490% \n",
      "best acc: 78.060000\n",
      "Epoch: [9][0/391]\tTime 0.297 (0.297)\tData 0.248 (0.248)\tLoss 0.4901 (0.4901)\tPrec 81.250% (81.250%)\n",
      "Epoch: [9][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.4780 (0.5212)\tPrec 85.156% (82.503%)\n",
      "Epoch: [9][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.5307 (0.5313)\tPrec 82.031% (82.047%)\n",
      "Epoch: [9][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.6017 (0.5248)\tPrec 80.469% (82.236%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.6282 (0.6282)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.090% \n",
      "best acc: 80.090000\n",
      "Epoch: [10][0/391]\tTime 0.288 (0.288)\tData 0.236 (0.236)\tLoss 0.4856 (0.4856)\tPrec 86.719% (86.719%)\n",
      "Epoch: [10][100/391]\tTime 0.051 (0.047)\tData 0.001 (0.004)\tLoss 0.3714 (0.4877)\tPrec 85.156% (83.563%)\n",
      "Epoch: [10][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.4998 (0.4975)\tPrec 80.469% (83.065%)\n",
      "Epoch: [10][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.4732 (0.4966)\tPrec 85.938% (83.220%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.6167 (0.6167)\tPrec 78.125% (78.125%)\n",
      " * Prec 77.310% \n",
      "best acc: 80.090000\n",
      "Epoch: [11][0/391]\tTime 0.323 (0.323)\tData 0.275 (0.275)\tLoss 0.3961 (0.3961)\tPrec 87.500% (87.500%)\n",
      "Epoch: [11][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.2489 (0.4668)\tPrec 91.406% (84.166%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.4049 (0.4690)\tPrec 87.500% (84.060%)\n",
      "Epoch: [11][300/391]\tTime 0.047 (0.046)\tData 0.002 (0.002)\tLoss 0.4361 (0.4742)\tPrec 82.031% (83.962%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 0.5543 (0.5543)\tPrec 77.344% (77.344%)\n",
      " * Prec 80.240% \n",
      "best acc: 80.240000\n",
      "Epoch: [12][0/391]\tTime 0.273 (0.273)\tData 0.232 (0.232)\tLoss 0.2916 (0.2916)\tPrec 88.281% (88.281%)\n",
      "Epoch: [12][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.4576 (0.4546)\tPrec 86.719% (84.623%)\n",
      "Epoch: [12][200/391]\tTime 0.048 (0.046)\tData 0.002 (0.003)\tLoss 0.4744 (0.4589)\tPrec 82.031% (84.301%)\n",
      "Epoch: [12][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.4628 (0.4575)\tPrec 85.156% (84.398%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.5012 (0.5012)\tPrec 83.594% (83.594%)\n",
      " * Prec 80.680% \n",
      "best acc: 80.680000\n",
      "Epoch: [13][0/391]\tTime 0.297 (0.297)\tData 0.251 (0.251)\tLoss 0.4971 (0.4971)\tPrec 83.594% (83.594%)\n",
      "Epoch: [13][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.004)\tLoss 0.4472 (0.4292)\tPrec 85.156% (85.589%)\n",
      "Epoch: [13][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.4596 (0.4404)\tPrec 83.594% (85.238%)\n",
      "Epoch: [13][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.5010 (0.4346)\tPrec 82.031% (85.364%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.4399 (0.4399)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.710% \n",
      "best acc: 82.710000\n",
      "Epoch: [14][0/391]\tTime 0.274 (0.274)\tData 0.228 (0.228)\tLoss 0.3914 (0.3914)\tPrec 83.594% (83.594%)\n",
      "Epoch: [14][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3492 (0.4157)\tPrec 87.500% (85.644%)\n",
      "Epoch: [14][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.2992 (0.4169)\tPrec 90.625% (85.728%)\n",
      "Epoch: [14][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.3848 (0.4215)\tPrec 87.500% (85.743%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.3853 (0.3853)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.200% \n",
      "best acc: 83.200000\n",
      "Epoch: [15][0/391]\tTime 0.287 (0.287)\tData 0.240 (0.240)\tLoss 0.2991 (0.2991)\tPrec 88.281% (88.281%)\n",
      "Epoch: [15][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3311 (0.4046)\tPrec 88.281% (86.061%)\n",
      "Epoch: [15][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3649 (0.4066)\tPrec 85.938% (86.167%)\n",
      "Epoch: [15][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.4215 (0.4080)\tPrec 85.938% (86.091%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.4761 (0.4761)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.460% \n",
      "best acc: 83.460000\n",
      "Epoch: [16][0/391]\tTime 0.339 (0.339)\tData 0.300 (0.300)\tLoss 0.3347 (0.3347)\tPrec 88.281% (88.281%)\n",
      "Epoch: [16][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.005)\tLoss 0.2882 (0.3830)\tPrec 88.281% (87.121%)\n",
      "Epoch: [16][200/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.2710 (0.3868)\tPrec 92.969% (86.975%)\n",
      "Epoch: [16][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.5034 (0.3883)\tPrec 83.594% (86.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.193 (0.193)\tLoss 0.4689 (0.4689)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.750% \n",
      "best acc: 83.460000\n",
      "Epoch: [17][0/391]\tTime 0.320 (0.320)\tData 0.275 (0.275)\tLoss 0.4436 (0.4436)\tPrec 85.156% (85.156%)\n",
      "Epoch: [17][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.3783 (0.3706)\tPrec 86.719% (87.454%)\n",
      "Epoch: [17][200/391]\tTime 0.046 (0.047)\tData 0.003 (0.003)\tLoss 0.5468 (0.3775)\tPrec 83.594% (87.271%)\n",
      "Epoch: [17][300/391]\tTime 0.047 (0.046)\tData 0.001 (0.002)\tLoss 0.4138 (0.3772)\tPrec 86.719% (87.230%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.3990 (0.3990)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.120% \n",
      "best acc: 83.460000\n",
      "Epoch: [18][0/391]\tTime 0.288 (0.288)\tData 0.240 (0.240)\tLoss 0.3166 (0.3166)\tPrec 85.938% (85.938%)\n",
      "Epoch: [18][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4741 (0.3409)\tPrec 87.500% (88.405%)\n",
      "Epoch: [18][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.3027 (0.3589)\tPrec 89.844% (87.718%)\n",
      "Epoch: [18][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3159 (0.3630)\tPrec 87.500% (87.588%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.2746 (0.2746)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.040% \n",
      "best acc: 84.040000\n",
      "Epoch: [19][0/391]\tTime 0.374 (0.374)\tData 0.329 (0.329)\tLoss 0.3155 (0.3155)\tPrec 90.625% (90.625%)\n",
      "Epoch: [19][100/391]\tTime 0.042 (0.048)\tData 0.001 (0.005)\tLoss 0.3871 (0.3308)\tPrec 87.500% (88.591%)\n",
      "Epoch: [19][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.4666 (0.3443)\tPrec 84.375% (88.254%)\n",
      "Epoch: [19][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3672 (0.3504)\tPrec 85.156% (88.126%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.5349 (0.5349)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.110% \n",
      "best acc: 84.040000\n",
      "Epoch: [20][0/391]\tTime 0.365 (0.365)\tData 0.319 (0.319)\tLoss 0.2168 (0.2168)\tPrec 92.188% (92.188%)\n",
      "Epoch: [20][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.005)\tLoss 0.2673 (0.3318)\tPrec 89.062% (88.776%)\n",
      "Epoch: [20][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2853 (0.3327)\tPrec 89.844% (88.822%)\n",
      "Epoch: [20][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3289 (0.3360)\tPrec 90.625% (88.697%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.4063 (0.4063)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.690% \n",
      "best acc: 84.040000\n",
      "Epoch: [21][0/391]\tTime 0.294 (0.294)\tData 0.244 (0.244)\tLoss 0.4218 (0.4218)\tPrec 86.719% (86.719%)\n",
      "Epoch: [21][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.004)\tLoss 0.4456 (0.3128)\tPrec 85.938% (89.503%)\n",
      "Epoch: [21][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3372 (0.3124)\tPrec 88.281% (89.296%)\n",
      "Epoch: [21][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.3348 (0.3183)\tPrec 88.281% (89.008%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.2928 (0.2928)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.230% \n",
      "best acc: 86.230000\n",
      "Epoch: [22][0/391]\tTime 0.272 (0.272)\tData 0.230 (0.230)\tLoss 0.2753 (0.2753)\tPrec 91.406% (91.406%)\n",
      "Epoch: [22][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2261 (0.2996)\tPrec 92.188% (89.650%)\n",
      "Epoch: [22][200/391]\tTime 0.043 (0.046)\tData 0.001 (0.003)\tLoss 0.2968 (0.3031)\tPrec 88.281% (89.568%)\n",
      "Epoch: [22][300/391]\tTime 0.042 (0.046)\tData 0.002 (0.002)\tLoss 0.2394 (0.3133)\tPrec 92.969% (89.281%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.5372 (0.5372)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.260% \n",
      "best acc: 86.230000\n",
      "Epoch: [23][0/391]\tTime 0.285 (0.285)\tData 0.239 (0.239)\tLoss 0.1672 (0.1672)\tPrec 93.750% (93.750%)\n",
      "Epoch: [23][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.3171 (0.3092)\tPrec 87.500% (89.465%)\n",
      "Epoch: [23][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.1922 (0.3111)\tPrec 93.750% (89.498%)\n",
      "Epoch: [23][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.3489 (0.3127)\tPrec 88.281% (89.361%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.3496 (0.3496)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.260% \n",
      "best acc: 86.230000\n",
      "Epoch: [24][0/391]\tTime 0.337 (0.337)\tData 0.289 (0.289)\tLoss 0.2093 (0.2093)\tPrec 89.844% (89.844%)\n",
      "Epoch: [24][100/391]\tTime 0.047 (0.048)\tData 0.001 (0.005)\tLoss 0.2820 (0.2967)\tPrec 89.062% (90.122%)\n",
      "Epoch: [24][200/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 0.2488 (0.2992)\tPrec 89.844% (89.991%)\n",
      "Epoch: [24][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3212 (0.3003)\tPrec 88.281% (89.870%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.3209 (0.3209)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.160% \n",
      "best acc: 86.230000\n",
      "Epoch: [25][0/391]\tTime 0.271 (0.271)\tData 0.226 (0.226)\tLoss 0.2994 (0.2994)\tPrec 85.938% (85.938%)\n",
      "Epoch: [25][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2557 (0.2717)\tPrec 92.969% (90.764%)\n",
      "Epoch: [25][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.2059 (0.2815)\tPrec 92.969% (90.524%)\n",
      "Epoch: [25][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.2718 (0.2880)\tPrec 89.844% (90.298%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.4298 (0.4298)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.190% \n",
      "best acc: 86.230000\n",
      "Epoch: [26][0/391]\tTime 0.277 (0.277)\tData 0.234 (0.234)\tLoss 0.1755 (0.1755)\tPrec 92.969% (92.969%)\n",
      "Epoch: [26][100/391]\tTime 0.043 (0.047)\tData 0.001 (0.004)\tLoss 0.2725 (0.2731)\tPrec 89.844% (90.780%)\n",
      "Epoch: [26][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.2487 (0.2773)\tPrec 92.969% (90.699%)\n",
      "Epoch: [26][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.3761 (0.2745)\tPrec 89.062% (90.682%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3316 (0.3316)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.760% \n",
      "best acc: 86.760000\n",
      "Epoch: [27][0/391]\tTime 0.263 (0.263)\tData 0.215 (0.215)\tLoss 0.2165 (0.2165)\tPrec 94.531% (94.531%)\n",
      "Epoch: [27][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2215 (0.2625)\tPrec 91.406% (91.306%)\n",
      "Epoch: [27][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.3228 (0.2661)\tPrec 88.281% (91.154%)\n",
      "Epoch: [27][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.1974 (0.2704)\tPrec 90.625% (91.022%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.5132 (0.5132)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.300% \n",
      "best acc: 86.760000\n",
      "Epoch: [28][0/391]\tTime 0.332 (0.332)\tData 0.288 (0.288)\tLoss 0.2782 (0.2782)\tPrec 89.062% (89.062%)\n",
      "Epoch: [28][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.2583 (0.2464)\tPrec 90.625% (91.615%)\n",
      "Epoch: [28][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1377 (0.2585)\tPrec 96.094% (91.262%)\n",
      "Epoch: [28][300/391]\tTime 0.047 (0.046)\tData 0.001 (0.003)\tLoss 0.3252 (0.2623)\tPrec 89.062% (91.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3551 (0.3551)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.070% \n",
      "best acc: 86.760000\n",
      "Epoch: [29][0/391]\tTime 0.290 (0.290)\tData 0.245 (0.245)\tLoss 0.1800 (0.1800)\tPrec 92.188% (92.188%)\n",
      "Epoch: [29][100/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.1535 (0.2487)\tPrec 94.531% (91.445%)\n",
      "Epoch: [29][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.4312 (0.2595)\tPrec 88.281% (91.235%)\n",
      "Epoch: [29][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.2575 (0.2569)\tPrec 92.188% (91.362%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.2407 (0.2407)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.380% \n",
      "best acc: 87.380000\n",
      "Epoch: [30][0/391]\tTime 0.323 (0.323)\tData 0.278 (0.278)\tLoss 0.1601 (0.1601)\tPrec 93.750% (93.750%)\n",
      "Epoch: [30][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2270 (0.2440)\tPrec 93.750% (92.041%)\n",
      "Epoch: [30][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2031 (0.2457)\tPrec 93.750% (91.775%)\n",
      "Epoch: [30][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1862 (0.2509)\tPrec 94.531% (91.632%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.2942 (0.2942)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.920% \n",
      "best acc: 87.380000\n",
      "Epoch: [31][0/391]\tTime 0.256 (0.256)\tData 0.223 (0.223)\tLoss 0.1848 (0.1848)\tPrec 94.531% (94.531%)\n",
      "Epoch: [31][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2174 (0.2446)\tPrec 92.969% (91.607%)\n",
      "Epoch: [31][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2735 (0.2441)\tPrec 91.406% (91.721%)\n",
      "Epoch: [31][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.3678 (0.2468)\tPrec 88.281% (91.645%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.3601 (0.3601)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.460% \n",
      "best acc: 87.380000\n",
      "Epoch: [32][0/391]\tTime 0.333 (0.333)\tData 0.286 (0.286)\tLoss 0.2304 (0.2304)\tPrec 89.844% (89.844%)\n",
      "Epoch: [32][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3595 (0.2366)\tPrec 89.062% (92.079%)\n",
      "Epoch: [32][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.003)\tLoss 0.2543 (0.2456)\tPrec 90.625% (91.752%)\n",
      "Epoch: [32][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.1879 (0.2458)\tPrec 93.750% (91.681%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3520 (0.3520)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.750% \n",
      "best acc: 87.380000\n",
      "Epoch: [33][0/391]\tTime 0.281 (0.281)\tData 0.233 (0.233)\tLoss 0.3082 (0.3082)\tPrec 87.500% (87.500%)\n",
      "Epoch: [33][100/391]\tTime 0.048 (0.047)\tData 0.002 (0.004)\tLoss 0.2559 (0.2227)\tPrec 89.844% (92.420%)\n",
      "Epoch: [33][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1243 (0.2310)\tPrec 96.875% (92.106%)\n",
      "Epoch: [33][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.2933 (0.2390)\tPrec 89.062% (91.819%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.255 (0.255)\tLoss 0.2281 (0.2281)\tPrec 94.531% (94.531%)\n",
      " * Prec 87.510% \n",
      "best acc: 87.510000\n",
      "Epoch: [34][0/391]\tTime 0.287 (0.287)\tData 0.245 (0.245)\tLoss 0.2368 (0.2368)\tPrec 91.406% (91.406%)\n",
      "Epoch: [34][100/391]\tTime 0.047 (0.047)\tData 0.001 (0.004)\tLoss 0.1345 (0.2238)\tPrec 93.750% (92.311%)\n",
      "Epoch: [34][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2364 (0.2251)\tPrec 91.406% (92.374%)\n",
      "Epoch: [34][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.1596 (0.2333)\tPrec 95.312% (92.063%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.4104 (0.4104)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.700% \n",
      "best acc: 87.510000\n",
      "Epoch: [35][0/391]\tTime 0.321 (0.321)\tData 0.275 (0.275)\tLoss 0.2564 (0.2564)\tPrec 90.625% (90.625%)\n",
      "Epoch: [35][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.2208 (0.2191)\tPrec 92.188% (92.621%)\n",
      "Epoch: [35][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3584 (0.2223)\tPrec 87.500% (92.467%)\n",
      "Epoch: [35][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.2264 (0.2223)\tPrec 92.188% (92.504%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.3952 (0.3952)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.070% \n",
      "best acc: 87.510000\n",
      "Epoch: [36][0/391]\tTime 0.286 (0.286)\tData 0.233 (0.233)\tLoss 0.2408 (0.2408)\tPrec 92.969% (92.969%)\n",
      "Epoch: [36][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2631 (0.2202)\tPrec 92.969% (92.768%)\n",
      "Epoch: [36][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1734 (0.2192)\tPrec 94.531% (92.716%)\n",
      "Epoch: [36][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.2349 (0.2221)\tPrec 90.625% (92.483%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.2309 (0.2309)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.840% \n",
      "best acc: 87.510000\n",
      "Epoch: [37][0/391]\tTime 0.305 (0.305)\tData 0.260 (0.260)\tLoss 0.2667 (0.2667)\tPrec 92.969% (92.969%)\n",
      "Epoch: [37][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.2529 (0.2198)\tPrec 92.969% (92.644%)\n",
      "Epoch: [37][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1634 (0.2154)\tPrec 94.531% (92.798%)\n",
      "Epoch: [37][300/391]\tTime 0.052 (0.046)\tData 0.002 (0.002)\tLoss 0.1627 (0.2185)\tPrec 94.531% (92.668%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.4260 (0.4260)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.690% \n",
      "best acc: 87.510000\n",
      "Epoch: [38][0/391]\tTime 0.304 (0.304)\tData 0.259 (0.259)\tLoss 0.2153 (0.2153)\tPrec 92.188% (92.188%)\n",
      "Epoch: [38][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.2206 (0.1963)\tPrec 90.625% (93.255%)\n",
      "Epoch: [38][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2646 (0.2167)\tPrec 92.969% (92.786%)\n",
      "Epoch: [38][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.2407 (0.2178)\tPrec 92.969% (92.688%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.263 (0.263)\tLoss 0.4450 (0.4450)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.000% \n",
      "best acc: 87.510000\n",
      "Epoch: [39][0/391]\tTime 0.298 (0.298)\tData 0.255 (0.255)\tLoss 0.1913 (0.1913)\tPrec 93.750% (93.750%)\n",
      "Epoch: [39][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.1368 (0.2013)\tPrec 95.312% (93.038%)\n",
      "Epoch: [39][200/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.2349 (0.2135)\tPrec 91.406% (92.771%)\n",
      "Epoch: [39][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.2405 (0.2160)\tPrec 93.750% (92.699%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.4853 (0.4853)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.160% \n",
      "best acc: 87.510000\n",
      "Epoch: [40][0/391]\tTime 0.327 (0.327)\tData 0.279 (0.279)\tLoss 0.1622 (0.1622)\tPrec 93.750% (93.750%)\n",
      "Epoch: [40][100/391]\tTime 0.042 (0.047)\tData 0.002 (0.005)\tLoss 0.1882 (0.1983)\tPrec 93.750% (93.147%)\n",
      "Epoch: [40][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1966 (0.2024)\tPrec 93.750% (93.140%)\n",
      "Epoch: [40][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2297 (0.2078)\tPrec 92.969% (92.974%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.4638 (0.4638)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.140% \n",
      "best acc: 87.510000\n",
      "Epoch: [41][0/391]\tTime 0.301 (0.301)\tData 0.259 (0.259)\tLoss 0.2564 (0.2564)\tPrec 91.406% (91.406%)\n",
      "Epoch: [41][100/391]\tTime 0.050 (0.048)\tData 0.002 (0.005)\tLoss 0.1880 (0.1942)\tPrec 94.531% (93.564%)\n",
      "Epoch: [41][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1312 (0.1986)\tPrec 96.094% (93.420%)\n",
      "Epoch: [41][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.003)\tLoss 0.2907 (0.2014)\tPrec 91.406% (93.259%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2720 (0.2720)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.420% \n",
      "best acc: 87.510000\n",
      "Epoch: [42][0/391]\tTime 0.289 (0.289)\tData 0.242 (0.242)\tLoss 0.0811 (0.0811)\tPrec 98.438% (98.438%)\n",
      "Epoch: [42][100/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.2169 (0.1885)\tPrec 92.969% (93.649%)\n",
      "Epoch: [42][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2243 (0.1974)\tPrec 91.406% (93.478%)\n",
      "Epoch: [42][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.2298 (0.2025)\tPrec 92.969% (93.291%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.2873 (0.2873)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.950% \n",
      "best acc: 87.950000\n",
      "Epoch: [43][0/391]\tTime 0.292 (0.292)\tData 0.248 (0.248)\tLoss 0.1672 (0.1672)\tPrec 94.531% (94.531%)\n",
      "Epoch: [43][100/391]\tTime 0.043 (0.048)\tData 0.001 (0.004)\tLoss 0.1107 (0.1803)\tPrec 96.875% (94.059%)\n",
      "Epoch: [43][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2543 (0.1948)\tPrec 91.406% (93.540%)\n",
      "Epoch: [43][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.2483 (0.1970)\tPrec 91.406% (93.496%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.3548 (0.3548)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.820% \n",
      "best acc: 87.950000\n",
      "Epoch: [44][0/391]\tTime 0.291 (0.291)\tData 0.243 (0.243)\tLoss 0.2380 (0.2380)\tPrec 93.750% (93.750%)\n",
      "Epoch: [44][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.1975 (0.1925)\tPrec 94.531% (93.649%)\n",
      "Epoch: [44][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1847 (0.1969)\tPrec 92.969% (93.521%)\n",
      "Epoch: [44][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2350 (0.1979)\tPrec 93.750% (93.423%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.2638 (0.2638)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.340% \n",
      "best acc: 87.950000\n",
      "Epoch: [45][0/391]\tTime 0.288 (0.288)\tData 0.236 (0.236)\tLoss 0.2960 (0.2960)\tPrec 88.281% (88.281%)\n",
      "Epoch: [45][100/391]\tTime 0.042 (0.047)\tData 0.001 (0.004)\tLoss 0.1292 (0.1876)\tPrec 95.312% (93.657%)\n",
      "Epoch: [45][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1747 (0.1867)\tPrec 93.750% (93.754%)\n",
      "Epoch: [45][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.2161 (0.1940)\tPrec 92.969% (93.467%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.1942 (0.1942)\tPrec 92.969% (92.969%)\n",
      " * Prec 88.170% \n",
      "best acc: 88.170000\n",
      "Epoch: [46][0/391]\tTime 0.414 (0.414)\tData 0.374 (0.374)\tLoss 0.1732 (0.1732)\tPrec 95.312% (95.312%)\n",
      "Epoch: [46][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.005)\tLoss 0.1513 (0.1719)\tPrec 94.531% (94.028%)\n",
      "Epoch: [46][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2400 (0.1841)\tPrec 92.969% (93.703%)\n",
      "Epoch: [46][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.2412 (0.1885)\tPrec 93.750% (93.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2392 (0.2392)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.410% \n",
      "best acc: 88.170000\n",
      "Epoch: [47][0/391]\tTime 0.289 (0.289)\tData 0.258 (0.258)\tLoss 0.1792 (0.1792)\tPrec 94.531% (94.531%)\n",
      "Epoch: [47][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.2194 (0.1828)\tPrec 92.188% (93.912%)\n",
      "Epoch: [47][200/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.1383 (0.1873)\tPrec 94.531% (93.793%)\n",
      "Epoch: [47][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1773 (0.1896)\tPrec 92.969% (93.607%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.4334 (0.4334)\tPrec 85.938% (85.938%)\n",
      " * Prec 87.190% \n",
      "best acc: 88.170000\n",
      "Epoch: [48][0/391]\tTime 0.304 (0.304)\tData 0.260 (0.260)\tLoss 0.2437 (0.2437)\tPrec 90.625% (90.625%)\n",
      "Epoch: [48][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.0845 (0.1766)\tPrec 97.656% (93.928%)\n",
      "Epoch: [48][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.1886 (0.1823)\tPrec 93.750% (93.742%)\n",
      "Epoch: [48][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.2227 (0.1874)\tPrec 91.406% (93.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.3929 (0.3929)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.310% \n",
      "best acc: 88.170000\n",
      "Epoch: [49][0/391]\tTime 0.322 (0.322)\tData 0.280 (0.280)\tLoss 0.1620 (0.1620)\tPrec 94.531% (94.531%)\n",
      "Epoch: [49][100/391]\tTime 0.052 (0.048)\tData 0.001 (0.004)\tLoss 0.1757 (0.1770)\tPrec 90.625% (94.075%)\n",
      "Epoch: [49][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.2967 (0.1802)\tPrec 89.062% (93.929%)\n",
      "Epoch: [49][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.1822 (0.1834)\tPrec 91.406% (93.846%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.3201 (0.3201)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.760% \n",
      "best acc: 88.170000\n",
      "Epoch: [50][0/391]\tTime 0.280 (0.280)\tData 0.238 (0.238)\tLoss 0.1593 (0.1593)\tPrec 93.750% (93.750%)\n",
      "Epoch: [50][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.1706 (0.1670)\tPrec 94.531% (94.160%)\n",
      "Epoch: [50][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.2313 (0.1741)\tPrec 92.969% (94.038%)\n",
      "Epoch: [50][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.1339 (0.1763)\tPrec 97.656% (93.991%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.3731 (0.3731)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.580% \n",
      "best acc: 88.170000\n",
      "Epoch: [51][0/391]\tTime 0.296 (0.296)\tData 0.248 (0.248)\tLoss 0.1382 (0.1382)\tPrec 93.750% (93.750%)\n",
      "Epoch: [51][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.1780 (0.1701)\tPrec 94.531% (94.431%)\n",
      "Epoch: [51][200/391]\tTime 0.050 (0.046)\tData 0.002 (0.003)\tLoss 0.1412 (0.1749)\tPrec 93.750% (94.162%)\n",
      "Epoch: [51][300/391]\tTime 0.049 (0.046)\tData 0.002 (0.003)\tLoss 0.2124 (0.1761)\tPrec 92.969% (94.095%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.3895 (0.3895)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.020% \n",
      "best acc: 88.170000\n",
      "Epoch: [52][0/391]\tTime 0.268 (0.268)\tData 0.229 (0.229)\tLoss 0.2033 (0.2033)\tPrec 91.406% (91.406%)\n",
      "Epoch: [52][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.004)\tLoss 0.1434 (0.1705)\tPrec 94.531% (94.245%)\n",
      "Epoch: [52][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2087 (0.1726)\tPrec 92.188% (94.127%)\n",
      "Epoch: [52][300/391]\tTime 0.048 (0.046)\tData 0.001 (0.002)\tLoss 0.3552 (0.1749)\tPrec 89.844% (94.093%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2652 (0.2652)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.560% \n",
      "best acc: 88.560000\n",
      "Epoch: [53][0/391]\tTime 0.304 (0.304)\tData 0.258 (0.258)\tLoss 0.1505 (0.1505)\tPrec 94.531% (94.531%)\n",
      "Epoch: [53][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.1740 (0.1491)\tPrec 94.531% (94.995%)\n",
      "Epoch: [53][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.1983 (0.1545)\tPrec 92.969% (94.807%)\n",
      "Epoch: [53][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1891 (0.1631)\tPrec 93.750% (94.430%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.3107 (0.3107)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.150% \n",
      "best acc: 88.560000\n",
      "Epoch: [54][0/391]\tTime 0.285 (0.285)\tData 0.240 (0.240)\tLoss 0.1045 (0.1045)\tPrec 95.312% (95.312%)\n",
      "Epoch: [54][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2418 (0.1608)\tPrec 94.531% (94.632%)\n",
      "Epoch: [54][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2578 (0.1659)\tPrec 88.281% (94.477%)\n",
      "Epoch: [54][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.1915 (0.1640)\tPrec 95.312% (94.523%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.259 (0.259)\tLoss 0.3441 (0.3441)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.800% \n",
      "best acc: 88.560000\n",
      "Epoch: [55][0/391]\tTime 0.366 (0.366)\tData 0.317 (0.317)\tLoss 0.2229 (0.2229)\tPrec 92.188% (92.188%)\n",
      "Epoch: [55][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.1420 (0.1428)\tPrec 96.875% (95.243%)\n",
      "Epoch: [55][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.1289 (0.1546)\tPrec 97.656% (94.831%)\n",
      "Epoch: [55][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.1762 (0.1648)\tPrec 95.312% (94.516%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.2877 (0.2877)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.920% \n",
      "best acc: 88.560000\n",
      "Epoch: [56][0/391]\tTime 0.270 (0.270)\tData 0.229 (0.229)\tLoss 0.0776 (0.0776)\tPrec 96.875% (96.875%)\n",
      "Epoch: [56][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2112 (0.1518)\tPrec 92.188% (94.748%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.1440 (0.1574)\tPrec 93.750% (94.558%)\n",
      "Epoch: [56][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1912 (0.1639)\tPrec 94.531% (94.386%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.2832 (0.2832)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.580% \n",
      "best acc: 88.560000\n",
      "Epoch: [57][0/391]\tTime 0.270 (0.270)\tData 0.230 (0.230)\tLoss 0.1472 (0.1472)\tPrec 93.750% (93.750%)\n",
      "Epoch: [57][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.1122 (0.1479)\tPrec 96.094% (94.918%)\n",
      "Epoch: [57][200/391]\tTime 0.054 (0.047)\tData 0.002 (0.003)\tLoss 0.1857 (0.1561)\tPrec 93.750% (94.601%)\n",
      "Epoch: [57][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2693 (0.1669)\tPrec 92.188% (94.228%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.3369 (0.3369)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.430% \n",
      "best acc: 88.560000\n",
      "Epoch: [58][0/391]\tTime 0.281 (0.281)\tData 0.236 (0.236)\tLoss 0.0977 (0.0977)\tPrec 97.656% (97.656%)\n",
      "Epoch: [58][100/391]\tTime 0.044 (0.048)\tData 0.002 (0.004)\tLoss 0.1979 (0.1501)\tPrec 95.312% (95.135%)\n",
      "Epoch: [58][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.1272 (0.1575)\tPrec 94.531% (94.792%)\n",
      "Epoch: [58][300/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.1722 (0.1623)\tPrec 94.531% (94.549%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3682 (0.3682)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.140% \n",
      "best acc: 88.560000\n",
      "Epoch: [59][0/391]\tTime 0.355 (0.355)\tData 0.308 (0.308)\tLoss 0.2804 (0.2804)\tPrec 87.500% (87.500%)\n",
      "Epoch: [59][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.1784 (0.1606)\tPrec 93.750% (94.493%)\n",
      "Epoch: [59][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.1318 (0.1676)\tPrec 96.094% (94.220%)\n",
      "Epoch: [59][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.3158 (0.1670)\tPrec 87.500% (94.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.3804 (0.3804)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.370% \n",
      "best acc: 88.560000\n",
      "Epoch: [60][0/391]\tTime 0.361 (0.361)\tData 0.315 (0.315)\tLoss 0.1662 (0.1662)\tPrec 95.312% (95.312%)\n",
      "Epoch: [60][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.005)\tLoss 0.1462 (0.1585)\tPrec 95.312% (94.694%)\n",
      "Epoch: [60][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.0479 (0.1571)\tPrec 100.000% (94.827%)\n",
      "Epoch: [60][300/391]\tTime 0.047 (0.046)\tData 0.001 (0.003)\tLoss 0.1501 (0.1590)\tPrec 95.312% (94.716%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.2510 (0.2510)\tPrec 87.500% (87.500%)\n",
      " * Prec 87.700% \n",
      "best acc: 88.560000\n",
      "Epoch: [61][0/391]\tTime 0.292 (0.292)\tData 0.253 (0.253)\tLoss 0.1315 (0.1315)\tPrec 96.094% (96.094%)\n",
      "Epoch: [61][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.1603 (0.1439)\tPrec 96.094% (95.181%)\n",
      "Epoch: [61][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2853 (0.1513)\tPrec 92.188% (94.897%)\n",
      "Epoch: [61][300/391]\tTime 0.042 (0.046)\tData 0.002 (0.002)\tLoss 0.1380 (0.1586)\tPrec 95.312% (94.586%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.4066 (0.4066)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.620% \n",
      "best acc: 88.560000\n",
      "Epoch: [62][0/391]\tTime 0.289 (0.289)\tData 0.242 (0.242)\tLoss 0.1424 (0.1424)\tPrec 92.969% (92.969%)\n",
      "Epoch: [62][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.1536 (0.1483)\tPrec 94.531% (95.111%)\n",
      "Epoch: [62][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2058 (0.1534)\tPrec 91.406% (94.866%)\n",
      "Epoch: [62][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1074 (0.1570)\tPrec 96.875% (94.690%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2405 (0.2405)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.350% \n",
      "best acc: 88.560000\n",
      "Epoch: [63][0/391]\tTime 0.278 (0.278)\tData 0.233 (0.233)\tLoss 0.1416 (0.1416)\tPrec 94.531% (94.531%)\n",
      "Epoch: [63][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.1889 (0.1488)\tPrec 95.312% (94.964%)\n",
      "Epoch: [63][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2580 (0.1530)\tPrec 91.406% (94.831%)\n",
      "Epoch: [63][300/391]\tTime 0.044 (0.046)\tData 0.002 (0.002)\tLoss 0.1260 (0.1559)\tPrec 95.312% (94.682%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.3382 (0.3382)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.530% \n",
      "best acc: 88.560000\n",
      "Epoch: [64][0/391]\tTime 0.299 (0.299)\tData 0.253 (0.253)\tLoss 0.1414 (0.1414)\tPrec 95.312% (95.312%)\n",
      "Epoch: [64][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2080 (0.1546)\tPrec 93.750% (94.841%)\n",
      "Epoch: [64][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1785 (0.1581)\tPrec 94.531% (94.679%)\n",
      "Epoch: [64][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.2091 (0.1584)\tPrec 92.969% (94.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.2738 (0.2738)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.070% \n",
      "best acc: 88.560000\n",
      "Epoch: [65][0/391]\tTime 0.271 (0.271)\tData 0.229 (0.229)\tLoss 0.1765 (0.1765)\tPrec 93.750% (93.750%)\n",
      "Epoch: [65][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.1135 (0.1386)\tPrec 97.656% (95.251%)\n",
      "Epoch: [65][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.0757 (0.1437)\tPrec 96.875% (95.106%)\n",
      "Epoch: [65][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.1640 (0.1479)\tPrec 94.531% (94.957%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.4016 (0.4016)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.960% \n",
      "best acc: 88.560000\n",
      "Epoch: [66][0/391]\tTime 0.286 (0.286)\tData 0.241 (0.241)\tLoss 0.0940 (0.0940)\tPrec 97.656% (97.656%)\n",
      "Epoch: [66][100/391]\tTime 0.042 (0.047)\tData 0.001 (0.004)\tLoss 0.0966 (0.1438)\tPrec 98.438% (95.173%)\n",
      "Epoch: [66][200/391]\tTime 0.042 (0.046)\tData 0.001 (0.003)\tLoss 0.1814 (0.1477)\tPrec 95.312% (95.060%)\n",
      "Epoch: [66][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.0966 (0.1508)\tPrec 96.875% (94.902%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.3098 (0.3098)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.680% \n",
      "best acc: 88.560000\n",
      "Epoch: [67][0/391]\tTime 0.294 (0.294)\tData 0.246 (0.246)\tLoss 0.0914 (0.0914)\tPrec 97.656% (97.656%)\n",
      "Epoch: [67][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.1301 (0.1441)\tPrec 95.312% (95.050%)\n",
      "Epoch: [67][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.0938 (0.1458)\tPrec 96.094% (95.060%)\n",
      "Epoch: [67][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.1788 (0.1520)\tPrec 93.750% (94.853%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.3694 (0.3694)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.390% \n",
      "best acc: 88.560000\n",
      "Epoch: [68][0/391]\tTime 0.306 (0.306)\tData 0.261 (0.261)\tLoss 0.2958 (0.2958)\tPrec 90.625% (90.625%)\n",
      "Epoch: [68][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.2381 (0.1427)\tPrec 90.625% (95.305%)\n",
      "Epoch: [68][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.1614 (0.1447)\tPrec 96.875% (95.293%)\n",
      "Epoch: [68][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1737 (0.1491)\tPrec 93.750% (95.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.4039 (0.4039)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.270% \n",
      "best acc: 88.560000\n",
      "Epoch: [69][0/391]\tTime 0.278 (0.278)\tData 0.233 (0.233)\tLoss 0.0713 (0.0713)\tPrec 98.438% (98.438%)\n",
      "Epoch: [69][100/391]\tTime 0.043 (0.047)\tData 0.001 (0.004)\tLoss 0.1311 (0.1474)\tPrec 96.094% (95.050%)\n",
      "Epoch: [69][200/391]\tTime 0.050 (0.046)\tData 0.002 (0.003)\tLoss 0.1554 (0.1556)\tPrec 96.875% (94.764%)\n",
      "Epoch: [69][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1482 (0.1537)\tPrec 92.969% (94.843%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.2865 (0.2865)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.880% \n",
      "best acc: 88.880000\n",
      "Epoch: [70][0/391]\tTime 0.298 (0.298)\tData 0.251 (0.251)\tLoss 0.2128 (0.2128)\tPrec 93.750% (93.750%)\n",
      "Epoch: [70][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2191 (0.1445)\tPrec 92.188% (95.096%)\n",
      "Epoch: [70][200/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.1046 (0.1422)\tPrec 96.875% (95.165%)\n",
      "Epoch: [70][300/391]\tTime 0.047 (0.046)\tData 0.001 (0.002)\tLoss 0.1147 (0.1489)\tPrec 96.875% (94.980%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2701 (0.2701)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.220% \n",
      "best acc: 88.880000\n",
      "Epoch: [71][0/391]\tTime 0.285 (0.285)\tData 0.239 (0.239)\tLoss 0.1415 (0.1415)\tPrec 96.094% (96.094%)\n",
      "Epoch: [71][100/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.2185 (0.1365)\tPrec 92.188% (95.436%)\n",
      "Epoch: [71][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.1566 (0.1482)\tPrec 93.750% (94.970%)\n",
      "Epoch: [71][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.1040 (0.1482)\tPrec 97.656% (94.934%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.5194 (0.5194)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.600% \n",
      "best acc: 88.880000\n",
      "Epoch: [72][0/391]\tTime 0.296 (0.296)\tData 0.253 (0.253)\tLoss 0.1853 (0.1853)\tPrec 95.312% (95.312%)\n",
      "Epoch: [72][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.1534 (0.1553)\tPrec 95.312% (94.817%)\n",
      "Epoch: [72][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1424 (0.1545)\tPrec 96.875% (94.877%)\n",
      "Epoch: [72][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1114 (0.1558)\tPrec 94.531% (94.780%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.2867 (0.2867)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.170% \n",
      "best acc: 88.880000\n",
      "Epoch: [73][0/391]\tTime 0.326 (0.326)\tData 0.280 (0.280)\tLoss 0.1437 (0.1437)\tPrec 95.312% (95.312%)\n",
      "Epoch: [73][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.0790 (0.1361)\tPrec 97.656% (95.444%)\n",
      "Epoch: [73][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.1842 (0.1441)\tPrec 92.969% (95.196%)\n",
      "Epoch: [73][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.0876 (0.1503)\tPrec 98.438% (94.970%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.264 (0.264)\tLoss 0.4148 (0.4148)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.070% \n",
      "best acc: 88.880000\n",
      "Epoch: [74][0/391]\tTime 0.305 (0.305)\tData 0.259 (0.259)\tLoss 0.1562 (0.1562)\tPrec 95.312% (95.312%)\n",
      "Epoch: [74][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.2248 (0.1310)\tPrec 92.969% (95.560%)\n",
      "Epoch: [74][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1996 (0.1386)\tPrec 95.312% (95.301%)\n",
      "Epoch: [74][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.0626 (0.1432)\tPrec 98.438% (95.159%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.2693 (0.2693)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.610% \n",
      "best acc: 88.880000\n",
      "Epoch: [75][0/391]\tTime 0.322 (0.322)\tData 0.271 (0.271)\tLoss 0.1246 (0.1246)\tPrec 94.531% (94.531%)\n",
      "Epoch: [75][100/391]\tTime 0.044 (0.048)\tData 0.001 (0.004)\tLoss 0.0938 (0.1436)\tPrec 97.656% (95.142%)\n",
      "Epoch: [75][200/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.1328 (0.1411)\tPrec 94.531% (95.266%)\n",
      "Epoch: [75][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.0875 (0.1476)\tPrec 96.875% (95.022%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.275 (0.275)\tLoss 0.2662 (0.2662)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.950% \n",
      "best acc: 88.880000\n",
      "Epoch: [76][0/391]\tTime 0.316 (0.316)\tData 0.267 (0.267)\tLoss 0.1451 (0.1451)\tPrec 95.312% (95.312%)\n",
      "Epoch: [76][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.005)\tLoss 0.1184 (0.1363)\tPrec 95.312% (95.444%)\n",
      "Epoch: [76][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.1244 (0.1362)\tPrec 97.656% (95.445%)\n",
      "Epoch: [76][300/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.1444 (0.1396)\tPrec 96.875% (95.284%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.2279 (0.2279)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.040% \n",
      "best acc: 88.880000\n",
      "Epoch: [77][0/391]\tTime 0.302 (0.302)\tData 0.260 (0.260)\tLoss 0.1720 (0.1720)\tPrec 94.531% (94.531%)\n",
      "Epoch: [77][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.1155 (0.1370)\tPrec 94.531% (95.421%)\n",
      "Epoch: [77][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.0939 (0.1447)\tPrec 97.656% (95.122%)\n",
      "Epoch: [77][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1066 (0.1445)\tPrec 96.094% (95.162%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.2475 (0.2475)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.770% \n",
      "best acc: 88.880000\n",
      "Epoch: [78][0/391]\tTime 0.276 (0.276)\tData 0.231 (0.231)\tLoss 0.0951 (0.0951)\tPrec 97.656% (97.656%)\n",
      "Epoch: [78][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.1056 (0.1234)\tPrec 96.875% (96.001%)\n",
      "Epoch: [78][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.0710 (0.1270)\tPrec 98.438% (95.791%)\n",
      "Epoch: [78][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.1647 (0.1322)\tPrec 93.750% (95.564%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.2998 (0.2998)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.940% \n",
      "best acc: 88.880000\n",
      "Epoch: [79][0/391]\tTime 0.348 (0.348)\tData 0.301 (0.301)\tLoss 0.1482 (0.1482)\tPrec 95.312% (95.312%)\n",
      "Epoch: [79][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.005)\tLoss 0.1330 (0.1325)\tPrec 95.312% (95.444%)\n",
      "Epoch: [79][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.003)\tLoss 0.2375 (0.1355)\tPrec 89.844% (95.433%)\n",
      "Epoch: [79][300/391]\tTime 0.046 (0.046)\tData 0.003 (0.003)\tLoss 0.1782 (0.1369)\tPrec 92.969% (95.388%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.3381 (0.3381)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.940% \n",
      "best acc: 88.880000\n",
      "Epoch: [80][0/391]\tTime 0.288 (0.288)\tData 0.250 (0.250)\tLoss 0.0769 (0.0769)\tPrec 98.438% (98.438%)\n",
      "Epoch: [80][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.1709 (0.1252)\tPrec 94.531% (95.869%)\n",
      "Epoch: [80][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1699 (0.1313)\tPrec 96.094% (95.515%)\n",
      "Epoch: [80][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.1489 (0.1362)\tPrec 96.094% (95.372%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3686 (0.3686)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.370% \n",
      "best acc: 88.880000\n",
      "Epoch: [81][0/391]\tTime 0.303 (0.303)\tData 0.255 (0.255)\tLoss 0.1631 (0.1631)\tPrec 94.531% (94.531%)\n",
      "Epoch: [81][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.1242 (0.1270)\tPrec 95.312% (95.545%)\n",
      "Epoch: [81][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.003)\tLoss 0.0506 (0.1376)\tPrec 100.000% (95.351%)\n",
      "Epoch: [81][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1381 (0.1383)\tPrec 94.531% (95.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.3749 (0.3749)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.870% \n",
      "best acc: 88.880000\n",
      "Epoch: [82][0/391]\tTime 0.264 (0.264)\tData 0.223 (0.223)\tLoss 0.1838 (0.1838)\tPrec 95.312% (95.312%)\n",
      "Epoch: [82][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.1179 (0.1272)\tPrec 96.094% (95.699%)\n",
      "Epoch: [82][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.1302 (0.1354)\tPrec 94.531% (95.421%)\n",
      "Epoch: [82][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.0987 (0.1362)\tPrec 96.094% (95.455%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.3860 (0.3860)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.770% \n",
      "best acc: 88.880000\n",
      "Epoch: [83][0/391]\tTime 0.358 (0.358)\tData 0.313 (0.313)\tLoss 0.1050 (0.1050)\tPrec 96.094% (96.094%)\n",
      "Epoch: [83][100/391]\tTime 0.043 (0.048)\tData 0.002 (0.005)\tLoss 0.1177 (0.1311)\tPrec 97.656% (95.514%)\n",
      "Epoch: [83][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.1375 (0.1357)\tPrec 94.531% (95.441%)\n",
      "Epoch: [83][300/391]\tTime 0.043 (0.047)\tData 0.004 (0.003)\tLoss 0.1340 (0.1415)\tPrec 96.094% (95.219%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.2593 (0.2593)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.870% \n",
      "best acc: 88.880000\n",
      "Epoch: [84][0/391]\tTime 0.288 (0.288)\tData 0.242 (0.242)\tLoss 0.1610 (0.1610)\tPrec 94.531% (94.531%)\n",
      "Epoch: [84][100/391]\tTime 0.042 (0.047)\tData 0.002 (0.004)\tLoss 0.1090 (0.1204)\tPrec 96.875% (95.924%)\n",
      "Epoch: [84][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.1115 (0.1305)\tPrec 96.875% (95.596%)\n",
      "Epoch: [84][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.002)\tLoss 0.1770 (0.1311)\tPrec 92.188% (95.567%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.3212 (0.3212)\tPrec 92.188% (92.188%)\n",
      " * Prec 89.060% \n",
      "best acc: 89.060000\n",
      "Epoch: [85][0/391]\tTime 0.303 (0.303)\tData 0.255 (0.255)\tLoss 0.1092 (0.1092)\tPrec 96.875% (96.875%)\n",
      "Epoch: [85][100/391]\tTime 0.047 (0.047)\tData 0.003 (0.004)\tLoss 0.0981 (0.1291)\tPrec 97.656% (95.684%)\n",
      "Epoch: [85][200/391]\tTime 0.048 (0.046)\tData 0.002 (0.003)\tLoss 0.0972 (0.1351)\tPrec 95.312% (95.367%)\n",
      "Epoch: [85][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.1096 (0.1353)\tPrec 96.094% (95.370%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.2125 (0.2125)\tPrec 93.750% (93.750%)\n",
      " * Prec 87.910% \n",
      "best acc: 89.060000\n",
      "Epoch: [86][0/391]\tTime 0.276 (0.276)\tData 0.227 (0.227)\tLoss 0.1674 (0.1674)\tPrec 92.969% (92.969%)\n",
      "Epoch: [86][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.0971 (0.1284)\tPrec 97.656% (95.668%)\n",
      "Epoch: [86][200/391]\tTime 0.044 (0.046)\tData 0.001 (0.003)\tLoss 0.1166 (0.1316)\tPrec 95.312% (95.542%)\n",
      "Epoch: [86][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.0796 (0.1313)\tPrec 96.875% (95.536%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.4067 (0.4067)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.590% \n",
      "best acc: 89.060000\n",
      "Epoch: [87][0/391]\tTime 0.327 (0.327)\tData 0.280 (0.280)\tLoss 0.0577 (0.0577)\tPrec 98.438% (98.438%)\n",
      "Epoch: [87][100/391]\tTime 0.042 (0.048)\tData 0.002 (0.004)\tLoss 0.0691 (0.1255)\tPrec 97.656% (95.761%)\n",
      "Epoch: [87][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.003)\tLoss 0.1492 (0.1302)\tPrec 94.531% (95.569%)\n",
      "Epoch: [87][300/391]\tTime 0.044 (0.046)\tData 0.002 (0.003)\tLoss 0.2051 (0.1356)\tPrec 92.188% (95.398%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2836 (0.2836)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.580% \n",
      "best acc: 89.060000\n",
      "Epoch: [88][0/391]\tTime 0.271 (0.271)\tData 0.224 (0.224)\tLoss 0.1061 (0.1061)\tPrec 96.875% (96.875%)\n",
      "Epoch: [88][100/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2317 (0.1178)\tPrec 92.188% (96.109%)\n",
      "Epoch: [88][200/391]\tTime 0.045 (0.046)\tData 0.002 (0.003)\tLoss 0.2834 (0.1291)\tPrec 89.062% (95.748%)\n",
      "Epoch: [88][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1182 (0.1340)\tPrec 96.875% (95.562%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.216 (0.216)\tLoss 0.2574 (0.2574)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.150% \n",
      "best acc: 89.060000\n",
      "Epoch: [89][0/391]\tTime 0.277 (0.277)\tData 0.229 (0.229)\tLoss 0.0596 (0.0596)\tPrec 98.438% (98.438%)\n",
      "Epoch: [89][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.1937 (0.1227)\tPrec 95.312% (95.777%)\n",
      "Epoch: [89][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1104 (0.1258)\tPrec 96.875% (95.794%)\n",
      "Epoch: [89][300/391]\tTime 0.049 (0.046)\tData 0.002 (0.002)\tLoss 0.2252 (0.1266)\tPrec 92.188% (95.754%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.1964 (0.1964)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.610% \n",
      "best acc: 89.060000\n",
      "Epoch: [90][0/391]\tTime 0.294 (0.294)\tData 0.249 (0.249)\tLoss 0.0898 (0.0898)\tPrec 96.875% (96.875%)\n",
      "Epoch: [90][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.1400 (0.1226)\tPrec 96.875% (96.001%)\n",
      "Epoch: [90][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.1238 (0.1217)\tPrec 94.531% (95.938%)\n",
      "Epoch: [90][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1531 (0.1248)\tPrec 93.750% (95.754%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.4013 (0.4013)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.970% \n",
      "best acc: 89.060000\n",
      "Epoch: [91][0/391]\tTime 0.281 (0.281)\tData 0.235 (0.235)\tLoss 0.1840 (0.1840)\tPrec 94.531% (94.531%)\n",
      "Epoch: [91][100/391]\tTime 0.044 (0.047)\tData 0.001 (0.004)\tLoss 0.1003 (0.1174)\tPrec 95.312% (96.001%)\n",
      "Epoch: [91][200/391]\tTime 0.042 (0.046)\tData 0.001 (0.003)\tLoss 0.1475 (0.1253)\tPrec 95.312% (95.701%)\n",
      "Epoch: [91][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.1355 (0.1241)\tPrec 96.094% (95.780%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.2194 (0.2194)\tPrec 92.969% (92.969%)\n",
      " * Prec 89.330% \n",
      "best acc: 89.330000\n",
      "Epoch: [92][0/391]\tTime 0.361 (0.361)\tData 0.314 (0.314)\tLoss 0.1535 (0.1535)\tPrec 93.750% (93.750%)\n",
      "Epoch: [92][100/391]\tTime 0.048 (0.048)\tData 0.002 (0.005)\tLoss 0.2146 (0.1186)\tPrec 93.750% (95.924%)\n",
      "Epoch: [92][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.1707 (0.1232)\tPrec 94.531% (95.903%)\n",
      "Epoch: [92][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.0698 (0.1277)\tPrec 97.656% (95.712%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.2808 (0.2808)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.190% \n",
      "best acc: 89.330000\n",
      "Epoch: [93][0/391]\tTime 0.249 (0.249)\tData 0.208 (0.208)\tLoss 0.1093 (0.1093)\tPrec 96.094% (96.094%)\n",
      "Epoch: [93][100/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.2110 (0.1292)\tPrec 92.188% (95.537%)\n",
      "Epoch: [93][200/391]\tTime 0.043 (0.046)\tData 0.001 (0.003)\tLoss 0.1262 (0.1232)\tPrec 93.750% (95.775%)\n",
      "Epoch: [93][300/391]\tTime 0.043 (0.046)\tData 0.002 (0.002)\tLoss 0.1353 (0.1259)\tPrec 94.531% (95.723%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2498 (0.2498)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.700% \n",
      "best acc: 89.330000\n",
      "Epoch: [94][0/391]\tTime 0.300 (0.300)\tData 0.254 (0.254)\tLoss 0.1663 (0.1663)\tPrec 94.531% (94.531%)\n",
      "Epoch: [94][100/391]\tTime 0.048 (0.048)\tData 0.002 (0.004)\tLoss 0.1150 (0.1205)\tPrec 96.875% (96.001%)\n",
      "Epoch: [94][200/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.0714 (0.1289)\tPrec 98.438% (95.701%)\n",
      "Epoch: [94][300/391]\tTime 0.043 (0.046)\tData 0.001 (0.002)\tLoss 0.0717 (0.1311)\tPrec 96.875% (95.653%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2875 (0.2875)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.000% \n",
      "best acc: 89.330000\n",
      "Epoch: [95][0/391]\tTime 0.283 (0.283)\tData 0.242 (0.242)\tLoss 0.0648 (0.0648)\tPrec 96.875% (96.875%)\n",
      "Epoch: [95][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.0953 (0.1219)\tPrec 95.312% (95.924%)\n",
      "Epoch: [95][200/391]\tTime 0.044 (0.046)\tData 0.003 (0.003)\tLoss 0.1695 (0.1230)\tPrec 93.750% (95.837%)\n",
      "Epoch: [95][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.0877 (0.1226)\tPrec 96.875% (95.832%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3201 (0.3201)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.810% \n",
      "best acc: 89.330000\n",
      "Epoch: [96][0/391]\tTime 0.275 (0.275)\tData 0.228 (0.228)\tLoss 0.0751 (0.0751)\tPrec 96.875% (96.875%)\n",
      "Epoch: [96][100/391]\tTime 0.041 (0.047)\tData 0.002 (0.004)\tLoss 0.2202 (0.1027)\tPrec 92.188% (96.612%)\n",
      "Epoch: [96][200/391]\tTime 0.039 (0.046)\tData 0.002 (0.003)\tLoss 0.0647 (0.1159)\tPrec 98.438% (96.133%)\n",
      "Epoch: [96][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.002)\tLoss 0.0740 (0.1197)\tPrec 97.656% (96.042%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.1752 (0.1752)\tPrec 93.750% (93.750%)\n",
      " * Prec 89.290% \n",
      "best acc: 89.330000\n",
      "Epoch: [97][0/391]\tTime 0.273 (0.273)\tData 0.229 (0.229)\tLoss 0.1122 (0.1122)\tPrec 96.094% (96.094%)\n",
      "Epoch: [97][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.0716 (0.1111)\tPrec 96.875% (96.272%)\n",
      "Epoch: [97][200/391]\tTime 0.043 (0.046)\tData 0.002 (0.003)\tLoss 0.1011 (0.1203)\tPrec 94.531% (95.884%)\n",
      "Epoch: [97][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.003)\tLoss 0.2073 (0.1266)\tPrec 93.750% (95.715%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.2519 (0.2519)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.480% \n",
      "best acc: 89.330000\n",
      "Epoch: [98][0/391]\tTime 0.273 (0.273)\tData 0.235 (0.235)\tLoss 0.1544 (0.1544)\tPrec 95.312% (95.312%)\n",
      "Epoch: [98][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.004)\tLoss 0.1217 (0.1115)\tPrec 96.875% (96.310%)\n",
      "Epoch: [98][200/391]\tTime 0.046 (0.046)\tData 0.001 (0.003)\tLoss 0.0837 (0.1166)\tPrec 96.875% (96.117%)\n",
      "Epoch: [98][300/391]\tTime 0.044 (0.046)\tData 0.001 (0.002)\tLoss 0.2301 (0.1209)\tPrec 89.844% (95.904%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.3803 (0.3803)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.760% \n",
      "best acc: 89.330000\n",
      "Epoch: [99][0/391]\tTime 0.286 (0.286)\tData 0.243 (0.243)\tLoss 0.1000 (0.1000)\tPrec 97.656% (97.656%)\n",
      "Epoch: [99][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.0518 (0.1251)\tPrec 98.438% (96.016%)\n",
      "Epoch: [99][200/391]\tTime 0.046 (0.046)\tData 0.002 (0.003)\tLoss 0.0586 (0.1245)\tPrec 97.656% (95.934%)\n",
      "Epoch: [99][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.002)\tLoss 0.1652 (0.1243)\tPrec 96.094% (95.912%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.4088 (0.4088)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.720% \n",
      "best acc: 89.330000\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "weight_decay = 1e-3\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# weight decay: for regularization to prevent overfitting\n",
    "     \n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "    \n",
    "fdir = 'result/'+str(model_name)+str('_1206_2bit')\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "#PATH = \"result/VGG16_quant1129_5/model_best.pth.tar\"\n",
    "#checkpoint = torch.load(PATH)\n",
    "#model.load_state_dict(checkpoint['state_dict'])\n",
    "#device = torch.device(\"cuda\") \n",
    "        \n",
    "        \n",
    "        \n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9127/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant1130_2_newmodel/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driving-tanzania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "QuantConv2d(\n",
      "  3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 1\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 2\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 3\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 4\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 5\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 6\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 7\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 8\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 9\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 10\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 11\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 12\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 13\n"
     ]
    }
   ],
   "source": [
    "## Send an image and use prehook to grab the inputs of all the QuantConv2d layers\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "counter =0\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        counter += 1\n",
    "        print(layer, counter)\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e6a298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[[[-0.6771,  0.3386,  0.3386],\n",
      "          [ 0.0000,  1.6928,  1.0157],\n",
      "          [ 1.3543,  2.0314,  1.3543]],\n",
      "\n",
      "         [[-0.3386, -0.6771, -0.6771],\n",
      "          [-1.0157, -0.3386,  0.0000],\n",
      "          [-0.6771,  0.6771,  0.3386]],\n",
      "\n",
      "         [[-1.6928, -1.0157, -0.0000],\n",
      "          [ 1.6928,  1.0157,  0.6771],\n",
      "          [ 1.0157, -0.3386, -0.3386]],\n",
      "\n",
      "         [[ 0.6771, -0.3386,  0.6771],\n",
      "          [-1.3543, -0.0000, -0.0000],\n",
      "          [-0.6771,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.6771, -1.3543, -1.0157],\n",
      "          [-0.6771, -0.6771, -0.6771],\n",
      "          [-0.0000, -1.0157, -0.3386]],\n",
      "\n",
      "         [[-1.6928,  0.6771,  1.3543],\n",
      "          [-2.3700,  0.6771,  1.6928],\n",
      "          [-0.3386, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.6771, -0.6771,  0.6771],\n",
      "          [-1.0157, -0.6771, -0.6771],\n",
      "          [-2.3700, -0.6771, -0.3386]],\n",
      "\n",
      "         [[ 0.0000,  1.0157,  1.3543],\n",
      "          [-0.6771,  1.3543,  2.0314],\n",
      "          [-2.0314, -1.3543,  0.3386]]],\n",
      "\n",
      "\n",
      "        [[[-0.3386,  1.6928,  1.3543],\n",
      "          [ 0.3386,  0.3386,  0.3386],\n",
      "          [-1.0157, -0.0000, -0.3386]],\n",
      "\n",
      "         [[-1.6928, -0.3386,  0.3386],\n",
      "          [-0.6771, -0.0000, -0.6771],\n",
      "          [ 1.0157,  0.3386, -0.3386]],\n",
      "\n",
      "         [[-0.3386, -0.6771,  0.0000],\n",
      "          [ 0.3386,  0.6771,  0.6771],\n",
      "          [-0.6771, -1.6928, -1.0157]],\n",
      "\n",
      "         [[-1.0157, -1.0157, -0.3386],\n",
      "          [-0.6771, -0.6771,  0.3386],\n",
      "          [ 1.6928,  1.0157,  0.6771]],\n",
      "\n",
      "         [[-1.3543, -0.6771,  1.0157],\n",
      "          [-0.0000, -0.3386,  0.3386],\n",
      "          [-1.0157, -1.0157, -1.0157]],\n",
      "\n",
      "         [[-0.6771, -0.0000, -2.0314],\n",
      "          [-0.6771, -1.3543, -2.0314],\n",
      "          [-0.3386, -1.3543, -1.0157]],\n",
      "\n",
      "         [[ 1.0157,  1.3543,  1.6928],\n",
      "          [ 1.0157,  0.6771,  1.0157],\n",
      "          [-0.3386,  0.3386, -0.3386]],\n",
      "\n",
      "         [[ 2.3700,  1.6928, -1.0157],\n",
      "          [ 1.6928,  1.3543,  1.0157],\n",
      "          [ 1.6928,  1.0157,  0.3386]]],\n",
      "\n",
      "\n",
      "        [[[-0.6771,  0.6771,  1.3543],\n",
      "          [ 2.0314,  1.0157,  1.0157],\n",
      "          [ 0.6771,  0.3386, -0.3386]],\n",
      "\n",
      "         [[-0.6771, -1.3543,  1.0157],\n",
      "          [-0.3386,  0.3386,  0.6771],\n",
      "          [-0.6771,  1.3543,  1.6928]],\n",
      "\n",
      "         [[ 0.6771,  0.0000, -0.3386],\n",
      "          [ 2.0314, -0.3386, -1.0157],\n",
      "          [ 0.3386, -0.3386,  0.0000]],\n",
      "\n",
      "         [[ 0.6771, -1.0157, -0.3386],\n",
      "          [-1.3543, -1.3543,  0.6771],\n",
      "          [-1.0157, -1.6928,  0.6771]],\n",
      "\n",
      "         [[ 0.6771, -0.3386,  0.0000],\n",
      "          [ 1.0157,  0.6771,  0.6771],\n",
      "          [ 0.3386,  0.3386,  1.0157]],\n",
      "\n",
      "         [[-1.0157, -0.0000,  0.6771],\n",
      "          [-1.0157, -2.0314, -1.6928],\n",
      "          [-0.6771, -1.3543, -2.0314]],\n",
      "\n",
      "         [[-0.0000,  0.6771,  0.3386],\n",
      "          [ 0.6771,  1.0157,  1.0157],\n",
      "          [ 0.6771,  0.3386,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.3386, -0.3386],\n",
      "          [-1.3543, -0.0000, -1.0157],\n",
      "          [-0.6771,  0.0000,  1.0157]]],\n",
      "\n",
      "\n",
      "        [[[-0.6771,  0.0000, -2.3700],\n",
      "          [-1.3543, -0.3386, -1.6928],\n",
      "          [-0.0000,  2.0314, -0.3386]],\n",
      "\n",
      "         [[ 1.6928, -1.0157, -1.0157],\n",
      "          [ 1.3543, -0.6771, -0.0000],\n",
      "          [ 0.3386, -1.6928, -0.3386]],\n",
      "\n",
      "         [[-0.6771, -0.3386,  0.6771],\n",
      "          [-0.0000,  1.3543,  1.0157],\n",
      "          [-1.0157, -0.3386, -0.3386]],\n",
      "\n",
      "         [[ 0.6771,  1.3543,  0.3386],\n",
      "          [-0.3386,  1.0157,  0.0000],\n",
      "          [-1.0157,  0.6771, -0.3386]],\n",
      "\n",
      "         [[ 0.3386,  0.6771,  0.6771],\n",
      "          [ 1.3543,  2.3700,  2.3700],\n",
      "          [ 0.3386,  0.3386,  0.6771]],\n",
      "\n",
      "         [[-1.0157, -0.3386,  0.6771],\n",
      "          [-1.3543, -0.6771, -1.3543],\n",
      "          [-2.0314, -1.0157, -1.0157]],\n",
      "\n",
      "         [[-1.0157,  1.0157,  1.0157],\n",
      "          [-1.0157,  0.6771,  1.6928],\n",
      "          [-1.0157,  0.6771, -0.3386]],\n",
      "\n",
      "         [[-0.3386,  0.0000,  0.0000],\n",
      "          [-0.3386,  0.3386,  0.3386],\n",
      "          [ 0.3386,  1.0157,  1.0157]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6771, -0.3386, -0.6771],\n",
      "          [ 1.0157,  0.6771, -0.6771],\n",
      "          [ 0.3386,  0.3386, -1.0157]],\n",
      "\n",
      "         [[ 1.3543,  0.3386, -1.0157],\n",
      "          [ 1.6928, -0.0000,  1.0157],\n",
      "          [-0.6771, -0.3386,  0.6771]],\n",
      "\n",
      "         [[-0.6771,  0.0000, -1.0157],\n",
      "          [-0.6771,  0.3386,  0.0000],\n",
      "          [ 2.0314,  1.0157, -0.6771]],\n",
      "\n",
      "         [[-0.6771, -1.6928, -1.6928],\n",
      "          [-0.3386, -0.6771, -1.3543],\n",
      "          [-0.3386, -0.3386, -0.3386]],\n",
      "\n",
      "         [[ 1.3543,  1.0157, -0.0000],\n",
      "          [ 0.6771,  0.3386,  0.6771],\n",
      "          [ 1.0157,  0.6771,  0.0000]],\n",
      "\n",
      "         [[-0.3386,  0.3386, -0.3386],\n",
      "          [-1.0157,  0.6771,  0.6771],\n",
      "          [-1.0157,  1.0157,  1.6928]],\n",
      "\n",
      "         [[-1.0157,  1.0157, -0.3386],\n",
      "          [-0.6771,  0.0000,  0.0000],\n",
      "          [-2.3700, -2.0314, -1.3543]],\n",
      "\n",
      "         [[ 0.0000,  0.6771,  1.0157],\n",
      "          [ 1.6928,  1.6928,  1.0157],\n",
      "          [ 2.3700,  1.0157, -1.0157]]],\n",
      "\n",
      "\n",
      "        [[[-1.0157,  0.3386,  1.3543],\n",
      "          [-2.3700, -0.6771,  0.3386],\n",
      "          [-1.3543, -0.3386,  0.6771]],\n",
      "\n",
      "         [[-0.6771, -1.3543,  0.3386],\n",
      "          [ 2.3700,  2.3700,  2.3700],\n",
      "          [ 0.0000,  2.3700,  2.3700]],\n",
      "\n",
      "         [[-0.0000, -1.0157, -1.0157],\n",
      "          [-0.3386, -0.6771, -1.0157],\n",
      "          [ 1.0157,  1.3543, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.3386, -1.0157],\n",
      "          [-0.0000, -0.3386,  0.3386],\n",
      "          [-1.3543, -2.0314, -0.6771]],\n",
      "\n",
      "         [[ 1.3543,  0.3386,  1.0157],\n",
      "          [-1.0157, -0.6771, -0.3386],\n",
      "          [-1.0157,  0.0000, -1.3543]],\n",
      "\n",
      "         [[-0.3386,  0.3386, -0.3386],\n",
      "          [ 0.3386, -0.6771, -0.6771],\n",
      "          [ 0.6771,  0.0000, -1.0157]],\n",
      "\n",
      "         [[-1.0157, -0.3386, -0.6771],\n",
      "          [ 0.3386,  1.0157, -0.3386],\n",
      "          [ 0.6771,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.3543,  1.3543, -0.0000],\n",
      "          [ 1.0157,  0.6771,  1.0157],\n",
      "          [-0.6771, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.3386],\n",
      "          [ 0.3386,  0.3386, -1.0157],\n",
      "          [ 0.6771,  1.0157,  0.3386]],\n",
      "\n",
      "         [[-0.3386,  0.0000,  0.6771],\n",
      "          [-1.3543,  1.0157,  0.0000],\n",
      "          [-1.0157,  0.3386,  0.6771]],\n",
      "\n",
      "         [[-0.0000, -0.3386, -1.3543],\n",
      "          [ 1.0157,  0.3386, -0.6771],\n",
      "          [-1.3543, -2.3700, -2.3700]],\n",
      "\n",
      "         [[ 1.0157, -0.6771, -1.6928],\n",
      "          [ 1.6928,  0.3386, -1.0157],\n",
      "          [-0.3386, -0.3386,  0.3386]],\n",
      "\n",
      "         [[ 1.6928,  1.6928,  2.0314],\n",
      "          [ 0.6771,  0.3386,  1.0157],\n",
      "          [-1.3543, -0.6771, -0.3386]],\n",
      "\n",
      "         [[-0.3386,  0.3386,  0.3386],\n",
      "          [-0.0000,  1.6928,  1.3543],\n",
      "          [ 0.6771,  0.3386, -0.3386]],\n",
      "\n",
      "         [[-0.0000,  0.3386, -0.0000],\n",
      "          [-1.6928,  1.0157,  1.3543],\n",
      "          [-2.3700, -0.0000,  2.0314]],\n",
      "\n",
      "         [[-1.3543, -1.3543, -0.3386],\n",
      "          [-2.0314, -1.3543, -0.6771],\n",
      "          [-0.3386, -1.0157,  0.3386]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6771, -0.3386, -1.3543],\n",
      "          [-1.3543, -1.3543, -1.6928],\n",
      "          [ 0.0000, -0.3386, -1.0157]],\n",
      "\n",
      "         [[-1.3543,  1.3543,  0.3386],\n",
      "          [ 1.0157,  2.3700,  0.3386],\n",
      "          [-0.6771,  2.0314,  1.0157]],\n",
      "\n",
      "         [[-0.6771,  1.0157, -0.3386],\n",
      "          [-0.3386,  1.0157, -0.6771],\n",
      "          [ 0.3386,  2.0314, -1.3543]],\n",
      "\n",
      "         [[ 1.0157,  0.6771, -1.3543],\n",
      "          [ 1.6928,  2.0314, -0.3386],\n",
      "          [ 0.6771,  0.6771,  0.3386]],\n",
      "\n",
      "         [[-1.3543, -1.0157,  0.0000],\n",
      "          [-0.6771,  0.0000, -0.3386],\n",
      "          [-0.0000,  0.0000, -1.0157]],\n",
      "\n",
      "         [[-0.6771,  0.0000,  0.3386],\n",
      "          [ 0.3386,  0.3386, -0.6771],\n",
      "          [-0.3386, -0.0000, -1.3543]],\n",
      "\n",
      "         [[ 1.0157,  0.0000,  0.3386],\n",
      "          [ 0.6771,  0.3386,  1.0157],\n",
      "          [-2.0314, -1.0157,  0.0000]],\n",
      "\n",
      "         [[-0.3386, -2.0314, -1.3543],\n",
      "          [-0.6771, -0.6771, -0.3386],\n",
      "          [-0.0000, -0.3386,  0.3386]]]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor(2.3700, device='cuda:0', requires_grad=True)\n",
      "tensor([[[[-2.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  5.0000,  3.0000],\n",
      "          [ 4.0000,  6.0000,  4.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-3.0000, -1.0000,  0.0000],\n",
      "          [-2.0000,  2.0000,  1.0000]],\n",
      "\n",
      "         [[-5.0000, -3.0000, -0.0000],\n",
      "          [ 5.0000,  3.0000,  2.0000],\n",
      "          [ 3.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 2.0000, -1.0000,  2.0000],\n",
      "          [-4.0000, -0.0000, -0.0000],\n",
      "          [-2.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000, -3.0000],\n",
      "          [-2.0000, -2.0000, -2.0000],\n",
      "          [-0.0000, -3.0000, -1.0000]],\n",
      "\n",
      "         [[-5.0000,  2.0000,  4.0000],\n",
      "          [-7.0000,  2.0000,  5.0000],\n",
      "          [-1.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000,  2.0000],\n",
      "          [-3.0000, -2.0000, -2.0000],\n",
      "          [-7.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  4.0000],\n",
      "          [-2.0000,  4.0000,  6.0000],\n",
      "          [-6.0000, -4.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  5.0000,  4.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000],\n",
      "          [-3.0000, -0.0000, -1.0000]],\n",
      "\n",
      "         [[-5.0000, -1.0000,  1.0000],\n",
      "          [-2.0000, -0.0000, -2.0000],\n",
      "          [ 3.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000,  0.0000],\n",
      "          [ 1.0000,  2.0000,  2.0000],\n",
      "          [-2.0000, -5.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000, -3.0000, -1.0000],\n",
      "          [-2.0000, -2.0000,  1.0000],\n",
      "          [ 5.0000,  3.0000,  2.0000]],\n",
      "\n",
      "         [[-4.0000, -2.0000,  3.0000],\n",
      "          [-0.0000, -1.0000,  1.0000],\n",
      "          [-3.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-2.0000, -0.0000, -6.0000],\n",
      "          [-2.0000, -4.0000, -6.0000],\n",
      "          [-1.0000, -4.0000, -3.0000]],\n",
      "\n",
      "         [[ 3.0000,  4.0000,  5.0000],\n",
      "          [ 3.0000,  2.0000,  3.0000],\n",
      "          [-1.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[ 7.0000,  5.0000, -3.0000],\n",
      "          [ 5.0000,  4.0000,  3.0000],\n",
      "          [ 5.0000,  3.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000,  2.0000,  4.0000],\n",
      "          [ 6.0000,  3.0000,  3.0000],\n",
      "          [ 2.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000,  3.0000],\n",
      "          [-1.0000,  1.0000,  2.0000],\n",
      "          [-2.0000,  4.0000,  5.0000]],\n",
      "\n",
      "         [[ 2.0000,  0.0000, -1.0000],\n",
      "          [ 6.0000, -1.0000, -3.0000],\n",
      "          [ 1.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000, -3.0000, -1.0000],\n",
      "          [-4.0000, -4.0000,  2.0000],\n",
      "          [-3.0000, -5.0000,  2.0000]],\n",
      "\n",
      "         [[ 2.0000, -1.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  3.0000]],\n",
      "\n",
      "         [[-3.0000, -0.0000,  2.0000],\n",
      "          [-3.0000, -6.0000, -5.0000],\n",
      "          [-2.0000, -4.0000, -6.0000]],\n",
      "\n",
      "         [[-0.0000,  2.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000, -1.0000],\n",
      "          [-4.0000, -0.0000, -3.0000],\n",
      "          [-2.0000,  0.0000,  3.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000,  0.0000, -7.0000],\n",
      "          [-4.0000, -1.0000, -5.0000],\n",
      "          [-0.0000,  6.0000, -1.0000]],\n",
      "\n",
      "         [[ 5.0000, -3.0000, -3.0000],\n",
      "          [ 4.0000, -2.0000, -0.0000],\n",
      "          [ 1.0000, -5.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000,  2.0000],\n",
      "          [-0.0000,  4.0000,  3.0000],\n",
      "          [-3.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 2.0000,  4.0000,  1.0000],\n",
      "          [-1.0000,  3.0000,  0.0000],\n",
      "          [-3.0000,  2.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000],\n",
      "          [ 4.0000,  7.0000,  7.0000],\n",
      "          [ 1.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-3.0000, -1.0000,  2.0000],\n",
      "          [-4.0000, -2.0000, -4.0000],\n",
      "          [-6.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000,  3.0000,  3.0000],\n",
      "          [-3.0000,  2.0000,  5.0000],\n",
      "          [-3.0000,  2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  0.0000],\n",
      "          [-1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  3.0000,  3.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, -1.0000, -2.0000],\n",
      "          [ 3.0000,  2.0000, -2.0000],\n",
      "          [ 1.0000,  1.0000, -3.0000]],\n",
      "\n",
      "         [[ 4.0000,  1.0000, -3.0000],\n",
      "          [ 5.0000, -0.0000,  3.0000],\n",
      "          [-2.0000, -1.0000,  2.0000]],\n",
      "\n",
      "         [[-2.0000,  0.0000, -3.0000],\n",
      "          [-2.0000,  1.0000,  0.0000],\n",
      "          [ 6.0000,  3.0000, -2.0000]],\n",
      "\n",
      "         [[-2.0000, -5.0000, -5.0000],\n",
      "          [-1.0000, -2.0000, -4.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 4.0000,  3.0000, -0.0000],\n",
      "          [ 2.0000,  1.0000,  2.0000],\n",
      "          [ 3.0000,  2.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000, -1.0000],\n",
      "          [-3.0000,  2.0000,  2.0000],\n",
      "          [-3.0000,  3.0000,  5.0000]],\n",
      "\n",
      "         [[-3.0000,  3.0000, -1.0000],\n",
      "          [-2.0000,  0.0000,  0.0000],\n",
      "          [-7.0000, -6.0000, -4.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  3.0000],\n",
      "          [ 5.0000,  5.0000,  3.0000],\n",
      "          [ 7.0000,  3.0000, -3.0000]]],\n",
      "\n",
      "\n",
      "        [[[-3.0000,  1.0000,  4.0000],\n",
      "          [-7.0000, -2.0000,  1.0000],\n",
      "          [-4.0000, -1.0000,  2.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000,  1.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 0.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-0.0000, -3.0000, -3.0000],\n",
      "          [-1.0000, -2.0000, -3.0000],\n",
      "          [ 3.0000,  4.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  1.0000, -3.0000],\n",
      "          [-0.0000, -1.0000,  1.0000],\n",
      "          [-4.0000, -6.0000, -2.0000]],\n",
      "\n",
      "         [[ 4.0000,  1.0000,  3.0000],\n",
      "          [-3.0000, -2.0000, -1.0000],\n",
      "          [-3.0000,  0.0000, -4.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000, -1.0000],\n",
      "          [ 1.0000, -2.0000, -2.0000],\n",
      "          [ 2.0000,  0.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000, -1.0000, -2.0000],\n",
      "          [ 1.0000,  3.0000, -1.0000],\n",
      "          [ 2.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 4.0000,  4.0000, -0.0000],\n",
      "          [ 3.0000,  2.0000,  3.0000],\n",
      "          [-2.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -1.0000],\n",
      "          [ 1.0000,  1.0000, -3.0000],\n",
      "          [ 2.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  2.0000],\n",
      "          [-4.0000,  3.0000,  0.0000],\n",
      "          [-3.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -4.0000],\n",
      "          [ 3.0000,  1.0000, -2.0000],\n",
      "          [-4.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 3.0000, -2.0000, -5.0000],\n",
      "          [ 5.0000,  1.0000, -3.0000],\n",
      "          [-1.0000, -1.0000,  1.0000]],\n",
      "\n",
      "         [[ 5.0000,  5.0000,  6.0000],\n",
      "          [ 2.0000,  1.0000,  3.0000],\n",
      "          [-4.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000,  1.0000],\n",
      "          [-0.0000,  5.0000,  4.0000],\n",
      "          [ 2.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000,  1.0000, -0.0000],\n",
      "          [-5.0000,  3.0000,  4.0000],\n",
      "          [-7.0000, -0.0000,  6.0000]],\n",
      "\n",
      "         [[-4.0000, -4.0000, -1.0000],\n",
      "          [-6.0000, -4.0000, -2.0000],\n",
      "          [-1.0000, -3.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, -1.0000, -4.0000],\n",
      "          [-4.0000, -4.0000, -5.0000],\n",
      "          [ 0.0000, -1.0000, -3.0000]],\n",
      "\n",
      "         [[-4.0000,  4.0000,  1.0000],\n",
      "          [ 3.0000,  7.0000,  1.0000],\n",
      "          [-2.0000,  6.0000,  3.0000]],\n",
      "\n",
      "         [[-2.0000,  3.0000, -1.0000],\n",
      "          [-1.0000,  3.0000, -2.0000],\n",
      "          [ 1.0000,  6.0000, -4.0000]],\n",
      "\n",
      "         [[ 3.0000,  2.0000, -4.0000],\n",
      "          [ 5.0000,  6.0000, -1.0000],\n",
      "          [ 2.0000,  2.0000,  1.0000]],\n",
      "\n",
      "         [[-4.0000, -3.0000,  0.0000],\n",
      "          [-2.0000,  0.0000, -1.0000],\n",
      "          [-0.0000,  0.0000, -3.0000]],\n",
      "\n",
      "         [[-2.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000, -2.0000],\n",
      "          [-1.0000, -0.0000, -4.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  3.0000],\n",
      "          [-6.0000, -3.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -6.0000, -4.0000],\n",
      "          [-2.0000, -2.0000, -1.0000],\n",
      "          [-0.0000, -1.0000,  1.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "quantConv2d = model.features[27]\n",
    "print(quantConv2d)\n",
    "#weight_q = model.layer1[0].conv2.weight_q # quantized value is stored during the training\n",
    "weight_q = quantConv2d.weight_q\n",
    "print(weight_q)\n",
    "\n",
    "weight_alpha = quantConv2d.weight_quant.wgt_alpha\n",
    "print(weight_alpha)\n",
    "w_delta = weight_alpha / (2 ** (w_bit - 1) - 1)   # delta can be calculated by using alpha and w_bit\n",
    "weight_int =  weight_q / w_delta# w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "furnished-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  5.0000,  3.0000],\n",
      "          [ 4.0000,  6.0000,  4.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-3.0000, -1.0000,  0.0000],\n",
      "          [-2.0000,  2.0000,  1.0000]],\n",
      "\n",
      "         [[-5.0000, -3.0000, -0.0000],\n",
      "          [ 5.0000,  3.0000,  2.0000],\n",
      "          [ 3.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 2.0000, -1.0000,  2.0000],\n",
      "          [-4.0000, -0.0000, -0.0000],\n",
      "          [-2.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000, -3.0000],\n",
      "          [-2.0000, -2.0000, -2.0000],\n",
      "          [-0.0000, -3.0000, -1.0000]],\n",
      "\n",
      "         [[-5.0000,  2.0000,  4.0000],\n",
      "          [-7.0000,  2.0000,  5.0000],\n",
      "          [-1.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000,  2.0000],\n",
      "          [-3.0000, -2.0000, -2.0000],\n",
      "          [-7.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  4.0000],\n",
      "          [-2.0000,  4.0000,  6.0000],\n",
      "          [-6.0000, -4.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  5.0000,  4.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000],\n",
      "          [-3.0000, -0.0000, -1.0000]],\n",
      "\n",
      "         [[-5.0000, -1.0000,  1.0000],\n",
      "          [-2.0000, -0.0000, -2.0000],\n",
      "          [ 3.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000,  0.0000],\n",
      "          [ 1.0000,  2.0000,  2.0000],\n",
      "          [-2.0000, -5.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000, -3.0000, -1.0000],\n",
      "          [-2.0000, -2.0000,  1.0000],\n",
      "          [ 5.0000,  3.0000,  2.0000]],\n",
      "\n",
      "         [[-4.0000, -2.0000,  3.0000],\n",
      "          [-0.0000, -1.0000,  1.0000],\n",
      "          [-3.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-2.0000, -0.0000, -6.0000],\n",
      "          [-2.0000, -4.0000, -6.0000],\n",
      "          [-1.0000, -4.0000, -3.0000]],\n",
      "\n",
      "         [[ 3.0000,  4.0000,  5.0000],\n",
      "          [ 3.0000,  2.0000,  3.0000],\n",
      "          [-1.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[ 7.0000,  5.0000, -3.0000],\n",
      "          [ 5.0000,  4.0000,  3.0000],\n",
      "          [ 5.0000,  3.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000,  2.0000,  4.0000],\n",
      "          [ 6.0000,  3.0000,  3.0000],\n",
      "          [ 2.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000,  3.0000],\n",
      "          [-1.0000,  1.0000,  2.0000],\n",
      "          [-2.0000,  4.0000,  5.0000]],\n",
      "\n",
      "         [[ 2.0000,  0.0000, -1.0000],\n",
      "          [ 6.0000, -1.0000, -3.0000],\n",
      "          [ 1.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000, -3.0000, -1.0000],\n",
      "          [-4.0000, -4.0000,  2.0000],\n",
      "          [-3.0000, -5.0000,  2.0000]],\n",
      "\n",
      "         [[ 2.0000, -1.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  3.0000]],\n",
      "\n",
      "         [[-3.0000, -0.0000,  2.0000],\n",
      "          [-3.0000, -6.0000, -5.0000],\n",
      "          [-2.0000, -4.0000, -6.0000]],\n",
      "\n",
      "         [[-0.0000,  2.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000, -1.0000],\n",
      "          [-4.0000, -0.0000, -3.0000],\n",
      "          [-2.0000,  0.0000,  3.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000,  0.0000, -7.0000],\n",
      "          [-4.0000, -1.0000, -5.0000],\n",
      "          [-0.0000,  6.0000, -1.0000]],\n",
      "\n",
      "         [[ 5.0000, -3.0000, -3.0000],\n",
      "          [ 4.0000, -2.0000, -0.0000],\n",
      "          [ 1.0000, -5.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000,  2.0000],\n",
      "          [-0.0000,  4.0000,  3.0000],\n",
      "          [-3.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 2.0000,  4.0000,  1.0000],\n",
      "          [-1.0000,  3.0000,  0.0000],\n",
      "          [-3.0000,  2.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000],\n",
      "          [ 4.0000,  7.0000,  7.0000],\n",
      "          [ 1.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-3.0000, -1.0000,  2.0000],\n",
      "          [-4.0000, -2.0000, -4.0000],\n",
      "          [-6.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000,  3.0000,  3.0000],\n",
      "          [-3.0000,  2.0000,  5.0000],\n",
      "          [-3.0000,  2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  0.0000],\n",
      "          [-1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  3.0000,  3.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, -1.0000, -2.0000],\n",
      "          [ 3.0000,  2.0000, -2.0000],\n",
      "          [ 1.0000,  1.0000, -3.0000]],\n",
      "\n",
      "         [[ 4.0000,  1.0000, -3.0000],\n",
      "          [ 5.0000, -0.0000,  3.0000],\n",
      "          [-2.0000, -1.0000,  2.0000]],\n",
      "\n",
      "         [[-2.0000,  0.0000, -3.0000],\n",
      "          [-2.0000,  1.0000,  0.0000],\n",
      "          [ 6.0000,  3.0000, -2.0000]],\n",
      "\n",
      "         [[-2.0000, -5.0000, -5.0000],\n",
      "          [-1.0000, -2.0000, -4.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 4.0000,  3.0000, -0.0000],\n",
      "          [ 2.0000,  1.0000,  2.0000],\n",
      "          [ 3.0000,  2.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000, -1.0000],\n",
      "          [-3.0000,  2.0000,  2.0000],\n",
      "          [-3.0000,  3.0000,  5.0000]],\n",
      "\n",
      "         [[-3.0000,  3.0000, -1.0000],\n",
      "          [-2.0000,  0.0000,  0.0000],\n",
      "          [-7.0000, -6.0000, -4.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  3.0000],\n",
      "          [ 5.0000,  5.0000,  3.0000],\n",
      "          [ 7.0000,  3.0000, -3.0000]]],\n",
      "\n",
      "\n",
      "        [[[-3.0000,  1.0000,  4.0000],\n",
      "          [-7.0000, -2.0000,  1.0000],\n",
      "          [-4.0000, -1.0000,  2.0000]],\n",
      "\n",
      "         [[-2.0000, -4.0000,  1.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 0.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-0.0000, -3.0000, -3.0000],\n",
      "          [-1.0000, -2.0000, -3.0000],\n",
      "          [ 3.0000,  4.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  1.0000, -3.0000],\n",
      "          [-0.0000, -1.0000,  1.0000],\n",
      "          [-4.0000, -6.0000, -2.0000]],\n",
      "\n",
      "         [[ 4.0000,  1.0000,  3.0000],\n",
      "          [-3.0000, -2.0000, -1.0000],\n",
      "          [-3.0000,  0.0000, -4.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000, -1.0000],\n",
      "          [ 1.0000, -2.0000, -2.0000],\n",
      "          [ 2.0000,  0.0000, -3.0000]],\n",
      "\n",
      "         [[-3.0000, -1.0000, -2.0000],\n",
      "          [ 1.0000,  3.0000, -1.0000],\n",
      "          [ 2.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 4.0000,  4.0000, -0.0000],\n",
      "          [ 3.0000,  2.0000,  3.0000],\n",
      "          [-2.0000, -0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -1.0000],\n",
      "          [ 1.0000,  1.0000, -3.0000],\n",
      "          [ 2.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  2.0000],\n",
      "          [-4.0000,  3.0000,  0.0000],\n",
      "          [-3.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -4.0000],\n",
      "          [ 3.0000,  1.0000, -2.0000],\n",
      "          [-4.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 3.0000, -2.0000, -5.0000],\n",
      "          [ 5.0000,  1.0000, -3.0000],\n",
      "          [-1.0000, -1.0000,  1.0000]],\n",
      "\n",
      "         [[ 5.0000,  5.0000,  6.0000],\n",
      "          [ 2.0000,  1.0000,  3.0000],\n",
      "          [-4.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000,  1.0000],\n",
      "          [-0.0000,  5.0000,  4.0000],\n",
      "          [ 2.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000,  1.0000, -0.0000],\n",
      "          [-5.0000,  3.0000,  4.0000],\n",
      "          [-7.0000, -0.0000,  6.0000]],\n",
      "\n",
      "         [[-4.0000, -4.0000, -1.0000],\n",
      "          [-6.0000, -4.0000, -2.0000],\n",
      "          [-1.0000, -3.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, -1.0000, -4.0000],\n",
      "          [-4.0000, -4.0000, -5.0000],\n",
      "          [ 0.0000, -1.0000, -3.0000]],\n",
      "\n",
      "         [[-4.0000,  4.0000,  1.0000],\n",
      "          [ 3.0000,  7.0000,  1.0000],\n",
      "          [-2.0000,  6.0000,  3.0000]],\n",
      "\n",
      "         [[-2.0000,  3.0000, -1.0000],\n",
      "          [-1.0000,  3.0000, -2.0000],\n",
      "          [ 1.0000,  6.0000, -4.0000]],\n",
      "\n",
      "         [[ 3.0000,  2.0000, -4.0000],\n",
      "          [ 5.0000,  6.0000, -1.0000],\n",
      "          [ 2.0000,  2.0000,  1.0000]],\n",
      "\n",
      "         [[-4.0000, -3.0000,  0.0000],\n",
      "          [-2.0000,  0.0000, -1.0000],\n",
      "          [-0.0000,  0.0000, -3.0000]],\n",
      "\n",
      "         [[-2.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000, -2.0000],\n",
      "          [-1.0000, -0.0000, -4.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  3.0000],\n",
      "          [-6.0000, -3.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -6.0000, -4.0000],\n",
      "          [-2.0000, -2.0000, -1.0000],\n",
      "          [-0.0000, -1.0000,  1.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "##### Find \"weight_int\" for features[3] ####\n",
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "textile-cancer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  1.0000,  2.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[ 6.0000,  4.0000,  1.0000,  2.0000],\n",
      "          [ 2.0000,  0.0000,  2.0000,  3.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  1.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  2.0000,  1.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  3.0000,  2.0000],\n",
      "          [ 0.0000,  1.0000,  3.0000,  1.0000],\n",
      "          [ 0.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 1.0000,  2.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  5.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000,  1.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  3.0000,  4.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  4.0000],\n",
      "          [ 0.0000,  1.0000,  3.0000,  4.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  2.0000,  1.0000],\n",
      "          [ 4.0000,  5.0000,  5.0000,  3.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 2.0000,  4.0000,  3.0000,  1.0000],\n",
      "          [ 3.0000,  6.0000,  5.0000,  3.0000],\n",
      "          [ 4.0000,  4.0000,  3.0000,  3.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  2.0000,  2.0000],\n",
      "          [ 1.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  2.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 4.0000,  8.0000,  8.0000,  5.0000],\n",
      "          [ 3.0000,  5.0000,  5.0000,  3.0000],\n",
      "          [ 4.0000,  6.0000,  6.0000,  5.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  4.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  2.0000,  0.0000],\n",
      "          [ 2.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  4.0000,  1.0000,  0.0000],\n",
      "          [ 6.0000, 10.0000,  6.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 2.0000,  4.0000,  5.0000,  5.0000],\n",
      "          [ 0.0000,  3.0000,  7.0000,  7.0000],\n",
      "          [ 0.0000,  0.0000,  4.0000,  7.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[11.0000,  6.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  1.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 5.0000,  4.0000,  1.0000,  0.0000],\n",
      "          [ 5.0000,  6.0000,  3.0000,  0.0000],\n",
      "          [ 3.0000,  4.0000,  2.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  5.0000,  6.0000,  5.0000],\n",
      "          [ 0.0000,  5.0000,  7.0000,  6.0000],\n",
      "          [ 3.0000,  7.0000,  6.0000,  4.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  2.0000]],\n",
      "\n",
      "         [[ 2.0000, 10.0000, 12.0000,  7.0000],\n",
      "          [ 0.0000,  2.0000,  7.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 3.0000,  4.0000,  3.0000,  2.0000],\n",
      "          [ 4.0000,  5.0000,  5.0000,  6.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  4.0000],\n",
      "          [ 3.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  3.0000,  4.0000,  3.0000],\n",
      "          [ 2.0000,  2.0000,  4.0000,  3.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  3.0000],\n",
      "          [ 1.0000,  2.0000,  5.0000,  5.0000],\n",
      "          [ 1.0000,  3.0000,  8.0000,  8.0000],\n",
      "          [ 1.0000,  3.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  2.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  2.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0000,  7.0000, 10.0000,  7.0000],\n",
      "          [ 5.0000,  9.0000, 13.0000,  9.0000],\n",
      "          [ 0.0000,  1.0000,  5.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[ 6.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000,  7.0000,  5.0000,  6.0000],\n",
      "          [ 8.0000, 10.0000, 10.0000,  8.0000]],\n",
      "\n",
      "         [[ 3.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  6.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  6.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 0.0000,  2.0000,  5.0000,  4.0000]],\n",
      "\n",
      "         [[ 2.0000,  5.0000,  5.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "act = save_output.outputs[8][0] # input of 27th quantconv\n",
    "act_alpha  = model.features[27].act_alpha\n",
    "act_bit = 4\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "act_delta = act_alpha / (2**act_bit - 1)\n",
    "act_int = act_q / act_delta\n",
    "print(act_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "selected-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref = save_output.outputs[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(8,8, kernel_size = 3, padding = 1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "out_int = conv_int(act_int)\n",
    "relu = model.features[28]\n",
    "out_recovered = out_int * act_delta * w_delta\n",
    "out_recovered = relu(out_recovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2647e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs(out_ref - out_recovered)\n",
    "print(difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057cfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
